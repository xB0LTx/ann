{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814704ca-3c42-4491-a8b9-6a14ad2c71b1",
   "metadata": {
    "id": "814704ca-3c42-4491-a8b9-6a14ad2c71b1"
   },
   "source": [
    "## Sztuczne sieci neuronowe - laboratorium 10"
   ]
  },
  {
   "cell_type": "code",
   "id": "232faca3-7e9f-4c77-9a78-11f7b94fc537",
   "metadata": {
    "id": "232faca3-7e9f-4c77-9a78-11f7b94fc537",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:29.074162Z",
     "start_time": "2025-05-22T17:39:25.159263Z"
    }
   },
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import v2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d8e71cfa-7e7d-4193-9208-d449cbb12c77",
   "metadata": {
    "id": "d8e71cfa-7e7d-4193-9208-d449cbb12c77",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:29.199780Z",
     "start_time": "2025-05-22T17:39:29.092178Z"
    }
   },
   "source": [
    "# sprawdzenie, czy GPU jest widoczne\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "6383c1b4-ff57-4721-86f2-8ec0ff0e0790",
   "metadata": {
    "id": "6383c1b4-ff57-4721-86f2-8ec0ff0e0790"
   },
   "source": [
    "## Transfer learning\n",
    "\n",
    "Dzisiejsze zajęcia będą dotyczyły zagadnienia **transfer learning** - trenowania modeli polegającego na wykorzystaniu architektury i zestawu wag wytrenowanych wcześniej (np. przez kogoś innego - najczęściej badaczy z największych firm, na dużym zbiorze danych), celem wykorzystania \"wiedzy\" zgromadzonej w już wytrenowanym modelu i przeniesienia jej (stąd \"transfer\") do innego, zwykle węższego problemu (np. poprzez dotrenowanie na znacznie mniejszym zbiorze danych).\n",
    "\n",
    "Fazy te nazywają się odpowiednio **pre-training** (tzw. modele pretrenowane, *pretrained models*) i **fine-tuning**.\n",
    "\n",
    "Wiele z takich gotowych (pretrenowanych) modeli dostępnych jest w pakiecie `torchvision` - części PyTorcha związanej z przetwarzaniem obrazów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab0b22-defd-4dc6-9ecc-c28f663d7a96",
   "metadata": {
    "id": "41ab0b22-defd-4dc6-9ecc-c28f663d7a96"
   },
   "source": [
    "#### Ćwiczenie\n",
    "Uruchom poniższą komórkę, aby wypisać dostępne w `torchvision` modele. Nazwy modeli zaczynające się dużą literą oznaczają klasy implementujące poszczególne architektury sieci. Ich odpowiedniki pisane małymi literami to funkcje pozwalające zainicjalizować model (https://pytorch.org/vision/stable/models.html).\n",
    "\n",
    "Funkcje te mają argument `pretrained` - gdy podamy wartość `True`, inicjalizujemy model pretrenowanymi wagami (dla `False` - losowymi).\n",
    "\n",
    "Wczytaj po kolei wybrane modele (np. `resnet18`) do zmiennej. Sprawdź jej zawartość."
   ]
  },
  {
   "cell_type": "code",
   "id": "fd40543c-050f-4418-95fe-bc20e9d2fa81",
   "metadata": {
    "id": "fd40543c-050f-4418-95fe-bc20e9d2fa81",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:29.426265Z",
     "start_time": "2025-05-22T17:39:29.417877Z"
    }
   },
   "source": [
    "dir(models)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlexNet',\n",
       " 'AlexNet_Weights',\n",
       " 'ConvNeXt',\n",
       " 'ConvNeXt_Base_Weights',\n",
       " 'ConvNeXt_Large_Weights',\n",
       " 'ConvNeXt_Small_Weights',\n",
       " 'ConvNeXt_Tiny_Weights',\n",
       " 'DenseNet',\n",
       " 'DenseNet121_Weights',\n",
       " 'DenseNet161_Weights',\n",
       " 'DenseNet169_Weights',\n",
       " 'DenseNet201_Weights',\n",
       " 'EfficientNet',\n",
       " 'EfficientNet_B0_Weights',\n",
       " 'EfficientNet_B1_Weights',\n",
       " 'EfficientNet_B2_Weights',\n",
       " 'EfficientNet_B3_Weights',\n",
       " 'EfficientNet_B4_Weights',\n",
       " 'EfficientNet_B5_Weights',\n",
       " 'EfficientNet_B6_Weights',\n",
       " 'EfficientNet_B7_Weights',\n",
       " 'EfficientNet_V2_L_Weights',\n",
       " 'EfficientNet_V2_M_Weights',\n",
       " 'EfficientNet_V2_S_Weights',\n",
       " 'GoogLeNet',\n",
       " 'GoogLeNetOutputs',\n",
       " 'GoogLeNet_Weights',\n",
       " 'Inception3',\n",
       " 'InceptionOutputs',\n",
       " 'Inception_V3_Weights',\n",
       " 'MNASNet',\n",
       " 'MNASNet0_5_Weights',\n",
       " 'MNASNet0_75_Weights',\n",
       " 'MNASNet1_0_Weights',\n",
       " 'MNASNet1_3_Weights',\n",
       " 'MaxVit',\n",
       " 'MaxVit_T_Weights',\n",
       " 'MobileNetV2',\n",
       " 'MobileNetV3',\n",
       " 'MobileNet_V2_Weights',\n",
       " 'MobileNet_V3_Large_Weights',\n",
       " 'MobileNet_V3_Small_Weights',\n",
       " 'RegNet',\n",
       " 'RegNet_X_16GF_Weights',\n",
       " 'RegNet_X_1_6GF_Weights',\n",
       " 'RegNet_X_32GF_Weights',\n",
       " 'RegNet_X_3_2GF_Weights',\n",
       " 'RegNet_X_400MF_Weights',\n",
       " 'RegNet_X_800MF_Weights',\n",
       " 'RegNet_X_8GF_Weights',\n",
       " 'RegNet_Y_128GF_Weights',\n",
       " 'RegNet_Y_16GF_Weights',\n",
       " 'RegNet_Y_1_6GF_Weights',\n",
       " 'RegNet_Y_32GF_Weights',\n",
       " 'RegNet_Y_3_2GF_Weights',\n",
       " 'RegNet_Y_400MF_Weights',\n",
       " 'RegNet_Y_800MF_Weights',\n",
       " 'RegNet_Y_8GF_Weights',\n",
       " 'ResNeXt101_32X8D_Weights',\n",
       " 'ResNeXt101_64X4D_Weights',\n",
       " 'ResNeXt50_32X4D_Weights',\n",
       " 'ResNet',\n",
       " 'ResNet101_Weights',\n",
       " 'ResNet152_Weights',\n",
       " 'ResNet18_Weights',\n",
       " 'ResNet34_Weights',\n",
       " 'ResNet50_Weights',\n",
       " 'ShuffleNetV2',\n",
       " 'ShuffleNet_V2_X0_5_Weights',\n",
       " 'ShuffleNet_V2_X1_0_Weights',\n",
       " 'ShuffleNet_V2_X1_5_Weights',\n",
       " 'ShuffleNet_V2_X2_0_Weights',\n",
       " 'SqueezeNet',\n",
       " 'SqueezeNet1_0_Weights',\n",
       " 'SqueezeNet1_1_Weights',\n",
       " 'SwinTransformer',\n",
       " 'Swin_B_Weights',\n",
       " 'Swin_S_Weights',\n",
       " 'Swin_T_Weights',\n",
       " 'Swin_V2_B_Weights',\n",
       " 'Swin_V2_S_Weights',\n",
       " 'Swin_V2_T_Weights',\n",
       " 'VGG',\n",
       " 'VGG11_BN_Weights',\n",
       " 'VGG11_Weights',\n",
       " 'VGG13_BN_Weights',\n",
       " 'VGG13_Weights',\n",
       " 'VGG16_BN_Weights',\n",
       " 'VGG16_Weights',\n",
       " 'VGG19_BN_Weights',\n",
       " 'VGG19_Weights',\n",
       " 'ViT_B_16_Weights',\n",
       " 'ViT_B_32_Weights',\n",
       " 'ViT_H_14_Weights',\n",
       " 'ViT_L_16_Weights',\n",
       " 'ViT_L_32_Weights',\n",
       " 'VisionTransformer',\n",
       " 'Weights',\n",
       " 'WeightsEnum',\n",
       " 'Wide_ResNet101_2_Weights',\n",
       " 'Wide_ResNet50_2_Weights',\n",
       " '_GoogLeNetOutputs',\n",
       " '_InceptionOutputs',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_api',\n",
       " '_meta',\n",
       " '_utils',\n",
       " 'alexnet',\n",
       " 'convnext',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'detection',\n",
       " 'efficientnet',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'get_model',\n",
       " 'get_model_builder',\n",
       " 'get_model_weights',\n",
       " 'get_weight',\n",
       " 'googlenet',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'list_models',\n",
       " 'maxvit',\n",
       " 'maxvit_t',\n",
       " 'mnasnet',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mobilenetv2',\n",
       " 'mobilenetv3',\n",
       " 'optical_flow',\n",
       " 'quantization',\n",
       " 'regnet',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'segmentation',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'shufflenetv2',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_transformer',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'video',\n",
       " 'vision_transformer',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "ed0411d0-76bb-4481-98bb-60fbebe15401",
   "metadata": {
    "id": "ed0411d0-76bb-4481-98bb-60fbebe15401",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:29.785592Z",
     "start_time": "2025-05-22T17:39:29.592536Z"
    }
   },
   "source": [
    "model = models.resnet18()\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "ab83b536-cc20-40cf-b552-af9a076c17ff",
   "metadata": {
    "id": "ab83b536-cc20-40cf-b552-af9a076c17ff"
   },
   "source": [
    "### Klasyfikacja binarna - przygotowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8f4ae-3303-4018-8842-a18f8c1bddea",
   "metadata": {
    "id": "a1d8f4ae-3303-4018-8842-a18f8c1bddea"
   },
   "source": [
    "Będziemy trenować model do klasyfikacji binarnej zdjęć pszczół i mrówek z wykorzystaniem transfer learningu.\n",
    "\n",
    "Zbiór ten jest dostępny do pobrania tutaj: https://download.pytorch.org/tutorial/hymenoptera_data.zip  \n",
    "(*hymenoptera* - *błonoskrzydłe* https://pl.wikipedia.org/wiki/B%C5%82onkoskrzyd%C5%82e)\n",
    "\n",
    "Zbiór ten należy rozpakować do katalogu `common/data` (w razie gdyby go jeszcze tam nie było)."
   ]
  },
  {
   "cell_type": "code",
   "id": "b53f09e1-b7bc-4e14-8023-da743017080a",
   "metadata": {
    "id": "b53f09e1-b7bc-4e14-8023-da743017080a",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:29.801966Z",
     "start_time": "2025-05-22T17:39:29.799637Z"
    }
   },
   "source": [
    "import pathlib\n",
    "\n",
    "DATA_PATH = pathlib.Path(\"data/hymenoptera_data\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "409a0b4a-c05e-4b07-bde0-3233203111fb",
   "metadata": {
    "id": "409a0b4a-c05e-4b07-bde0-3233203111fb"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Wczytaj zbiór zdjęć (dwa podbiory - train i val) wykorzystując klasę **ImageFolder** dataset dostępną w PyTorch (https://pytorch.org/vision/stable/datasets.html)\n",
    "\n",
    "Sprawdź rozmiar obu podzbiorów.  \n",
    "Sprawdź wymiary kilku wybranych zdjęć w zbiorze (używając możliwości klasy `Dataset`, nie przeglądając obrazki w katalogu).\n",
    "\n",
    "Sprawdź zawartość atrybutów klasy `ImageFolder` (https://pytorch.org/vision/stable/_modules/torchvision/datasets/folder.html#ImageFolder)"
   ]
  },
  {
   "cell_type": "code",
   "id": "e2bde3ff-787b-432b-b917-6e3b04099e36",
   "metadata": {
    "id": "e2bde3ff-787b-432b-b917-6e3b04099e36",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:29.861974Z",
     "start_time": "2025-05-22T17:39:29.857252Z"
    }
   },
   "source": [
    "transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "train_path = DATA_PATH / \"train\"\n",
    "val_path = DATA_PATH / \"val\"\n",
    "train_dataset = ImageFolder(train_path, transform=transform)\n",
    "val_dataset = ImageFolder(val_path, transform=transform)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "b343e503-c8f0-47c3-96af-72f269024bea",
   "metadata": {
    "id": "b343e503-c8f0-47c3-96af-72f269024bea",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:29.938914Z",
     "start_time": "2025-05-22T17:39:29.907626Z"
    }
   },
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "train_image, train_label = train_dataset[0]\n",
    "print(train_image.size())\n",
    "val_image, val_label = val_dataset[0]\n",
    "print(val_image.size())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n",
      "153\n",
      "torch.Size([3, 512, 768])\n",
      "torch.Size([3, 375, 500])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "b9ecb958-4df9-4ec3-b739-1158ec84ff9f",
   "metadata": {
    "id": "b9ecb958-4df9-4ec3-b739-1158ec84ff9f",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:29.974809Z",
     "start_time": "2025-05-22T17:39:29.970240Z"
    }
   },
   "source": "dir(ImageFolder)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_format_transform_repr',\n",
       " '_repr_indent',\n",
       " 'extra_repr',\n",
       " 'find_classes',\n",
       " 'make_dataset']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "3440cc1d-caba-4b52-ab95-1406ca9fd5f0",
   "metadata": {
    "id": "3440cc1d-caba-4b52-ab95-1406ca9fd5f0"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Domyślnie `ImageFolder` dataset przechowuje obrazki jako `PIL.Image`. Należy je przekształcić do tensorów, aby ich użyć w treningu modeli.\n",
    "\n",
    "Odszukaj odpowiednią funkcję z `torchvision.transforms` (https://pytorch.org/vision/stable/transforms.html) i ponownie wczytaj zbiory danych z jej wykorzystaniem."
   ]
  },
  {
   "cell_type": "code",
   "id": "e07b35de-2e0a-4d91-878e-e8bb85414199",
   "metadata": {
    "id": "e07b35de-2e0a-4d91-878e-e8bb85414199",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:30.042478Z",
     "start_time": "2025-05-22T17:39:30.037747Z"
    }
   },
   "source": [
    "transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "train_path = DATA_PATH / \"train\"\n",
    "val_path = DATA_PATH / \"val\"\n",
    "train_dataset = ImageFolder(train_path, transform=transform)\n",
    "val_dataset = ImageFolder(val_path, transform=transform)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "ed6f8d25-eda5-404a-aa6f-0ef9107d0a64",
   "metadata": {
    "id": "ed6f8d25-eda5-404a-aa6f-0ef9107d0a64"
   },
   "source": [
    "#### Ćwiczenie\n",
    "W kolejnym kroku należy znormalizować wejściowe obrazki. Ponownie odszukaj odpowiednią transformację w `torchvision.transforms` i zbuduj listy transformacji `train_transforms` i `valid_transforms` z użyciem `transforms.Compose`. Wykorzystaj odpowiednie informacje z poniższej komórki."
   ]
  },
  {
   "cell_type": "code",
   "id": "dcf64ade-5906-4c2e-8381-9d19685e41cb",
   "metadata": {
    "id": "dcf64ade-5906-4c2e-8381-9d19685e41cb",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:30.105793Z",
     "start_time": "2025-05-22T17:39:30.103024Z"
    }
   },
   "source": [
    "# średnie i odchylenia standardowe dla kanałów RGB dla zbioru uczącego ImageNet\n",
    "# ciekawostka: https://github.com/pytorch/vision/issues/1439\n",
    "IMAGENET_MEANS = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "aa2162a6-e7e9-44d7-a51c-381945613f87",
   "metadata": {
    "id": "aa2162a6-e7e9-44d7-a51c-381945613f87",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:30.159300Z",
     "start_time": "2025-05-22T17:39:30.156009Z"
    }
   },
   "source": [
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=IMAGENET_MEANS, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=IMAGENET_MEANS, std=IMAGENET_STD)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "89a553af-5f09-4116-94a2-cfaf9ae95c9f",
   "metadata": {
    "id": "89a553af-5f09-4116-94a2-cfaf9ae95c9f"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Należy także doprowadzić oryginalne obrazki do odpowiednich wymiarów (224 na 224 piksele).\n",
    "\n",
    "W przypadku sieci (pre)trenowanych na danych ImageNet przyjęło się robić to dwukrokowo:\n",
    "- \"resize\" obrazka, aby krótszy wymiar miał długość 256\n",
    "- przycięcie (\"crop\") obrazka do jego środkowej części 224x224\n",
    "\n",
    "Rozszerz listę transformacji **podczas walidacji** zgodnie z powyższym opisem, wykorzystując odpowiednie funkcje z https://pytorch.org/vision/stable/transforms.html. Transformacjami treningowymi zajmiemy się w następnym ćwiczeniu."
   ]
  },
  {
   "cell_type": "code",
   "id": "1f588da1-71ad-46be-8e02-a1162d432dd1",
   "metadata": {
    "id": "1f588da1-71ad-46be-8e02-a1162d432dd1",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:30.259489Z",
     "start_time": "2025-05-22T17:39:30.256505Z"
    }
   },
   "source": [
    "IMAGENET_IMG_SIZE = 224\n",
    "IMAGENET_RESIZE = 256"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "88ad810b-dd2f-4f4c-baf1-fbc311a7c969",
   "metadata": {
    "id": "88ad810b-dd2f-4f4c-baf1-fbc311a7c969",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:30.317015Z",
     "start_time": "2025-05-22T17:39:30.314253Z"
    }
   },
   "source": [
    "val_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize(IMAGENET_RESIZE),\n",
    "    v2.CenterCrop(IMAGENET_IMG_SIZE),\n",
    "    v2.Normalize(mean=IMAGENET_MEANS, std=IMAGENET_STD)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "7c86cdc9-1b9b-4c80-b45a-5ff123591fcf",
   "metadata": {
    "id": "7c86cdc9-1b9b-4c80-b45a-5ff123591fcf"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Podczas treningu warto - zwłaszcza w przypadku posiadania niewielkiego zbioru danych - zastosować tzw. augmentację danych (więcej na kolejnych zajęciach).\n",
    "\n",
    "Zamiast \"sztywnego\" resize'owania obrazka i przycinania go względem środka, w czasie treningu:\n",
    "- dokonaj \"resize\" do losowej skali, a następnie przytnij do (losowego) fragmentu 224x224\n",
    "- dodatkowo losowo (domyślnie: prawdopodobieństwo 50%) przerzuć obrazek względem osi pionowej\n",
    "\n",
    "Znajdź odpowiednie funkcje w https://pytorch.org/vision/stable/transforms.html.\n",
    "Stwórz w ten sposób listę transformacji `train_transforms`.\n",
    "\n",
    "Wczytaj ponownie zbiór uczący i walidacyjny, podając odpowiednie listy transformacji do `ImageFolder`."
   ]
  },
  {
   "cell_type": "code",
   "id": "fcdf0f6f-45c1-4ca9-bc9a-7ac1f982308b",
   "metadata": {
    "id": "fcdf0f6f-45c1-4ca9-bc9a-7ac1f982308b",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:30.376814Z",
     "start_time": "2025-05-22T17:39:30.372991Z"
    }
   },
   "source": [
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomResizedCrop(IMAGENET_IMG_SIZE),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.Normalize(mean=IMAGENET_MEANS, std=IMAGENET_STD)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "f9c67625-2098-42e4-9cd7-e369297997a7",
   "metadata": {
    "id": "f9c67625-2098-42e4-9cd7-e369297997a7",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:30.432037Z",
     "start_time": "2025-05-22T17:39:30.428647Z"
    }
   },
   "source": [
    "train_dataset = ImageFolder(train_path, transform=train_transforms)\n",
    "val_dataset = ImageFolder(val_path, transform=val_transforms)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "51d410ce-76d7-450c-bc48-f0ccb273202c",
   "metadata": {
    "id": "51d410ce-76d7-450c-bc48-f0ccb273202c"
   },
   "source": [
    "### Transfer learning\n",
    "\n",
    "Fine-tuningu modeli można dokonać na dwa główne sposoby:\n",
    "- dotrenować (optymalizować) wszystkie parametry (we wszystkich warstwach) pretrenowanego modelu\n",
    "- \"zamrozić\" pretrenowaną część modelu i dotrenować\n",
    "\n",
    "Na początek zajmiemy się pierwszym z wymienionych sposobów transfer learningu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a00d07-86d8-4586-ae22-1c0e29d39327",
   "metadata": {
    "id": "83a00d07-86d8-4586-ae22-1c0e29d39327"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Załaduj pretrenowaną sieć `resnet18` do zmiennej `model` i dostosuj ją do rozważanego problemu (co musisz zrobić?).\n",
    "Następnie uruchom trening modelu."
   ]
  },
  {
   "cell_type": "code",
   "id": "4502d424-5eff-4f71-a0f9-6fadc478cf71",
   "metadata": {
    "id": "4502d424-5eff-4f71-a0f9-6fadc478cf71",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:30.487505Z",
     "start_time": "2025-05-22T17:39:30.483184Z"
    }
   },
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        epoch_loss = loss_train / len(train_loader)\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {epoch_loss}\")\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "85b03e84-6ea0-4c03-bdd6-e5b7de4cb0d4",
   "metadata": {
    "id": "85b03e84-6ea0-4c03-bdd6-e5b7de4cb0d4",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:39:30.915571Z",
     "start_time": "2025-05-22T17:39:30.536823Z"
    }
   },
   "source": "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "3a940c0c-930d-46e8-b412-247177665ba4",
   "metadata": {
    "id": "3a940c0c-930d-46e8-b412-247177665ba4",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:40:10.976897Z",
     "start_time": "2025-05-22T17:39:30.933789Z"
    }
   },
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# SGD with momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 25,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 5.311984598636627\n",
      "Epoch 5, Training loss 0.36217445135116577\n",
      "Epoch 10, Training loss 0.2655116282403469\n",
      "Epoch 15, Training loss 0.17122965026646852\n",
      "Epoch 20, Training loss 0.11138878762722015\n",
      "Epoch 25, Training loss 0.04979503434151411\n",
      "Training complete in 0m 40s\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "bf088a4a-741a-42d5-a9ba-c39f02bb7bde",
   "metadata": {
    "id": "bf088a4a-741a-42d5-a9ba-c39f02bb7bde"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Sprawdź jakość wytrenowanego modelu uruchamiając poniższe komórki."
   ]
  },
  {
   "cell_type": "code",
   "id": "4dfe483d-5c1c-400a-b556-feefa4558756",
   "metadata": {
    "id": "4dfe483d-5c1c-400a-b556-feefa4558756",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:40:10.992214Z",
     "start_time": "2025-05-22T17:40:10.989723Z"
    }
   },
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "4f327f88-2f78-48a6-9ad6-a5b0e835d025",
   "metadata": {
    "id": "4f327f88-2f78-48a6-9ad6-a5b0e835d025",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:40:11.050681Z",
     "start_time": "2025-05-22T17:40:11.045695Z"
    }
   },
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    model.eval()\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((preds == labels).sum())\n",
    "\n",
    "        print(f\"{name} accuracy: {correct/total}\")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "878bfe02-f79e-4450-993c-0256d74d0368",
   "metadata": {
    "id": "878bfe02-f79e-4450-993c-0256d74d0368",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:40:13.268975Z",
     "start_time": "2025-05-22T17:40:11.098926Z"
    }
   },
   "source": [
    "validate(model, train_loader, val_loader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9795081967213115\n",
      "val accuracy: 0.8562091503267973\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "13af6f0f-aa61-4742-9d3b-ecd4c400084c",
   "metadata": {
    "id": "13af6f0f-aa61-4742-9d3b-ecd4c400084c"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Jeszcze raz załaduj i przygotuj model `resnet18` (np. do zmiennej `model_frozen`, tym razem \"zamrażając\" wszystkie pretrenowane warstwy modelu.\n",
    "Wytrenuj model i sprawdź jego dokładność."
   ]
  },
  {
   "cell_type": "code",
   "id": "2eac2545-1d4e-4ba4-8056-36016db0d3f9",
   "metadata": {
    "id": "2eac2545-1d4e-4ba4-8056-36016db0d3f9",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:40:13.439851Z",
     "start_time": "2025-05-22T17:40:13.284014Z"
    }
   },
   "source": [
    "model_frozen = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)\n",
    "for param in model_frozen.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_frozen.fc.parameters():\n",
    "    param.requires_grad = True"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "090e74b8-5298-481c-ada4-df0e460ee0a3",
   "metadata": {
    "id": "090e74b8-5298-481c-ada4-df0e460ee0a3",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:40:40.218604Z",
     "start_time": "2025-05-22T17:40:13.518060Z"
    }
   },
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# SGD with momentum\n",
    "optimizer = torch.optim.SGD(model_frozen.parameters(), lr=1e-2, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 25,\n",
    "    optimizer = optimizer,\n",
    "    model = model_frozen,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 5.883200943470001\n",
      "Epoch 5, Training loss 0.33941885456442833\n",
      "Epoch 10, Training loss 0.2079898789525032\n",
      "Epoch 15, Training loss 0.13808961398899555\n",
      "Epoch 20, Training loss 0.11620101425796747\n",
      "Epoch 25, Training loss 0.1448836624622345\n",
      "Training complete in 0m 27s\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "e46b7d01-8126-4b1e-86d7-5c177812e55c",
   "metadata": {
    "id": "e46b7d01-8126-4b1e-86d7-5c177812e55c",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:40:40.254340Z",
     "start_time": "2025-05-22T17:40:40.251896Z"
    }
   },
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "a2d15f3b-a05b-4df7-9a10-600107529add",
   "metadata": {
    "id": "a2d15f3b-a05b-4df7-9a10-600107529add",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:40:42.233968Z",
     "start_time": "2025-05-22T17:40:40.331943Z"
    }
   },
   "source": [
    "validate(model_frozen, train_loader, val_loader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9631147540983607\n",
      "val accuracy: 0.9411764705882353\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "c8a80497-5b7b-4408-8386-ec185f3c1c37",
   "metadata": {
    "id": "c8a80497-5b7b-4408-8386-ec185f3c1c37"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Porównaj powyższe wyniki z uzyskanymi dla modelu `Net` stworzonego na wcześniejszych zajęciach (lekko zmodyfikowane wymiary dla warstw gęstych - inny rozmiar obrazków wejściowych)."
   ]
  },
  {
   "cell_type": "code",
   "id": "9dcc4290-a886-4b7e-8746-e3babacff921",
   "metadata": {
    "id": "9dcc4290-a886-4b7e-8746-e3babacff921",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:40:42.266670Z",
     "start_time": "2025-05-22T17:40:42.263091Z"
    }
   },
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 56 * 56, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 56 * 56)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "18f9fe6d-2fee-480d-8acd-c9c5401c7d0b",
   "metadata": {
    "id": "18f9fe6d-2fee-480d-8acd-c9c5401c7d0b",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:41:06.359624Z",
     "start_time": "2025-05-22T17:40:42.338466Z"
    }
   },
   "source": [
    "net_model = Net()\n",
    "net_model = net_model.to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# SGD with momentum\n",
    "optimizer = torch.optim.SGD(net_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 25,\n",
    "    optimizer = optimizer,\n",
    "    model = net_model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.6892561465501785\n",
      "Epoch 5, Training loss 0.6995721310377121\n",
      "Epoch 10, Training loss 0.6391260623931885\n",
      "Epoch 15, Training loss 0.606040894985199\n",
      "Epoch 20, Training loss 0.5905006229877472\n",
      "Epoch 25, Training loss 0.5931582301855087\n",
      "Training complete in 0m 24s\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "753720a6-5e69-4ccc-afe0-4eb63bfce9ca",
   "metadata": {
    "id": "753720a6-5e69-4ccc-afe0-4eb63bfce9ca",
    "ExecuteTime": {
     "end_time": "2025-05-22T17:41:08.015528Z",
     "start_time": "2025-05-22T17:41:06.393788Z"
    }
   },
   "source": [
    "validate(net_model, train_loader, val_loader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.6598360655737705\n",
      "val accuracy: 0.6339869281045751\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Wnioski\n",
    "Dzięki zajęciom zrozumiałem czym jest transfer learning. Słyszałem już o nim wcześniej w internecie lecz nie wiedziałem, że tak się nazywa. Myślę, iż pozwala na zaoszczędzenie czasu na treningu modelu. Sam planowałem z niego skorzystać w mojej pracy inżynierskiej."
   ],
   "id": "10f07cce56a8e28d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1ab5c1e6e58f733e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

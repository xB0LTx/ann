{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sztuczne sieci neuronowe - laboratorium 3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:04.193468Z",
     "start_time": "2025-03-20T16:22:04.190359Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytania kontrolne\n",
    "\n",
    "1. Jakie znasz funkcje straty (w regresji i w klasyfikacji)?\n",
    "    1. Regresja:\n",
    "        1. Mean Squared Error (MSE)\n",
    "        2. Mean Absolute Error (MAE)\n",
    "        3. Huber Loss\n",
    "    2. Klasyfikacja:\n",
    "        1. Cross-Entropy Loss\n",
    "        2. Binary Cross-Entropy Loss\n",
    "        3. Hinge Loss\n",
    "2. Jakie znasz algorytmy (min. jeden, wraz z wariantami) minimalizacji funkcji straty?\n",
    "    1. Gradient Descent (GD)\n",
    "        1. Stochastic Gradient Descent (SGD)\n",
    "        2. Mini-batch Gradient Descent\n",
    "        3. Batch Gradient Descent\n",
    "    2. Adam (Adaptive Moment Estimation)\n",
    "    3. RMSProp\n",
    "    4. Adagrad\n",
    "3. Czym jest gradient funkcji?\n",
    "    1. Gradient funkcji to wektor zawierający pochodne cząstkowe tej funkcji względem wszystkich jej zmiennych. Wskazuje kierunek najszybszego wzrostu funkcji.\n",
    "4. Wymień znane Ci hiperparametry stosowane w algorytmie najszybszego spadku.\n",
    "    1. Learning rate (współczynnik uczenia)\n",
    "    2. Momentum\n",
    "    3. Batch size\n",
    "    4. Number of iterations (liczba iteracji)\n",
    "    5. Weight decay (regularyzacja)\n",
    "5. Jakie problemy mogą wystąpić przy nieodpowiednim doborze hiperparametrów tego algorytmu?\n",
    "    1. Zbyt duży learning rate: może powodować \"wybuchanie\" gradientów i niestabilność treningu.\n",
    "    2. Zbyt mały learning rate: może prowadzić do bardzo wolnej konwergencji.\n",
    "    3. Brak odpowiedniego momentum: może powodować utknięcie w lokalnych minimach.\n",
    "    4. Zbyt duży batch size: może prowadzić do problemów z pamięcią.\n",
    "    5. Zbyt mały batch size: może prowadzić do niestabilnych aktualizacji gradientów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przypomnienie poprzedniego ćwiczenia\n",
    "\n",
    "Poprzednie ćwiczenie:\n",
    "> Załóżmy, że mamy dwa termometry:\n",
    "> - jeden mierzy temperaturę w stopniach Celsjusza\n",
    "> - drugi mierzy temperaturę w nieznanej nam skali, ale jest bardzo ładny i chcemy go powiesić na ścianie\n",
    ">\n",
    "> Zanim to zrobimy, chcemy się dowiedzieć, jak przeliczać wskazania drugiego termometru na stopnie Celsjusza.\n",
    "> Spróbujemy znaleźć \"wzór\" tego przekształcenia na podstawie pomiarów dokonanych obydwoma termometrami.\n",
    "\n",
    "W poprzednim ćwiczeniu:\n",
    "- na podstawie wizualizacji danych wybraliśmy do tego problemu model liniowy\n",
    "- zaimplementowaliśmy go jako funkcję `model` (dwa parametry: `w` i `b`)\n",
    "- zdefiniowaliśmy funkcję straty dla regresji liniowej - błąd średniokwadratowy (funkcja `loss_fn`)\n",
    "- na potrzeby obliczenia gradientu funkcji straty (wektora pochodnych względem parametrów modelu, `dL/dw`) zdefiniowailśmy funkcje obliczające poszczególne komponenty reguły łańcuchowej (`dL/dw = (dL/dtp) * (dtp/dw)`)\n",
    "- podjęliśmy próbę minimalizacji funkcji straty algorytmem najszybszego spadku\n",
    "- eksperymentowaliśmy z wartością stałej uczącej (`learning_rate`)\n",
    "- zauważyliśmy konieczność normalizacji danych\n",
    "- na podstawie wyznaczonych wartości parametrów znaleźliśmy przekształcenie ze skali Fahrenheita do skali Celsjusza\n",
    "\n",
    "Dzisiaj dowiemy się, jak niektóre z tych kroków zrealizować \"automatycznie\" z użyciem PyTorch `autograd` i `torch.optim`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:04.959009Z",
     "start_time": "2025-03-20T16:22:04.955171Z"
    }
   },
   "source": [
    "data_unknown = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "data_celsius = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "\n",
    "t_u = torch.tensor(data_unknown)\n",
    "t_c = torch.tensor(data_celsius)\n"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:05.108337Z",
     "start_time": "2025-03-20T16:22:05.104850Z"
    }
   },
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:05.217943Z",
     "start_time": "2025-03-20T16:22:05.214082Z"
    }
   },
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c) ** 2\n",
    "    return squared_diffs.mean()"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizacja danych\n",
    "\n",
    "W poprzednim ćwiczeniu, aby algorytm najszybszego spadku nie \"wybuchł\", należało znormalizować dane wejściowe. Zgodnie z oryginalnym przykładem z książki \"Deep Learning with PyTorch\" normalizacja polegała na dziesięciokrotnym pomniejszeniu wejść (aby sprowadzić dane do \"bezpieczniejszego\" przedziału). Ta niestandardowa normalizacja wprowadziła jednak nieco niepotrzebnego chaosu do zajęć.\n",
    "\n",
    "Dlatego dzisiaj zrobimy inaczej - poddamy dane wejściowe standaryzacji.\n",
    "\n",
    "\n",
    "#### Ćwiczenie\n",
    "Dokonaj standaryzacji danych wejściowych i przypisz je do tensora `t_un`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:05.730075Z",
     "start_time": "2025-03-20T16:22:05.726530Z"
    }
   },
   "source": "t_un = (t_u - t_u.mean()) / t_u.std()",
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch autograd\n",
    "\n",
    "W przypadku regresji liniowej dla jednej zmiennej wejściowej i dwóch parametrów, \"ręczne\" obliczenie pochodnych nie było szczególnie uciążliwe. Dla bardziej skomplikowanych modeli (a takimi będą sieci neuronowe) warto użyć narzędzia, które może wykonać tę pracę za nas.\n",
    "\n",
    "`autograd` to wbudowany w PyTorch silnik do obliczania pochodnych funkcji - także złożonych.\n",
    "\n",
    "Poza przechowywaniem wartości liczbowych, tensory w PyTorch mogą zapamiętywać, poprzez jakie operacje i z których (innych) tensorów powstały, tworząc strukturę grafu obliczeń (\"computation graph\"). Na podstawie tego grafu `autograd` może następnie obliczyć pochodne względem poszczególnych parametrów modelu.\n",
    "\n",
    "Wartości pochodnych będą zapisane w atrybucie `.grad` tensora. Aby włączyć działanie `autograd` dla konkretnego tensora (oraz wszystkich tensorów, które z niego powstają w wyniku różnych operacji), należy przy tworzeniu go podać argument `requires_grad=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Stwórz tensor `params` zawierający początkowe wartości parametrów: w = 1 i b = 0. Zadbaj o włączenie `autograd` dla tego tensora. Sprawdź wartość `params.grad` zaraz po utworzeniu tensora."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:07.538669Z",
     "start_time": "2025-03-20T16:22:07.534910Z"
    }
   },
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "print(params.grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktualizacja gradientów\n",
    "\n",
    "Zawartość atrybutu `grad` aktualizuje się po wywołaniu funkcji `loss.backward()` (gdzie `loss` to tensor reprezentujący  funkcję straty po przejściu \"w przód\" przez graf).\n",
    "\n",
    "Wyjaśnienie:  \n",
    "Podczas obliczania funkcji straty (gdy dla `params` ustawimy `requires_grad=True`), poza samymi obliczeniami tworzy się graf reprezentujący poszczególne operacje jako wierzchołki (końcowym wierzchołkiem jest `loss`). Po wywołaniu `loss.backward()` graf przetwarzany jest wstecz i obliczane są pochodne względem poszczególnych parametrów.\n",
    "\n",
    "#### Ćwiczenie\n",
    "Użyj przygotowanych funkcji `model` i `loss_fn`, aby stworzyć graf obliczeń i uzyskać tensor reprezentujący funkcję straty, a następnie uruchom na nim metodę `backward()`. Sprawdź wartość `params.grad`. Sprawdź też (np. z użyciem `print`), jaka jest zawartość tensorów funkcji straty i predykcji."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:08.847039Z",
     "start_time": "2025-03-20T16:22:08.841432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model(t_u, *params))\n",
    "print(loss_fn(model(t_u, *params), t_c).backward())\n",
    "print(params.grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
      "        48.4000, 60.4000, 68.4000], grad_fn=<AddBackward0>)\n",
      "None\n",
      "tensor([4517.2969,   82.6000])\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akumulacja gradientów\n",
    "Uwaga:  \n",
    "Ze względów praktycznych (np. ze względu na zastosowanie w przypadku ograniczonej pamięci), każde wywołanie `loss.backward()` powoduje akumulację  gradientów (dodanie nowo obliczonych do już istniejących wartości w `.grad`, a nie nadpisanie). Zwykle pożądanym zachowaniem jest jednak użycie jedynie gradientów z aktualnej iteracji. W tym celu należy \"wyzerować\" gradienty, np. na początku każdej iteracji algorytmu optymalizacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Do wyzerowania gradientów na początku każdej iteracji można użyć funkcji `.zero()`.\n",
    "Wyzeruj gradienty tensora `params`. Pomyśl, jak zabezpieczyć się przed błędem, który może się pojawić w pierwszej iteracji algorytmu optymalizacji.\n",
    "\n",
    "Uwaga:  \n",
    "W PyTorchu obowiązuje konwencja, że funkcje z `_` na końcu nazwy służą do wywoływania operacji \"w miejscu\" (in-place) - nie tworzą kopii tensora / widoku na pamieć, ale zmieniają jej zawartość. W ćwiczeniu użyj wariantu \"in-place\" funkcji zerującej tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:10.643962Z",
     "start_time": "2025-03-20T16:22:10.638841Z"
    }
   },
   "source": "params.grad.zero_()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Korzystając z kodu funkcji `training_loop` z poprzedniego ćwiczenia (przeklejonej poniżej), napisz funkcję `training_loop_autograd`, w której ręczne obliczanie gradientu zastąpi użycie silnika `autograd`.\n",
    "\n",
    "Uwaga:  \n",
    "Krok aktualizacji tensora parametrów należy dodatkowo zamknąć w bloku `with torch.no_grad()`: (aby \"na chwilę\" wyłączyć autograd na potrzeby zmiany zawartości tensora `params`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:12.498478Z",
     "start_time": "2025-03-20T16:22:12.493197Z"
    }
   },
   "source": [
    "def training_loop(n_iters, learning_rate, params, t_u, t_c):\n",
    "    for iteration in range(1, n_iters+1):\n",
    "        w, b = params\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "        \n",
    "        # tzw. \"vanilla\" gradient descent        \n",
    "        params = params - learning_rate * grad\n",
    "        \n",
    "        print('After iteration %d, Loss %f' % (iteration, float(loss)))\n",
    "    \n",
    "    return params"
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:13.153842Z",
     "start_time": "2025-03-20T16:22:13.148473Z"
    }
   },
   "source": [
    "def training_loop_autograd(n_iters, learning_rate, params, t_u, t_c):\n",
    "    for iteration in range(1, n_iters+1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "            params.grad.zero_()\n",
    "\n",
    "        print('After iteration %d, Loss %f' % (iteration, float(loss)))\n",
    "\n",
    "    return params"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Uruchom `training_loop_autograd` dla 1000 iteracji, ustaw `learning_rate = 5e-3` i jako dane wejściowe podaj znormalizowany tensor `t_un`. Zainicjalizuj parametry tensorem [1,0, 0.0] (włącz dla niego autograd)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:19.568807Z",
     "start_time": "2025-03-20T16:22:19.360339Z"
    }
   },
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "training_loop_autograd(1000, 5e-3, params, t_un, t_c)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After iteration 1, Loss 171.868347\n",
      "After iteration 2, Loss 168.612122\n",
      "After iteration 3, Loss 165.418777\n",
      "After iteration 4, Loss 162.287109\n",
      "After iteration 5, Loss 159.215927\n",
      "After iteration 6, Loss 156.204025\n",
      "After iteration 7, Loss 153.250275\n",
      "After iteration 8, Loss 150.353577\n",
      "After iteration 9, Loss 147.512817\n",
      "After iteration 10, Loss 144.726883\n",
      "After iteration 11, Loss 141.994751\n",
      "After iteration 12, Loss 139.315399\n",
      "After iteration 13, Loss 136.687714\n",
      "After iteration 14, Loss 134.110779\n",
      "After iteration 15, Loss 131.583588\n",
      "After iteration 16, Loss 129.105179\n",
      "After iteration 17, Loss 126.674629\n",
      "After iteration 18, Loss 124.290962\n",
      "After iteration 19, Loss 121.953300\n",
      "After iteration 20, Loss 119.660767\n",
      "After iteration 21, Loss 117.412483\n",
      "After iteration 22, Loss 115.207573\n",
      "After iteration 23, Loss 113.045235\n",
      "After iteration 24, Loss 110.924606\n",
      "After iteration 25, Loss 108.844902\n",
      "After iteration 26, Loss 106.805344\n",
      "After iteration 27, Loss 104.805122\n",
      "After iteration 28, Loss 102.843498\n",
      "After iteration 29, Loss 100.919708\n",
      "After iteration 30, Loss 99.033058\n",
      "After iteration 31, Loss 97.182785\n",
      "After iteration 32, Loss 95.368210\n",
      "After iteration 33, Loss 93.588631\n",
      "After iteration 34, Loss 91.843407\n",
      "After iteration 35, Loss 90.131821\n",
      "After iteration 36, Loss 88.453262\n",
      "After iteration 37, Loss 86.807076\n",
      "After iteration 38, Loss 85.192635\n",
      "After iteration 39, Loss 83.609322\n",
      "After iteration 40, Loss 82.056557\n",
      "After iteration 41, Loss 80.533745\n",
      "After iteration 42, Loss 79.040283\n",
      "After iteration 43, Loss 77.575630\n",
      "After iteration 44, Loss 76.139206\n",
      "After iteration 45, Loss 74.730492\n",
      "After iteration 46, Loss 73.348930\n",
      "After iteration 47, Loss 71.994003\n",
      "After iteration 48, Loss 70.665207\n",
      "After iteration 49, Loss 69.362022\n",
      "After iteration 50, Loss 68.083954\n",
      "After iteration 51, Loss 66.830536\n",
      "After iteration 52, Loss 65.601265\n",
      "After iteration 53, Loss 64.395699\n",
      "After iteration 54, Loss 63.213367\n",
      "After iteration 55, Loss 62.053833\n",
      "After iteration 56, Loss 60.916641\n",
      "After iteration 57, Loss 59.801357\n",
      "After iteration 58, Loss 58.707581\n",
      "After iteration 59, Loss 57.634876\n",
      "After iteration 60, Loss 56.582840\n",
      "After iteration 61, Loss 55.551083\n",
      "After iteration 62, Loss 54.539211\n",
      "After iteration 63, Loss 53.546818\n",
      "After iteration 64, Loss 52.573559\n",
      "After iteration 65, Loss 51.619053\n",
      "After iteration 66, Loss 50.682919\n",
      "After iteration 67, Loss 49.764843\n",
      "After iteration 68, Loss 48.864441\n",
      "After iteration 69, Loss 47.981384\n",
      "After iteration 70, Loss 47.115341\n",
      "After iteration 71, Loss 46.265976\n",
      "After iteration 72, Loss 45.432980\n",
      "After iteration 73, Loss 44.616013\n",
      "After iteration 74, Loss 43.814785\n",
      "After iteration 75, Loss 43.028992\n",
      "After iteration 76, Loss 42.258327\n",
      "After iteration 77, Loss 41.502506\n",
      "After iteration 78, Loss 40.761246\n",
      "After iteration 79, Loss 40.034248\n",
      "After iteration 80, Loss 39.321259\n",
      "After iteration 81, Loss 38.621994\n",
      "After iteration 82, Loss 37.936195\n",
      "After iteration 83, Loss 37.263596\n",
      "After iteration 84, Loss 36.603947\n",
      "After iteration 85, Loss 35.956993\n",
      "After iteration 86, Loss 35.322498\n",
      "After iteration 87, Loss 34.700222\n",
      "After iteration 88, Loss 34.089920\n",
      "After iteration 89, Loss 33.491364\n",
      "After iteration 90, Loss 32.904331\n",
      "After iteration 91, Loss 32.328590\n",
      "After iteration 92, Loss 31.763939\n",
      "After iteration 93, Loss 31.210152\n",
      "After iteration 94, Loss 30.667021\n",
      "After iteration 95, Loss 30.134346\n",
      "After iteration 96, Loss 29.611914\n",
      "After iteration 97, Loss 29.099543\n",
      "After iteration 98, Loss 28.597027\n",
      "After iteration 99, Loss 28.104170\n",
      "After iteration 100, Loss 27.620808\n",
      "After iteration 101, Loss 27.146740\n",
      "After iteration 102, Loss 26.681791\n",
      "After iteration 103, Loss 26.225786\n",
      "After iteration 104, Loss 25.778557\n",
      "After iteration 105, Loss 25.339933\n",
      "After iteration 106, Loss 24.909740\n",
      "After iteration 107, Loss 24.487829\n",
      "After iteration 108, Loss 24.074024\n",
      "After iteration 109, Loss 23.668188\n",
      "After iteration 110, Loss 23.270149\n",
      "After iteration 111, Loss 22.879770\n",
      "After iteration 112, Loss 22.496895\n",
      "After iteration 113, Loss 22.121386\n",
      "After iteration 114, Loss 21.753096\n",
      "After iteration 115, Loss 21.391884\n",
      "After iteration 116, Loss 21.037621\n",
      "After iteration 117, Loss 20.690166\n",
      "After iteration 118, Loss 20.349394\n",
      "After iteration 119, Loss 20.015171\n",
      "After iteration 120, Loss 19.687378\n",
      "After iteration 121, Loss 19.365889\n",
      "After iteration 122, Loss 19.050573\n",
      "After iteration 123, Loss 18.741318\n",
      "After iteration 124, Loss 18.438011\n",
      "After iteration 125, Loss 18.140532\n",
      "After iteration 126, Loss 17.848768\n",
      "After iteration 127, Loss 17.562616\n",
      "After iteration 128, Loss 17.281963\n",
      "After iteration 129, Loss 17.006701\n",
      "After iteration 130, Loss 16.736732\n",
      "After iteration 131, Loss 16.471949\n",
      "After iteration 132, Loss 16.212254\n",
      "After iteration 133, Loss 15.957549\n",
      "After iteration 134, Loss 15.707737\n",
      "After iteration 135, Loss 15.462726\n",
      "After iteration 136, Loss 15.222418\n",
      "After iteration 137, Loss 14.986732\n",
      "After iteration 138, Loss 14.755570\n",
      "After iteration 139, Loss 14.528853\n",
      "After iteration 140, Loss 14.306489\n",
      "After iteration 141, Loss 14.088397\n",
      "After iteration 142, Loss 13.874491\n",
      "After iteration 143, Loss 13.664697\n",
      "After iteration 144, Loss 13.458929\n",
      "After iteration 145, Loss 13.257113\n",
      "After iteration 146, Loss 13.059177\n",
      "After iteration 147, Loss 12.865039\n",
      "After iteration 148, Loss 12.674628\n",
      "After iteration 149, Loss 12.487876\n",
      "After iteration 150, Loss 12.304708\n",
      "After iteration 151, Loss 12.125057\n",
      "After iteration 152, Loss 11.948853\n",
      "After iteration 153, Loss 11.776032\n",
      "After iteration 154, Loss 11.606530\n",
      "After iteration 155, Loss 11.440284\n",
      "After iteration 156, Loss 11.277226\n",
      "After iteration 157, Loss 11.117300\n",
      "After iteration 158, Loss 10.960444\n",
      "After iteration 159, Loss 10.806596\n",
      "After iteration 160, Loss 10.655707\n",
      "After iteration 161, Loss 10.507710\n",
      "After iteration 162, Loss 10.362550\n",
      "After iteration 163, Loss 10.220179\n",
      "After iteration 164, Loss 10.080539\n",
      "After iteration 165, Loss 9.943579\n",
      "After iteration 166, Loss 9.809249\n",
      "After iteration 167, Loss 9.677494\n",
      "After iteration 168, Loss 9.548268\n",
      "After iteration 169, Loss 9.421523\n",
      "After iteration 170, Loss 9.297210\n",
      "After iteration 171, Loss 9.175277\n",
      "After iteration 172, Loss 9.055688\n",
      "After iteration 173, Loss 8.938389\n",
      "After iteration 174, Loss 8.823342\n",
      "After iteration 175, Loss 8.710504\n",
      "After iteration 176, Loss 8.599831\n",
      "After iteration 177, Loss 8.491275\n",
      "After iteration 178, Loss 8.384809\n",
      "After iteration 179, Loss 8.280379\n",
      "After iteration 180, Loss 8.177951\n",
      "After iteration 181, Loss 8.077489\n",
      "After iteration 182, Loss 7.978956\n",
      "After iteration 183, Loss 7.882308\n",
      "After iteration 184, Loss 7.787517\n",
      "After iteration 185, Loss 7.694539\n",
      "After iteration 186, Loss 7.603349\n",
      "After iteration 187, Loss 7.513904\n",
      "After iteration 188, Loss 7.426172\n",
      "After iteration 189, Loss 7.340127\n",
      "After iteration 190, Loss 7.255729\n",
      "After iteration 191, Loss 7.172947\n",
      "After iteration 192, Loss 7.091752\n",
      "After iteration 193, Loss 7.012115\n",
      "After iteration 194, Loss 6.934002\n",
      "After iteration 195, Loss 6.857390\n",
      "After iteration 196, Loss 6.782245\n",
      "After iteration 197, Loss 6.708543\n",
      "After iteration 198, Loss 6.636247\n",
      "After iteration 199, Loss 6.565340\n",
      "After iteration 200, Loss 6.495791\n",
      "After iteration 201, Loss 6.427573\n",
      "After iteration 202, Loss 6.360664\n",
      "After iteration 203, Loss 6.295036\n",
      "After iteration 204, Loss 6.230664\n",
      "After iteration 205, Loss 6.167528\n",
      "After iteration 206, Loss 6.105601\n",
      "After iteration 207, Loss 6.044859\n",
      "After iteration 208, Loss 5.985281\n",
      "After iteration 209, Loss 5.926847\n",
      "After iteration 210, Loss 5.869526\n",
      "After iteration 211, Loss 5.813304\n",
      "After iteration 212, Loss 5.758162\n",
      "After iteration 213, Loss 5.704077\n",
      "After iteration 214, Loss 5.651025\n",
      "After iteration 215, Loss 5.598989\n",
      "After iteration 216, Loss 5.547948\n",
      "After iteration 217, Loss 5.497888\n",
      "After iteration 218, Loss 5.448784\n",
      "After iteration 219, Loss 5.400622\n",
      "After iteration 220, Loss 5.353383\n",
      "After iteration 221, Loss 5.307046\n",
      "After iteration 222, Loss 5.261597\n",
      "After iteration 223, Loss 5.217020\n",
      "After iteration 224, Loss 5.173294\n",
      "After iteration 225, Loss 5.130403\n",
      "After iteration 226, Loss 5.088335\n",
      "After iteration 227, Loss 5.047071\n",
      "After iteration 228, Loss 5.006599\n",
      "After iteration 229, Loss 4.966900\n",
      "After iteration 230, Loss 4.927960\n",
      "After iteration 231, Loss 4.889767\n",
      "After iteration 232, Loss 4.852305\n",
      "After iteration 233, Loss 4.815559\n",
      "After iteration 234, Loss 4.779518\n",
      "After iteration 235, Loss 4.744164\n",
      "After iteration 236, Loss 4.709485\n",
      "After iteration 237, Loss 4.675472\n",
      "After iteration 238, Loss 4.642109\n",
      "After iteration 239, Loss 4.609386\n",
      "After iteration 240, Loss 4.577288\n",
      "After iteration 241, Loss 4.545804\n",
      "After iteration 242, Loss 4.514920\n",
      "After iteration 243, Loss 4.484629\n",
      "After iteration 244, Loss 4.454917\n",
      "After iteration 245, Loss 4.425773\n",
      "After iteration 246, Loss 4.397185\n",
      "After iteration 247, Loss 4.369145\n",
      "After iteration 248, Loss 4.341642\n",
      "After iteration 249, Loss 4.314663\n",
      "After iteration 250, Loss 4.288200\n",
      "After iteration 251, Loss 4.262246\n",
      "After iteration 252, Loss 4.236786\n",
      "After iteration 253, Loss 4.211812\n",
      "After iteration 254, Loss 4.187316\n",
      "After iteration 255, Loss 4.163289\n",
      "After iteration 256, Loss 4.139719\n",
      "After iteration 257, Loss 4.116603\n",
      "After iteration 258, Loss 4.093929\n",
      "After iteration 259, Loss 4.071688\n",
      "After iteration 260, Loss 4.049870\n",
      "After iteration 261, Loss 4.028470\n",
      "After iteration 262, Loss 4.007478\n",
      "After iteration 263, Loss 3.986890\n",
      "After iteration 264, Loss 3.966693\n",
      "After iteration 265, Loss 3.946879\n",
      "After iteration 266, Loss 3.927451\n",
      "After iteration 267, Loss 3.908388\n",
      "After iteration 268, Loss 3.889692\n",
      "After iteration 269, Loss 3.871352\n",
      "After iteration 270, Loss 3.853364\n",
      "After iteration 271, Loss 3.835719\n",
      "After iteration 272, Loss 3.818410\n",
      "After iteration 273, Loss 3.801434\n",
      "After iteration 274, Loss 3.784781\n",
      "After iteration 275, Loss 3.768446\n",
      "After iteration 276, Loss 3.752423\n",
      "After iteration 277, Loss 3.736704\n",
      "After iteration 278, Loss 3.721289\n",
      "After iteration 279, Loss 3.706166\n",
      "After iteration 280, Loss 3.691331\n",
      "After iteration 281, Loss 3.676780\n",
      "After iteration 282, Loss 3.662508\n",
      "After iteration 283, Loss 3.648509\n",
      "After iteration 284, Loss 3.634778\n",
      "After iteration 285, Loss 3.621305\n",
      "After iteration 286, Loss 3.608095\n",
      "After iteration 287, Loss 3.595133\n",
      "After iteration 288, Loss 3.582420\n",
      "After iteration 289, Loss 3.569948\n",
      "After iteration 290, Loss 3.557715\n",
      "After iteration 291, Loss 3.545715\n",
      "After iteration 292, Loss 3.533945\n",
      "After iteration 293, Loss 3.522399\n",
      "After iteration 294, Loss 3.511074\n",
      "After iteration 295, Loss 3.499965\n",
      "After iteration 296, Loss 3.489067\n",
      "After iteration 297, Loss 3.478377\n",
      "After iteration 298, Loss 3.467892\n",
      "After iteration 299, Loss 3.457606\n",
      "After iteration 300, Loss 3.447519\n",
      "After iteration 301, Loss 3.437622\n",
      "After iteration 302, Loss 3.427914\n",
      "After iteration 303, Loss 3.418392\n",
      "After iteration 304, Loss 3.409053\n",
      "After iteration 305, Loss 3.399888\n",
      "After iteration 306, Loss 3.390902\n",
      "After iteration 307, Loss 3.382086\n",
      "After iteration 308, Loss 3.373436\n",
      "After iteration 309, Loss 3.364954\n",
      "After iteration 310, Loss 3.356632\n",
      "After iteration 311, Loss 3.348469\n",
      "After iteration 312, Loss 3.340463\n",
      "After iteration 313, Loss 3.332608\n",
      "After iteration 314, Loss 3.324905\n",
      "After iteration 315, Loss 3.317346\n",
      "After iteration 316, Loss 3.309933\n",
      "After iteration 317, Loss 3.302660\n",
      "After iteration 318, Loss 3.295528\n",
      "After iteration 319, Loss 3.288529\n",
      "After iteration 320, Loss 3.281666\n",
      "After iteration 321, Loss 3.274933\n",
      "After iteration 322, Loss 3.268327\n",
      "After iteration 323, Loss 3.261848\n",
      "After iteration 324, Loss 3.255492\n",
      "After iteration 325, Loss 3.249258\n",
      "After iteration 326, Loss 3.243142\n",
      "After iteration 327, Loss 3.237143\n",
      "After iteration 328, Loss 3.231258\n",
      "After iteration 329, Loss 3.225486\n",
      "After iteration 330, Loss 3.219824\n",
      "After iteration 331, Loss 3.214268\n",
      "After iteration 332, Loss 3.208820\n",
      "After iteration 333, Loss 3.203475\n",
      "After iteration 334, Loss 3.198232\n",
      "After iteration 335, Loss 3.193089\n",
      "After iteration 336, Loss 3.188042\n",
      "After iteration 337, Loss 3.183094\n",
      "After iteration 338, Loss 3.178240\n",
      "After iteration 339, Loss 3.173477\n",
      "After iteration 340, Loss 3.168805\n",
      "After iteration 341, Loss 3.164222\n",
      "After iteration 342, Loss 3.159726\n",
      "After iteration 343, Loss 3.155317\n",
      "After iteration 344, Loss 3.150990\n",
      "After iteration 345, Loss 3.146747\n",
      "After iteration 346, Loss 3.142585\n",
      "After iteration 347, Loss 3.138501\n",
      "After iteration 348, Loss 3.134495\n",
      "After iteration 349, Loss 3.130565\n",
      "After iteration 350, Loss 3.126711\n",
      "After iteration 351, Loss 3.122931\n",
      "After iteration 352, Loss 3.119220\n",
      "After iteration 353, Loss 3.115582\n",
      "After iteration 354, Loss 3.112011\n",
      "After iteration 355, Loss 3.108511\n",
      "After iteration 356, Loss 3.105077\n",
      "After iteration 357, Loss 3.101706\n",
      "After iteration 358, Loss 3.098400\n",
      "After iteration 359, Loss 3.095157\n",
      "After iteration 360, Loss 3.091978\n",
      "After iteration 361, Loss 3.088857\n",
      "After iteration 362, Loss 3.085798\n",
      "After iteration 363, Loss 3.082795\n",
      "After iteration 364, Loss 3.079849\n",
      "After iteration 365, Loss 3.076959\n",
      "After iteration 366, Loss 3.074125\n",
      "After iteration 367, Loss 3.071344\n",
      "After iteration 368, Loss 3.068617\n",
      "After iteration 369, Loss 3.065942\n",
      "After iteration 370, Loss 3.063316\n",
      "After iteration 371, Loss 3.060742\n",
      "After iteration 372, Loss 3.058216\n",
      "After iteration 373, Loss 3.055738\n",
      "After iteration 374, Loss 3.053308\n",
      "After iteration 375, Loss 3.050923\n",
      "After iteration 376, Loss 3.048584\n",
      "After iteration 377, Loss 3.046289\n",
      "After iteration 378, Loss 3.044038\n",
      "After iteration 379, Loss 3.041830\n",
      "After iteration 380, Loss 3.039665\n",
      "After iteration 381, Loss 3.037540\n",
      "After iteration 382, Loss 3.035455\n",
      "After iteration 383, Loss 3.033411\n",
      "After iteration 384, Loss 3.031405\n",
      "After iteration 385, Loss 3.029436\n",
      "After iteration 386, Loss 3.027506\n",
      "After iteration 387, Loss 3.025611\n",
      "After iteration 388, Loss 3.023754\n",
      "After iteration 389, Loss 3.021931\n",
      "After iteration 390, Loss 3.020145\n",
      "After iteration 391, Loss 3.018390\n",
      "After iteration 392, Loss 3.016669\n",
      "After iteration 393, Loss 3.014980\n",
      "After iteration 394, Loss 3.013324\n",
      "After iteration 395, Loss 3.011701\n",
      "After iteration 396, Loss 3.010107\n",
      "After iteration 397, Loss 3.008544\n",
      "After iteration 398, Loss 3.007009\n",
      "After iteration 399, Loss 3.005506\n",
      "After iteration 400, Loss 3.004029\n",
      "After iteration 401, Loss 3.002583\n",
      "After iteration 402, Loss 3.001163\n",
      "After iteration 403, Loss 2.999769\n",
      "After iteration 404, Loss 2.998402\n",
      "After iteration 405, Loss 2.997061\n",
      "After iteration 406, Loss 2.995745\n",
      "After iteration 407, Loss 2.994455\n",
      "After iteration 408, Loss 2.993188\n",
      "After iteration 409, Loss 2.991946\n",
      "After iteration 410, Loss 2.990729\n",
      "After iteration 411, Loss 2.989533\n",
      "After iteration 412, Loss 2.988361\n",
      "After iteration 413, Loss 2.987211\n",
      "After iteration 414, Loss 2.986083\n",
      "After iteration 415, Loss 2.984975\n",
      "After iteration 416, Loss 2.983889\n",
      "After iteration 417, Loss 2.982825\n",
      "After iteration 418, Loss 2.981778\n",
      "After iteration 419, Loss 2.980754\n",
      "After iteration 420, Loss 2.979748\n",
      "After iteration 421, Loss 2.978761\n",
      "After iteration 422, Loss 2.977794\n",
      "After iteration 423, Loss 2.976845\n",
      "After iteration 424, Loss 2.975913\n",
      "After iteration 425, Loss 2.974999\n",
      "After iteration 426, Loss 2.974102\n",
      "After iteration 427, Loss 2.973223\n",
      "After iteration 428, Loss 2.972360\n",
      "After iteration 429, Loss 2.971513\n",
      "After iteration 430, Loss 2.970683\n",
      "After iteration 431, Loss 2.969867\n",
      "After iteration 432, Loss 2.969068\n",
      "After iteration 433, Loss 2.968285\n",
      "After iteration 434, Loss 2.967515\n",
      "After iteration 435, Loss 2.966760\n",
      "After iteration 436, Loss 2.966022\n",
      "After iteration 437, Loss 2.965295\n",
      "After iteration 438, Loss 2.964583\n",
      "After iteration 439, Loss 2.963883\n",
      "After iteration 440, Loss 2.963197\n",
      "After iteration 441, Loss 2.962525\n",
      "After iteration 442, Loss 2.961865\n",
      "After iteration 443, Loss 2.961218\n",
      "After iteration 444, Loss 2.960582\n",
      "After iteration 445, Loss 2.959960\n",
      "After iteration 446, Loss 2.959348\n",
      "After iteration 447, Loss 2.958749\n",
      "After iteration 448, Loss 2.958160\n",
      "After iteration 449, Loss 2.957583\n",
      "After iteration 450, Loss 2.957017\n",
      "After iteration 451, Loss 2.956462\n",
      "After iteration 452, Loss 2.955917\n",
      "After iteration 453, Loss 2.955382\n",
      "After iteration 454, Loss 2.954857\n",
      "After iteration 455, Loss 2.954343\n",
      "After iteration 456, Loss 2.953838\n",
      "After iteration 457, Loss 2.953342\n",
      "After iteration 458, Loss 2.952857\n",
      "After iteration 459, Loss 2.952380\n",
      "After iteration 460, Loss 2.951913\n",
      "After iteration 461, Loss 2.951454\n",
      "After iteration 462, Loss 2.951004\n",
      "After iteration 463, Loss 2.950562\n",
      "After iteration 464, Loss 2.950129\n",
      "After iteration 465, Loss 2.949704\n",
      "After iteration 466, Loss 2.949287\n",
      "After iteration 467, Loss 2.948879\n",
      "After iteration 468, Loss 2.948478\n",
      "After iteration 469, Loss 2.948085\n",
      "After iteration 470, Loss 2.947697\n",
      "After iteration 471, Loss 2.947319\n",
      "After iteration 472, Loss 2.946947\n",
      "After iteration 473, Loss 2.946582\n",
      "After iteration 474, Loss 2.946224\n",
      "After iteration 475, Loss 2.945874\n",
      "After iteration 476, Loss 2.945530\n",
      "After iteration 477, Loss 2.945191\n",
      "After iteration 478, Loss 2.944860\n",
      "After iteration 479, Loss 2.944535\n",
      "After iteration 480, Loss 2.944215\n",
      "After iteration 481, Loss 2.943902\n",
      "After iteration 482, Loss 2.943596\n",
      "After iteration 483, Loss 2.943294\n",
      "After iteration 484, Loss 2.942999\n",
      "After iteration 485, Loss 2.942709\n",
      "After iteration 486, Loss 2.942423\n",
      "After iteration 487, Loss 2.942145\n",
      "After iteration 488, Loss 2.941871\n",
      "After iteration 489, Loss 2.941602\n",
      "After iteration 490, Loss 2.941339\n",
      "After iteration 491, Loss 2.941081\n",
      "After iteration 492, Loss 2.940826\n",
      "After iteration 493, Loss 2.940579\n",
      "After iteration 494, Loss 2.940335\n",
      "After iteration 495, Loss 2.940094\n",
      "After iteration 496, Loss 2.939859\n",
      "After iteration 497, Loss 2.939629\n",
      "After iteration 498, Loss 2.939403\n",
      "After iteration 499, Loss 2.939181\n",
      "After iteration 500, Loss 2.938963\n",
      "After iteration 501, Loss 2.938750\n",
      "After iteration 502, Loss 2.938540\n",
      "After iteration 503, Loss 2.938334\n",
      "After iteration 504, Loss 2.938133\n",
      "After iteration 505, Loss 2.937936\n",
      "After iteration 506, Loss 2.937740\n",
      "After iteration 507, Loss 2.937551\n",
      "After iteration 508, Loss 2.937363\n",
      "After iteration 509, Loss 2.937181\n",
      "After iteration 510, Loss 2.937000\n",
      "After iteration 511, Loss 2.936823\n",
      "After iteration 512, Loss 2.936651\n",
      "After iteration 513, Loss 2.936481\n",
      "After iteration 514, Loss 2.936315\n",
      "After iteration 515, Loss 2.936151\n",
      "After iteration 516, Loss 2.935990\n",
      "After iteration 517, Loss 2.935832\n",
      "After iteration 518, Loss 2.935678\n",
      "After iteration 519, Loss 2.935527\n",
      "After iteration 520, Loss 2.935378\n",
      "After iteration 521, Loss 2.935233\n",
      "After iteration 522, Loss 2.935089\n",
      "After iteration 523, Loss 2.934949\n",
      "After iteration 524, Loss 2.934810\n",
      "After iteration 525, Loss 2.934677\n",
      "After iteration 526, Loss 2.934544\n",
      "After iteration 527, Loss 2.934413\n",
      "After iteration 528, Loss 2.934287\n",
      "After iteration 529, Loss 2.934161\n",
      "After iteration 530, Loss 2.934038\n",
      "After iteration 531, Loss 2.933918\n",
      "After iteration 532, Loss 2.933800\n",
      "After iteration 533, Loss 2.933684\n",
      "After iteration 534, Loss 2.933570\n",
      "After iteration 535, Loss 2.933458\n",
      "After iteration 536, Loss 2.933348\n",
      "After iteration 537, Loss 2.933242\n",
      "After iteration 538, Loss 2.933136\n",
      "After iteration 539, Loss 2.933032\n",
      "After iteration 540, Loss 2.932930\n",
      "After iteration 541, Loss 2.932831\n",
      "After iteration 542, Loss 2.932733\n",
      "After iteration 543, Loss 2.932636\n",
      "After iteration 544, Loss 2.932544\n",
      "After iteration 545, Loss 2.932451\n",
      "After iteration 546, Loss 2.932360\n",
      "After iteration 547, Loss 2.932271\n",
      "After iteration 548, Loss 2.932184\n",
      "After iteration 549, Loss 2.932099\n",
      "After iteration 550, Loss 2.932015\n",
      "After iteration 551, Loss 2.931933\n",
      "After iteration 552, Loss 2.931853\n",
      "After iteration 553, Loss 2.931773\n",
      "After iteration 554, Loss 2.931695\n",
      "After iteration 555, Loss 2.931619\n",
      "After iteration 556, Loss 2.931544\n",
      "After iteration 557, Loss 2.931470\n",
      "After iteration 558, Loss 2.931398\n",
      "After iteration 559, Loss 2.931328\n",
      "After iteration 560, Loss 2.931258\n",
      "After iteration 561, Loss 2.931191\n",
      "After iteration 562, Loss 2.931124\n",
      "After iteration 563, Loss 2.931059\n",
      "After iteration 564, Loss 2.930994\n",
      "After iteration 565, Loss 2.930932\n",
      "After iteration 566, Loss 2.930869\n",
      "After iteration 567, Loss 2.930809\n",
      "After iteration 568, Loss 2.930749\n",
      "After iteration 569, Loss 2.930691\n",
      "After iteration 570, Loss 2.930634\n",
      "After iteration 571, Loss 2.930578\n",
      "After iteration 572, Loss 2.930523\n",
      "After iteration 573, Loss 2.930469\n",
      "After iteration 574, Loss 2.930415\n",
      "After iteration 575, Loss 2.930363\n",
      "After iteration 576, Loss 2.930312\n",
      "After iteration 577, Loss 2.930261\n",
      "After iteration 578, Loss 2.930212\n",
      "After iteration 579, Loss 2.930164\n",
      "After iteration 580, Loss 2.930118\n",
      "After iteration 581, Loss 2.930070\n",
      "After iteration 582, Loss 2.930025\n",
      "After iteration 583, Loss 2.929980\n",
      "After iteration 584, Loss 2.929935\n",
      "After iteration 585, Loss 2.929893\n",
      "After iteration 586, Loss 2.929850\n",
      "After iteration 587, Loss 2.929809\n",
      "After iteration 588, Loss 2.929768\n",
      "After iteration 589, Loss 2.929729\n",
      "After iteration 590, Loss 2.929689\n",
      "After iteration 591, Loss 2.929651\n",
      "After iteration 592, Loss 2.929614\n",
      "After iteration 593, Loss 2.929577\n",
      "After iteration 594, Loss 2.929540\n",
      "After iteration 595, Loss 2.929503\n",
      "After iteration 596, Loss 2.929470\n",
      "After iteration 597, Loss 2.929434\n",
      "After iteration 598, Loss 2.929402\n",
      "After iteration 599, Loss 2.929368\n",
      "After iteration 600, Loss 2.929336\n",
      "After iteration 601, Loss 2.929305\n",
      "After iteration 602, Loss 2.929272\n",
      "After iteration 603, Loss 2.929243\n",
      "After iteration 604, Loss 2.929213\n",
      "After iteration 605, Loss 2.929183\n",
      "After iteration 606, Loss 2.929155\n",
      "After iteration 607, Loss 2.929127\n",
      "After iteration 608, Loss 2.929098\n",
      "After iteration 609, Loss 2.929070\n",
      "After iteration 610, Loss 2.929044\n",
      "After iteration 611, Loss 2.929017\n",
      "After iteration 612, Loss 2.928992\n",
      "After iteration 613, Loss 2.928967\n",
      "After iteration 614, Loss 2.928942\n",
      "After iteration 615, Loss 2.928918\n",
      "After iteration 616, Loss 2.928894\n",
      "After iteration 617, Loss 2.928870\n",
      "After iteration 618, Loss 2.928847\n",
      "After iteration 619, Loss 2.928824\n",
      "After iteration 620, Loss 2.928802\n",
      "After iteration 621, Loss 2.928780\n",
      "After iteration 622, Loss 2.928760\n",
      "After iteration 623, Loss 2.928739\n",
      "After iteration 624, Loss 2.928718\n",
      "After iteration 625, Loss 2.928697\n",
      "After iteration 626, Loss 2.928678\n",
      "After iteration 627, Loss 2.928658\n",
      "After iteration 628, Loss 2.928640\n",
      "After iteration 629, Loss 2.928620\n",
      "After iteration 630, Loss 2.928602\n",
      "After iteration 631, Loss 2.928585\n",
      "After iteration 632, Loss 2.928567\n",
      "After iteration 633, Loss 2.928551\n",
      "After iteration 634, Loss 2.928533\n",
      "After iteration 635, Loss 2.928516\n",
      "After iteration 636, Loss 2.928498\n",
      "After iteration 637, Loss 2.928484\n",
      "After iteration 638, Loss 2.928468\n",
      "After iteration 639, Loss 2.928452\n",
      "After iteration 640, Loss 2.928437\n",
      "After iteration 641, Loss 2.928422\n",
      "After iteration 642, Loss 2.928409\n",
      "After iteration 643, Loss 2.928393\n",
      "After iteration 644, Loss 2.928380\n",
      "After iteration 645, Loss 2.928365\n",
      "After iteration 646, Loss 2.928352\n",
      "After iteration 647, Loss 2.928339\n",
      "After iteration 648, Loss 2.928326\n",
      "After iteration 649, Loss 2.928313\n",
      "After iteration 650, Loss 2.928301\n",
      "After iteration 651, Loss 2.928290\n",
      "After iteration 652, Loss 2.928277\n",
      "After iteration 653, Loss 2.928265\n",
      "After iteration 654, Loss 2.928253\n",
      "After iteration 655, Loss 2.928242\n",
      "After iteration 656, Loss 2.928230\n",
      "After iteration 657, Loss 2.928220\n",
      "After iteration 658, Loss 2.928209\n",
      "After iteration 659, Loss 2.928198\n",
      "After iteration 660, Loss 2.928188\n",
      "After iteration 661, Loss 2.928178\n",
      "After iteration 662, Loss 2.928168\n",
      "After iteration 663, Loss 2.928158\n",
      "After iteration 664, Loss 2.928148\n",
      "After iteration 665, Loss 2.928139\n",
      "After iteration 666, Loss 2.928130\n",
      "After iteration 667, Loss 2.928121\n",
      "After iteration 668, Loss 2.928112\n",
      "After iteration 669, Loss 2.928103\n",
      "After iteration 670, Loss 2.928095\n",
      "After iteration 671, Loss 2.928086\n",
      "After iteration 672, Loss 2.928078\n",
      "After iteration 673, Loss 2.928070\n",
      "After iteration 674, Loss 2.928061\n",
      "After iteration 675, Loss 2.928053\n",
      "After iteration 676, Loss 2.928046\n",
      "After iteration 677, Loss 2.928039\n",
      "After iteration 678, Loss 2.928030\n",
      "After iteration 679, Loss 2.928024\n",
      "After iteration 680, Loss 2.928017\n",
      "After iteration 681, Loss 2.928011\n",
      "After iteration 682, Loss 2.928004\n",
      "After iteration 683, Loss 2.927997\n",
      "After iteration 684, Loss 2.927990\n",
      "After iteration 685, Loss 2.927983\n",
      "After iteration 686, Loss 2.927976\n",
      "After iteration 687, Loss 2.927971\n",
      "After iteration 688, Loss 2.927964\n",
      "After iteration 689, Loss 2.927959\n",
      "After iteration 690, Loss 2.927953\n",
      "After iteration 691, Loss 2.927947\n",
      "After iteration 692, Loss 2.927942\n",
      "After iteration 693, Loss 2.927936\n",
      "After iteration 694, Loss 2.927931\n",
      "After iteration 695, Loss 2.927925\n",
      "After iteration 696, Loss 2.927920\n",
      "After iteration 697, Loss 2.927915\n",
      "After iteration 698, Loss 2.927910\n",
      "After iteration 699, Loss 2.927905\n",
      "After iteration 700, Loss 2.927899\n",
      "After iteration 701, Loss 2.927895\n",
      "After iteration 702, Loss 2.927890\n",
      "After iteration 703, Loss 2.927888\n",
      "After iteration 704, Loss 2.927882\n",
      "After iteration 705, Loss 2.927877\n",
      "After iteration 706, Loss 2.927873\n",
      "After iteration 707, Loss 2.927869\n",
      "After iteration 708, Loss 2.927864\n",
      "After iteration 709, Loss 2.927860\n",
      "After iteration 710, Loss 2.927856\n",
      "After iteration 711, Loss 2.927852\n",
      "After iteration 712, Loss 2.927848\n",
      "After iteration 713, Loss 2.927844\n",
      "After iteration 714, Loss 2.927842\n",
      "After iteration 715, Loss 2.927837\n",
      "After iteration 716, Loss 2.927834\n",
      "After iteration 717, Loss 2.927830\n",
      "After iteration 718, Loss 2.927826\n",
      "After iteration 719, Loss 2.927824\n",
      "After iteration 720, Loss 2.927820\n",
      "After iteration 721, Loss 2.927817\n",
      "After iteration 722, Loss 2.927814\n",
      "After iteration 723, Loss 2.927810\n",
      "After iteration 724, Loss 2.927807\n",
      "After iteration 725, Loss 2.927804\n",
      "After iteration 726, Loss 2.927801\n",
      "After iteration 727, Loss 2.927799\n",
      "After iteration 728, Loss 2.927796\n",
      "After iteration 729, Loss 2.927793\n",
      "After iteration 730, Loss 2.927790\n",
      "After iteration 731, Loss 2.927787\n",
      "After iteration 732, Loss 2.927785\n",
      "After iteration 733, Loss 2.927782\n",
      "After iteration 734, Loss 2.927779\n",
      "After iteration 735, Loss 2.927777\n",
      "After iteration 736, Loss 2.927774\n",
      "After iteration 737, Loss 2.927773\n",
      "After iteration 738, Loss 2.927770\n",
      "After iteration 739, Loss 2.927768\n",
      "After iteration 740, Loss 2.927765\n",
      "After iteration 741, Loss 2.927763\n",
      "After iteration 742, Loss 2.927761\n",
      "After iteration 743, Loss 2.927758\n",
      "After iteration 744, Loss 2.927756\n",
      "After iteration 745, Loss 2.927754\n",
      "After iteration 746, Loss 2.927752\n",
      "After iteration 747, Loss 2.927751\n",
      "After iteration 748, Loss 2.927750\n",
      "After iteration 749, Loss 2.927746\n",
      "After iteration 750, Loss 2.927744\n",
      "After iteration 751, Loss 2.927743\n",
      "After iteration 752, Loss 2.927741\n",
      "After iteration 753, Loss 2.927739\n",
      "After iteration 754, Loss 2.927737\n",
      "After iteration 755, Loss 2.927735\n",
      "After iteration 756, Loss 2.927734\n",
      "After iteration 757, Loss 2.927733\n",
      "After iteration 758, Loss 2.927730\n",
      "After iteration 759, Loss 2.927730\n",
      "After iteration 760, Loss 2.927727\n",
      "After iteration 761, Loss 2.927725\n",
      "After iteration 762, Loss 2.927724\n",
      "After iteration 763, Loss 2.927724\n",
      "After iteration 764, Loss 2.927722\n",
      "After iteration 765, Loss 2.927719\n",
      "After iteration 766, Loss 2.927718\n",
      "After iteration 767, Loss 2.927718\n",
      "After iteration 768, Loss 2.927716\n",
      "After iteration 769, Loss 2.927715\n",
      "After iteration 770, Loss 2.927713\n",
      "After iteration 771, Loss 2.927713\n",
      "After iteration 772, Loss 2.927711\n",
      "After iteration 773, Loss 2.927711\n",
      "After iteration 774, Loss 2.927709\n",
      "After iteration 775, Loss 2.927708\n",
      "After iteration 776, Loss 2.927706\n",
      "After iteration 777, Loss 2.927706\n",
      "After iteration 778, Loss 2.927704\n",
      "After iteration 779, Loss 2.927702\n",
      "After iteration 780, Loss 2.927702\n",
      "After iteration 781, Loss 2.927700\n",
      "After iteration 782, Loss 2.927699\n",
      "After iteration 783, Loss 2.927698\n",
      "After iteration 784, Loss 2.927697\n",
      "After iteration 785, Loss 2.927696\n",
      "After iteration 786, Loss 2.927696\n",
      "After iteration 787, Loss 2.927695\n",
      "After iteration 788, Loss 2.927693\n",
      "After iteration 789, Loss 2.927693\n",
      "After iteration 790, Loss 2.927692\n",
      "After iteration 791, Loss 2.927691\n",
      "After iteration 792, Loss 2.927691\n",
      "After iteration 793, Loss 2.927690\n",
      "After iteration 794, Loss 2.927689\n",
      "After iteration 795, Loss 2.927689\n",
      "After iteration 796, Loss 2.927687\n",
      "After iteration 797, Loss 2.927686\n",
      "After iteration 798, Loss 2.927686\n",
      "After iteration 799, Loss 2.927686\n",
      "After iteration 800, Loss 2.927684\n",
      "After iteration 801, Loss 2.927683\n",
      "After iteration 802, Loss 2.927683\n",
      "After iteration 803, Loss 2.927682\n",
      "After iteration 804, Loss 2.927681\n",
      "After iteration 805, Loss 2.927682\n",
      "After iteration 806, Loss 2.927680\n",
      "After iteration 807, Loss 2.927679\n",
      "After iteration 808, Loss 2.927679\n",
      "After iteration 809, Loss 2.927678\n",
      "After iteration 810, Loss 2.927678\n",
      "After iteration 811, Loss 2.927677\n",
      "After iteration 812, Loss 2.927676\n",
      "After iteration 813, Loss 2.927675\n",
      "After iteration 814, Loss 2.927676\n",
      "After iteration 815, Loss 2.927675\n",
      "After iteration 816, Loss 2.927674\n",
      "After iteration 817, Loss 2.927673\n",
      "After iteration 818, Loss 2.927673\n",
      "After iteration 819, Loss 2.927672\n",
      "After iteration 820, Loss 2.927673\n",
      "After iteration 821, Loss 2.927671\n",
      "After iteration 822, Loss 2.927671\n",
      "After iteration 823, Loss 2.927671\n",
      "After iteration 824, Loss 2.927670\n",
      "After iteration 825, Loss 2.927669\n",
      "After iteration 826, Loss 2.927670\n",
      "After iteration 827, Loss 2.927669\n",
      "After iteration 828, Loss 2.927669\n",
      "After iteration 829, Loss 2.927667\n",
      "After iteration 830, Loss 2.927667\n",
      "After iteration 831, Loss 2.927667\n",
      "After iteration 832, Loss 2.927666\n",
      "After iteration 833, Loss 2.927666\n",
      "After iteration 834, Loss 2.927666\n",
      "After iteration 835, Loss 2.927665\n",
      "After iteration 836, Loss 2.927664\n",
      "After iteration 837, Loss 2.927665\n",
      "After iteration 838, Loss 2.927664\n",
      "After iteration 839, Loss 2.927664\n",
      "After iteration 840, Loss 2.927663\n",
      "After iteration 841, Loss 2.927663\n",
      "After iteration 842, Loss 2.927663\n",
      "After iteration 843, Loss 2.927663\n",
      "After iteration 844, Loss 2.927663\n",
      "After iteration 845, Loss 2.927663\n",
      "After iteration 846, Loss 2.927662\n",
      "After iteration 847, Loss 2.927662\n",
      "After iteration 848, Loss 2.927661\n",
      "After iteration 849, Loss 2.927661\n",
      "After iteration 850, Loss 2.927660\n",
      "After iteration 851, Loss 2.927660\n",
      "After iteration 852, Loss 2.927660\n",
      "After iteration 853, Loss 2.927660\n",
      "After iteration 854, Loss 2.927659\n",
      "After iteration 855, Loss 2.927659\n",
      "After iteration 856, Loss 2.927659\n",
      "After iteration 857, Loss 2.927659\n",
      "After iteration 858, Loss 2.927658\n",
      "After iteration 859, Loss 2.927658\n",
      "After iteration 860, Loss 2.927658\n",
      "After iteration 861, Loss 2.927658\n",
      "After iteration 862, Loss 2.927658\n",
      "After iteration 863, Loss 2.927658\n",
      "After iteration 864, Loss 2.927657\n",
      "After iteration 865, Loss 2.927657\n",
      "After iteration 866, Loss 2.927657\n",
      "After iteration 867, Loss 2.927657\n",
      "After iteration 868, Loss 2.927656\n",
      "After iteration 869, Loss 2.927656\n",
      "After iteration 870, Loss 2.927656\n",
      "After iteration 871, Loss 2.927656\n",
      "After iteration 872, Loss 2.927655\n",
      "After iteration 873, Loss 2.927655\n",
      "After iteration 874, Loss 2.927655\n",
      "After iteration 875, Loss 2.927655\n",
      "After iteration 876, Loss 2.927655\n",
      "After iteration 877, Loss 2.927655\n",
      "After iteration 878, Loss 2.927655\n",
      "After iteration 879, Loss 2.927654\n",
      "After iteration 880, Loss 2.927654\n",
      "After iteration 881, Loss 2.927655\n",
      "After iteration 882, Loss 2.927654\n",
      "After iteration 883, Loss 2.927653\n",
      "After iteration 884, Loss 2.927654\n",
      "After iteration 885, Loss 2.927653\n",
      "After iteration 886, Loss 2.927653\n",
      "After iteration 887, Loss 2.927653\n",
      "After iteration 888, Loss 2.927653\n",
      "After iteration 889, Loss 2.927653\n",
      "After iteration 890, Loss 2.927653\n",
      "After iteration 891, Loss 2.927653\n",
      "After iteration 892, Loss 2.927653\n",
      "After iteration 893, Loss 2.927653\n",
      "After iteration 894, Loss 2.927652\n",
      "After iteration 895, Loss 2.927652\n",
      "After iteration 896, Loss 2.927651\n",
      "After iteration 897, Loss 2.927652\n",
      "After iteration 898, Loss 2.927651\n",
      "After iteration 899, Loss 2.927652\n",
      "After iteration 900, Loss 2.927651\n",
      "After iteration 901, Loss 2.927651\n",
      "After iteration 902, Loss 2.927651\n",
      "After iteration 903, Loss 2.927652\n",
      "After iteration 904, Loss 2.927651\n",
      "After iteration 905, Loss 2.927650\n",
      "After iteration 906, Loss 2.927649\n",
      "After iteration 907, Loss 2.927651\n",
      "After iteration 908, Loss 2.927651\n",
      "After iteration 909, Loss 2.927651\n",
      "After iteration 910, Loss 2.927650\n",
      "After iteration 911, Loss 2.927651\n",
      "After iteration 912, Loss 2.927651\n",
      "After iteration 913, Loss 2.927650\n",
      "After iteration 914, Loss 2.927650\n",
      "After iteration 915, Loss 2.927650\n",
      "After iteration 916, Loss 2.927649\n",
      "After iteration 917, Loss 2.927650\n",
      "After iteration 918, Loss 2.927649\n",
      "After iteration 919, Loss 2.927650\n",
      "After iteration 920, Loss 2.927650\n",
      "After iteration 921, Loss 2.927649\n",
      "After iteration 922, Loss 2.927649\n",
      "After iteration 923, Loss 2.927649\n",
      "After iteration 924, Loss 2.927649\n",
      "After iteration 925, Loss 2.927649\n",
      "After iteration 926, Loss 2.927649\n",
      "After iteration 927, Loss 2.927649\n",
      "After iteration 928, Loss 2.927649\n",
      "After iteration 929, Loss 2.927648\n",
      "After iteration 930, Loss 2.927648\n",
      "After iteration 931, Loss 2.927648\n",
      "After iteration 932, Loss 2.927649\n",
      "After iteration 933, Loss 2.927648\n",
      "After iteration 934, Loss 2.927648\n",
      "After iteration 935, Loss 2.927648\n",
      "After iteration 936, Loss 2.927649\n",
      "After iteration 937, Loss 2.927648\n",
      "After iteration 938, Loss 2.927648\n",
      "After iteration 939, Loss 2.927649\n",
      "After iteration 940, Loss 2.927648\n",
      "After iteration 941, Loss 2.927648\n",
      "After iteration 942, Loss 2.927648\n",
      "After iteration 943, Loss 2.927648\n",
      "After iteration 944, Loss 2.927648\n",
      "After iteration 945, Loss 2.927648\n",
      "After iteration 946, Loss 2.927648\n",
      "After iteration 947, Loss 2.927648\n",
      "After iteration 948, Loss 2.927649\n",
      "After iteration 949, Loss 2.927648\n",
      "After iteration 950, Loss 2.927647\n",
      "After iteration 951, Loss 2.927647\n",
      "After iteration 952, Loss 2.927647\n",
      "After iteration 953, Loss 2.927648\n",
      "After iteration 954, Loss 2.927648\n",
      "After iteration 955, Loss 2.927647\n",
      "After iteration 956, Loss 2.927647\n",
      "After iteration 957, Loss 2.927649\n",
      "After iteration 958, Loss 2.927647\n",
      "After iteration 959, Loss 2.927647\n",
      "After iteration 960, Loss 2.927647\n",
      "After iteration 961, Loss 2.927647\n",
      "After iteration 962, Loss 2.927647\n",
      "After iteration 963, Loss 2.927647\n",
      "After iteration 964, Loss 2.927648\n",
      "After iteration 965, Loss 2.927648\n",
      "After iteration 966, Loss 2.927646\n",
      "After iteration 967, Loss 2.927647\n",
      "After iteration 968, Loss 2.927647\n",
      "After iteration 969, Loss 2.927648\n",
      "After iteration 970, Loss 2.927647\n",
      "After iteration 971, Loss 2.927647\n",
      "After iteration 972, Loss 2.927647\n",
      "After iteration 973, Loss 2.927648\n",
      "After iteration 974, Loss 2.927647\n",
      "After iteration 975, Loss 2.927647\n",
      "After iteration 976, Loss 2.927647\n",
      "After iteration 977, Loss 2.927646\n",
      "After iteration 978, Loss 2.927647\n",
      "After iteration 979, Loss 2.927646\n",
      "After iteration 980, Loss 2.927647\n",
      "After iteration 981, Loss 2.927647\n",
      "After iteration 982, Loss 2.927646\n",
      "After iteration 983, Loss 2.927646\n",
      "After iteration 984, Loss 2.927646\n",
      "After iteration 985, Loss 2.927647\n",
      "After iteration 986, Loss 2.927647\n",
      "After iteration 987, Loss 2.927647\n",
      "After iteration 988, Loss 2.927646\n",
      "After iteration 989, Loss 2.927647\n",
      "After iteration 990, Loss 2.927647\n",
      "After iteration 991, Loss 2.927647\n",
      "After iteration 992, Loss 2.927647\n",
      "After iteration 993, Loss 2.927646\n",
      "After iteration 994, Loss 2.927647\n",
      "After iteration 995, Loss 2.927646\n",
      "After iteration 996, Loss 2.927647\n",
      "After iteration 997, Loss 2.927646\n",
      "After iteration 998, Loss 2.927647\n",
      "After iteration 999, Loss 2.927647\n",
      "After iteration 1000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 9.0340, 10.4995], requires_grad=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Zapisz równanie transformujące oryginalne dane wejściowe (`t_u`) do stopni Celsjusza (`t_p`).\n",
    "Znajdź współczynniki prostej opisującej model liniowy. Porównaj z wynikiem z poprzednich zajęć.\n",
    "\n",
    "Uwaga / hint:\n",
    "Uwzględnij średnią i odchylenie standardowe policzone dla danych wejściowych na początku dzisiejszych zajęć."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:24.525385Z",
     "start_time": "2025-03-20T16:22:24.520985Z"
    }
   },
   "source": [
    "w, b = params.detach().numpy()\n",
    "\n",
    "mean_t_u = t_u.mean().item()\n",
    "std_t_u = t_u.std().item()\n",
    "\n",
    "w_transformed = w / std_t_u\n",
    "b_transformed = b - (w * mean_t_u / std_t_u)\n",
    "\n",
    "print(f\"Równanie transformujące: t_p = {w_transformed} * t_u + {b_transformed}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Równanie transformujące: t_p = 0.5367202758789062 * t_u + -17.30256462097168\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:22:25.373116Z",
     "start_time": "2025-03-20T16:22:25.368834Z"
    }
   },
   "source": "print(\"Równanie z poprzednich zajęć: t_p = 0.1 * 5.367 * t_u + 32.0\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Równanie z poprzednich zajęć: t_p = 0.1 * 5.367 * t_u + 32.0\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim\n",
    "\n",
    "Algorytm najszybszego spadku jest przykładem jednego z wielu algorytmów optymalizacji dostępnych w PyTorch. Implementacje różnych optymalizatorów znajdują się w module `torch.optim`.\n",
    "\n",
    "Przykłady optymalizatorów w PyTorch:\n",
    "- `SGD` - Stochastic Gradient Descent (tak naprawdę tzw. Batch Gradient Descent), z dodatkową opcją tzw. \"momentum\"\n",
    "- `Adagrad`, `RMSProp` - \"adaptive, per-parameter learning rate\"\n",
    "- `Adam` - Adaptive Moment Estimation - połączenie SGD+momentum z RMSProp - obecnie bardzo popularny\n",
    "- ...\n",
    "\n",
    "Bardziej szczegółowo omówimy te algorytmy w późniejszym czasie."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:32:49.266062Z",
     "start_time": "2025-03-20T16:32:49.261084Z"
    }
   },
   "source": [
    "import torch.optim as optim\n",
    "dir(optim)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adafactor',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_adafactor',\n",
       " '_functional',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 103
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optymalizacja funkcji straty w torch.optim\n",
    "\n",
    "Optymalizator inicjalizuje się wybierając konkretną klasę z dostępnych w `torch.optim`. Pierwszym argumentem przy tworzeniu optymalizatora jest lista parametrów modelu (nie zawsze wszystkich - czasem np. \"zamraża\" się niektóre parametry i nie aktualizuje ich). Tworząc obiekt optymalizatora podaje się też jako argument m.in. wartość learning rate (argument `lr`).\n",
    "\n",
    "W API optymalizatora są dwie metody: `zero_grad()` oraz `step()`. Pierwsza z nich zeruje gradienty parametrów (patrz wyżej), a druga wykonuje krok aktualizacji parametrów zgodny z wybranym algorytmem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Zainicjalizuj tensor `params` jako [1.0, 0.0] (z włączonym autograd) oraz optymalizator SGD ze stałą uczącą o wartości 0.01.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:38:16.054778Z",
     "start_time": "2025-03-20T16:38:16.050785Z"
    }
   },
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "optim_SGD = optim.SGD([params], lr=0.01)"
   ],
   "outputs": [],
   "execution_count": 121
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Na podstawie `training_loop_autograd` (napisanej wyżej) napisz funkcję `training_loop_optim`, w której zerowanie gradientów i krok aktualizacji parametrów wykonane będą z użyciem optymalizatora z `torch.optim`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:38:17.097029Z",
     "start_time": "2025-03-20T16:38:17.092869Z"
    }
   },
   "source": [
    "def training_loop_optim(n_iters, optim, params, t_u, t_c):\n",
    "    for iteration in range(1, n_iters+1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        print('After iteration %d, Loss %f' % (iteration, float(loss)))\n",
    "\n",
    "    return params"
   ],
   "outputs": [],
   "execution_count": 122
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Uruchom `training_loop_optim` i porównaj wynik z `training_loop_autograd`. Powinno wyjść to samo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:38:19.538074Z",
     "start_time": "2025-03-20T16:38:19.234313Z"
    }
   },
   "source": "training_loop_optim(1000, optim_SGD,params, t_un, t_c)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After iteration 1, Loss 171.868347\n",
      "After iteration 2, Loss 165.387650\n",
      "After iteration 3, Loss 159.156021\n",
      "After iteration 4, Loss 153.163925\n",
      "After iteration 5, Loss 147.402084\n",
      "After iteration 6, Loss 141.861664\n",
      "After iteration 7, Loss 136.534119\n",
      "After iteration 8, Loss 131.411301\n",
      "After iteration 9, Loss 126.485237\n",
      "After iteration 10, Loss 121.748444\n",
      "After iteration 11, Loss 117.193611\n",
      "After iteration 12, Loss 112.813721\n",
      "After iteration 13, Loss 108.602028\n",
      "After iteration 14, Loss 104.552116\n",
      "After iteration 15, Loss 100.657707\n",
      "After iteration 16, Loss 96.912842\n",
      "After iteration 17, Loss 93.311745\n",
      "After iteration 18, Loss 89.848923\n",
      "After iteration 19, Loss 86.519012\n",
      "After iteration 20, Loss 83.316925\n",
      "After iteration 21, Loss 80.237762\n",
      "After iteration 22, Loss 77.276772\n",
      "After iteration 23, Loss 74.429413\n",
      "After iteration 24, Loss 71.691315\n",
      "After iteration 25, Loss 69.058289\n",
      "After iteration 26, Loss 66.526276\n",
      "After iteration 27, Loss 64.091408\n",
      "After iteration 28, Loss 61.749950\n",
      "After iteration 29, Loss 59.498318\n",
      "After iteration 30, Loss 57.333035\n",
      "After iteration 31, Loss 55.250809\n",
      "After iteration 32, Loss 53.248425\n",
      "After iteration 33, Loss 51.322838\n",
      "After iteration 34, Loss 49.471081\n",
      "After iteration 35, Loss 47.690319\n",
      "After iteration 36, Loss 45.977833\n",
      "After iteration 37, Loss 44.330997\n",
      "After iteration 38, Loss 42.747280\n",
      "After iteration 39, Loss 41.224274\n",
      "After iteration 40, Loss 39.759628\n",
      "After iteration 41, Loss 38.351124\n",
      "After iteration 42, Loss 36.996582\n",
      "After iteration 43, Loss 35.693947\n",
      "After iteration 44, Loss 34.441227\n",
      "After iteration 45, Loss 33.236496\n",
      "After iteration 46, Loss 32.077919\n",
      "After iteration 47, Loss 30.963709\n",
      "After iteration 48, Loss 29.892189\n",
      "After iteration 49, Loss 28.861692\n",
      "After iteration 50, Loss 27.870668\n",
      "After iteration 51, Loss 26.917591\n",
      "After iteration 52, Loss 26.001007\n",
      "After iteration 53, Loss 25.119518\n",
      "After iteration 54, Loss 24.271776\n",
      "After iteration 55, Loss 23.456480\n",
      "After iteration 56, Loss 22.672396\n",
      "After iteration 57, Loss 21.918318\n",
      "After iteration 58, Loss 21.193100\n",
      "After iteration 59, Loss 20.495638\n",
      "After iteration 60, Loss 19.824860\n",
      "After iteration 61, Loss 19.179749\n",
      "After iteration 62, Loss 18.559319\n",
      "After iteration 63, Loss 17.962622\n",
      "After iteration 64, Loss 17.388752\n",
      "After iteration 65, Loss 16.836828\n",
      "After iteration 66, Loss 16.306017\n",
      "After iteration 67, Loss 15.795502\n",
      "After iteration 68, Loss 15.304514\n",
      "After iteration 69, Loss 14.832294\n",
      "After iteration 70, Loss 14.378129\n",
      "After iteration 71, Loss 13.941333\n",
      "After iteration 72, Loss 13.521228\n",
      "After iteration 73, Loss 13.117185\n",
      "After iteration 74, Loss 12.728582\n",
      "After iteration 75, Loss 12.354837\n",
      "After iteration 76, Loss 11.995371\n",
      "After iteration 77, Loss 11.649640\n",
      "After iteration 78, Loss 11.317122\n",
      "After iteration 79, Loss 10.997303\n",
      "After iteration 80, Loss 10.689705\n",
      "After iteration 81, Loss 10.393856\n",
      "After iteration 82, Loss 10.109310\n",
      "After iteration 83, Loss 9.835629\n",
      "After iteration 84, Loss 9.572399\n",
      "After iteration 85, Loss 9.319222\n",
      "After iteration 86, Loss 9.075713\n",
      "After iteration 87, Loss 8.841498\n",
      "After iteration 88, Loss 8.616224\n",
      "After iteration 89, Loss 8.399553\n",
      "After iteration 90, Loss 8.191153\n",
      "After iteration 91, Loss 7.990705\n",
      "After iteration 92, Loss 7.797906\n",
      "After iteration 93, Loss 7.612467\n",
      "After iteration 94, Loss 7.434102\n",
      "After iteration 95, Loss 7.262542\n",
      "After iteration 96, Loss 7.097529\n",
      "After iteration 97, Loss 6.938811\n",
      "After iteration 98, Loss 6.786147\n",
      "After iteration 99, Loss 6.639305\n",
      "After iteration 100, Loss 6.498064\n",
      "After iteration 101, Loss 6.362207\n",
      "After iteration 102, Loss 6.231535\n",
      "After iteration 103, Loss 6.105843\n",
      "After iteration 104, Loss 5.984941\n",
      "After iteration 105, Loss 5.868649\n",
      "After iteration 106, Loss 5.756792\n",
      "After iteration 107, Loss 5.649198\n",
      "After iteration 108, Loss 5.545705\n",
      "After iteration 109, Loss 5.446156\n",
      "After iteration 110, Loss 5.350400\n",
      "After iteration 111, Loss 5.258291\n",
      "After iteration 112, Loss 5.169693\n",
      "After iteration 113, Loss 5.084470\n",
      "After iteration 114, Loss 5.002494\n",
      "After iteration 115, Loss 4.923639\n",
      "After iteration 116, Loss 4.847788\n",
      "After iteration 117, Loss 4.774829\n",
      "After iteration 118, Loss 4.704645\n",
      "After iteration 119, Loss 4.637136\n",
      "After iteration 120, Loss 4.572193\n",
      "After iteration 121, Loss 4.509727\n",
      "After iteration 122, Loss 4.449636\n",
      "After iteration 123, Loss 4.391836\n",
      "After iteration 124, Loss 4.336232\n",
      "After iteration 125, Loss 4.282745\n",
      "After iteration 126, Loss 4.231293\n",
      "After iteration 127, Loss 4.181801\n",
      "After iteration 128, Loss 4.134192\n",
      "After iteration 129, Loss 4.088393\n",
      "After iteration 130, Loss 4.044336\n",
      "After iteration 131, Loss 4.001956\n",
      "After iteration 132, Loss 3.961187\n",
      "After iteration 133, Loss 3.921968\n",
      "After iteration 134, Loss 3.884241\n",
      "After iteration 135, Loss 3.847950\n",
      "After iteration 136, Loss 3.813037\n",
      "After iteration 137, Loss 3.779454\n",
      "After iteration 138, Loss 3.747145\n",
      "After iteration 139, Loss 3.716065\n",
      "After iteration 140, Loss 3.686167\n",
      "After iteration 141, Loss 3.657404\n",
      "After iteration 142, Loss 3.629735\n",
      "After iteration 143, Loss 3.603117\n",
      "After iteration 144, Loss 3.577511\n",
      "After iteration 145, Loss 3.552877\n",
      "After iteration 146, Loss 3.529182\n",
      "After iteration 147, Loss 3.506383\n",
      "After iteration 148, Loss 3.484450\n",
      "After iteration 149, Loss 3.463352\n",
      "After iteration 150, Loss 3.443054\n",
      "After iteration 151, Loss 3.423527\n",
      "After iteration 152, Loss 3.404742\n",
      "After iteration 153, Loss 3.386669\n",
      "After iteration 154, Loss 3.369283\n",
      "After iteration 155, Loss 3.352557\n",
      "After iteration 156, Loss 3.336466\n",
      "After iteration 157, Loss 3.320985\n",
      "After iteration 158, Loss 3.306094\n",
      "After iteration 159, Loss 3.291766\n",
      "After iteration 160, Loss 3.277981\n",
      "After iteration 161, Loss 3.264720\n",
      "After iteration 162, Loss 3.251961\n",
      "After iteration 163, Loss 3.239687\n",
      "After iteration 164, Loss 3.227879\n",
      "After iteration 165, Loss 3.216519\n",
      "After iteration 166, Loss 3.205589\n",
      "After iteration 167, Loss 3.195073\n",
      "After iteration 168, Loss 3.184956\n",
      "After iteration 169, Loss 3.175221\n",
      "After iteration 170, Loss 3.165859\n",
      "After iteration 171, Loss 3.156848\n",
      "After iteration 172, Loss 3.148180\n",
      "After iteration 173, Loss 3.139841\n",
      "After iteration 174, Loss 3.131818\n",
      "After iteration 175, Loss 3.124098\n",
      "After iteration 176, Loss 3.116672\n",
      "After iteration 177, Loss 3.109526\n",
      "After iteration 178, Loss 3.102652\n",
      "After iteration 179, Loss 3.096037\n",
      "After iteration 180, Loss 3.089673\n",
      "After iteration 181, Loss 3.083550\n",
      "After iteration 182, Loss 3.077660\n",
      "After iteration 183, Loss 3.071994\n",
      "After iteration 184, Loss 3.066540\n",
      "After iteration 185, Loss 3.061295\n",
      "After iteration 186, Loss 3.056245\n",
      "After iteration 187, Loss 3.051389\n",
      "After iteration 188, Loss 3.046716\n",
      "After iteration 189, Loss 3.042219\n",
      "After iteration 190, Loss 3.037894\n",
      "After iteration 191, Loss 3.033731\n",
      "After iteration 192, Loss 3.029727\n",
      "After iteration 193, Loss 3.025873\n",
      "After iteration 194, Loss 3.022166\n",
      "After iteration 195, Loss 3.018599\n",
      "After iteration 196, Loss 3.015167\n",
      "After iteration 197, Loss 3.011863\n",
      "After iteration 198, Loss 3.008686\n",
      "After iteration 199, Loss 3.005628\n",
      "After iteration 200, Loss 3.002686\n",
      "After iteration 201, Loss 2.999856\n",
      "After iteration 202, Loss 2.997132\n",
      "After iteration 203, Loss 2.994512\n",
      "After iteration 204, Loss 2.991989\n",
      "After iteration 205, Loss 2.989564\n",
      "After iteration 206, Loss 2.987230\n",
      "After iteration 207, Loss 2.984984\n",
      "After iteration 208, Loss 2.982821\n",
      "After iteration 209, Loss 2.980742\n",
      "After iteration 210, Loss 2.978740\n",
      "After iteration 211, Loss 2.976815\n",
      "After iteration 212, Loss 2.974962\n",
      "After iteration 213, Loss 2.973179\n",
      "After iteration 214, Loss 2.971464\n",
      "After iteration 215, Loss 2.969813\n",
      "After iteration 216, Loss 2.968225\n",
      "After iteration 217, Loss 2.966695\n",
      "After iteration 218, Loss 2.965225\n",
      "After iteration 219, Loss 2.963810\n",
      "After iteration 220, Loss 2.962447\n",
      "After iteration 221, Loss 2.961138\n",
      "After iteration 222, Loss 2.959877\n",
      "After iteration 223, Loss 2.958664\n",
      "After iteration 224, Loss 2.957495\n",
      "After iteration 225, Loss 2.956372\n",
      "After iteration 226, Loss 2.955290\n",
      "After iteration 227, Loss 2.954251\n",
      "After iteration 228, Loss 2.953249\n",
      "After iteration 229, Loss 2.952286\n",
      "After iteration 230, Loss 2.951359\n",
      "After iteration 231, Loss 2.950466\n",
      "After iteration 232, Loss 2.949608\n",
      "After iteration 233, Loss 2.948781\n",
      "After iteration 234, Loss 2.947988\n",
      "After iteration 235, Loss 2.947222\n",
      "After iteration 236, Loss 2.946486\n",
      "After iteration 237, Loss 2.945776\n",
      "After iteration 238, Loss 2.945096\n",
      "After iteration 239, Loss 2.944440\n",
      "After iteration 240, Loss 2.943808\n",
      "After iteration 241, Loss 2.943201\n",
      "After iteration 242, Loss 2.942616\n",
      "After iteration 243, Loss 2.942053\n",
      "After iteration 244, Loss 2.941511\n",
      "After iteration 245, Loss 2.940991\n",
      "After iteration 246, Loss 2.940489\n",
      "After iteration 247, Loss 2.940007\n",
      "After iteration 248, Loss 2.939542\n",
      "After iteration 249, Loss 2.939096\n",
      "After iteration 250, Loss 2.938664\n",
      "After iteration 251, Loss 2.938251\n",
      "After iteration 252, Loss 2.937853\n",
      "After iteration 253, Loss 2.937470\n",
      "After iteration 254, Loss 2.937101\n",
      "After iteration 255, Loss 2.936746\n",
      "After iteration 256, Loss 2.936404\n",
      "After iteration 257, Loss 2.936075\n",
      "After iteration 258, Loss 2.935759\n",
      "After iteration 259, Loss 2.935454\n",
      "After iteration 260, Loss 2.935162\n",
      "After iteration 261, Loss 2.934880\n",
      "After iteration 262, Loss 2.934608\n",
      "After iteration 263, Loss 2.934348\n",
      "After iteration 264, Loss 2.934096\n",
      "After iteration 265, Loss 2.933853\n",
      "After iteration 266, Loss 2.933620\n",
      "After iteration 267, Loss 2.933397\n",
      "After iteration 268, Loss 2.933182\n",
      "After iteration 269, Loss 2.932974\n",
      "After iteration 270, Loss 2.932774\n",
      "After iteration 271, Loss 2.932581\n",
      "After iteration 272, Loss 2.932396\n",
      "After iteration 273, Loss 2.932218\n",
      "After iteration 274, Loss 2.932046\n",
      "After iteration 275, Loss 2.931881\n",
      "After iteration 276, Loss 2.931723\n",
      "After iteration 277, Loss 2.931570\n",
      "After iteration 278, Loss 2.931423\n",
      "After iteration 279, Loss 2.931282\n",
      "After iteration 280, Loss 2.931145\n",
      "After iteration 281, Loss 2.931014\n",
      "After iteration 282, Loss 2.930888\n",
      "After iteration 283, Loss 2.930767\n",
      "After iteration 284, Loss 2.930650\n",
      "After iteration 285, Loss 2.930537\n",
      "After iteration 286, Loss 2.930429\n",
      "After iteration 287, Loss 2.930325\n",
      "After iteration 288, Loss 2.930225\n",
      "After iteration 289, Loss 2.930129\n",
      "After iteration 290, Loss 2.930035\n",
      "After iteration 291, Loss 2.929945\n",
      "After iteration 292, Loss 2.929859\n",
      "After iteration 293, Loss 2.929776\n",
      "After iteration 294, Loss 2.929696\n",
      "After iteration 295, Loss 2.929620\n",
      "After iteration 296, Loss 2.929547\n",
      "After iteration 297, Loss 2.929475\n",
      "After iteration 298, Loss 2.929407\n",
      "After iteration 299, Loss 2.929341\n",
      "After iteration 300, Loss 2.929278\n",
      "After iteration 301, Loss 2.929216\n",
      "After iteration 302, Loss 2.929158\n",
      "After iteration 303, Loss 2.929101\n",
      "After iteration 304, Loss 2.929046\n",
      "After iteration 305, Loss 2.928993\n",
      "After iteration 306, Loss 2.928944\n",
      "After iteration 307, Loss 2.928895\n",
      "After iteration 308, Loss 2.928850\n",
      "After iteration 309, Loss 2.928803\n",
      "After iteration 310, Loss 2.928760\n",
      "After iteration 311, Loss 2.928719\n",
      "After iteration 312, Loss 2.928679\n",
      "After iteration 313, Loss 2.928639\n",
      "After iteration 314, Loss 2.928602\n",
      "After iteration 315, Loss 2.928567\n",
      "After iteration 316, Loss 2.928532\n",
      "After iteration 317, Loss 2.928500\n",
      "After iteration 318, Loss 2.928467\n",
      "After iteration 319, Loss 2.928437\n",
      "After iteration 320, Loss 2.928407\n",
      "After iteration 321, Loss 2.928379\n",
      "After iteration 322, Loss 2.928352\n",
      "After iteration 323, Loss 2.928325\n",
      "After iteration 324, Loss 2.928300\n",
      "After iteration 325, Loss 2.928275\n",
      "After iteration 326, Loss 2.928252\n",
      "After iteration 327, Loss 2.928228\n",
      "After iteration 328, Loss 2.928207\n",
      "After iteration 329, Loss 2.928187\n",
      "After iteration 330, Loss 2.928166\n",
      "After iteration 331, Loss 2.928146\n",
      "After iteration 332, Loss 2.928128\n",
      "After iteration 333, Loss 2.928111\n",
      "After iteration 334, Loss 2.928092\n",
      "After iteration 335, Loss 2.928075\n",
      "After iteration 336, Loss 2.928060\n",
      "After iteration 337, Loss 2.928044\n",
      "After iteration 338, Loss 2.928030\n",
      "After iteration 339, Loss 2.928015\n",
      "After iteration 340, Loss 2.928001\n",
      "After iteration 341, Loss 2.927989\n",
      "After iteration 342, Loss 2.927976\n",
      "After iteration 343, Loss 2.927963\n",
      "After iteration 344, Loss 2.927951\n",
      "After iteration 345, Loss 2.927940\n",
      "After iteration 346, Loss 2.927929\n",
      "After iteration 347, Loss 2.927917\n",
      "After iteration 348, Loss 2.927908\n",
      "After iteration 349, Loss 2.927898\n",
      "After iteration 350, Loss 2.927889\n",
      "After iteration 351, Loss 2.927880\n",
      "After iteration 352, Loss 2.927871\n",
      "After iteration 353, Loss 2.927863\n",
      "After iteration 354, Loss 2.927854\n",
      "After iteration 355, Loss 2.927847\n",
      "After iteration 356, Loss 2.927840\n",
      "After iteration 357, Loss 2.927831\n",
      "After iteration 358, Loss 2.927825\n",
      "After iteration 359, Loss 2.927819\n",
      "After iteration 360, Loss 2.927812\n",
      "After iteration 361, Loss 2.927807\n",
      "After iteration 362, Loss 2.927801\n",
      "After iteration 363, Loss 2.927794\n",
      "After iteration 364, Loss 2.927789\n",
      "After iteration 365, Loss 2.927783\n",
      "After iteration 366, Loss 2.927779\n",
      "After iteration 367, Loss 2.927774\n",
      "After iteration 368, Loss 2.927768\n",
      "After iteration 369, Loss 2.927763\n",
      "After iteration 370, Loss 2.927760\n",
      "After iteration 371, Loss 2.927755\n",
      "After iteration 372, Loss 2.927751\n",
      "After iteration 373, Loss 2.927747\n",
      "After iteration 374, Loss 2.927744\n",
      "After iteration 375, Loss 2.927740\n",
      "After iteration 376, Loss 2.927737\n",
      "After iteration 377, Loss 2.927733\n",
      "After iteration 378, Loss 2.927730\n",
      "After iteration 379, Loss 2.927727\n",
      "After iteration 380, Loss 2.927723\n",
      "After iteration 381, Loss 2.927720\n",
      "After iteration 382, Loss 2.927718\n",
      "After iteration 383, Loss 2.927715\n",
      "After iteration 384, Loss 2.927713\n",
      "After iteration 385, Loss 2.927710\n",
      "After iteration 386, Loss 2.927708\n",
      "After iteration 387, Loss 2.927705\n",
      "After iteration 388, Loss 2.927704\n",
      "After iteration 389, Loss 2.927700\n",
      "After iteration 390, Loss 2.927699\n",
      "After iteration 391, Loss 2.927696\n",
      "After iteration 392, Loss 2.927695\n",
      "After iteration 393, Loss 2.927692\n",
      "After iteration 394, Loss 2.927692\n",
      "After iteration 395, Loss 2.927690\n",
      "After iteration 396, Loss 2.927688\n",
      "After iteration 397, Loss 2.927686\n",
      "After iteration 398, Loss 2.927685\n",
      "After iteration 399, Loss 2.927683\n",
      "After iteration 400, Loss 2.927682\n",
      "After iteration 401, Loss 2.927680\n",
      "After iteration 402, Loss 2.927679\n",
      "After iteration 403, Loss 2.927679\n",
      "After iteration 404, Loss 2.927677\n",
      "After iteration 405, Loss 2.927675\n",
      "After iteration 406, Loss 2.927675\n",
      "After iteration 407, Loss 2.927673\n",
      "After iteration 408, Loss 2.927673\n",
      "After iteration 409, Loss 2.927671\n",
      "After iteration 410, Loss 2.927670\n",
      "After iteration 411, Loss 2.927670\n",
      "After iteration 412, Loss 2.927669\n",
      "After iteration 413, Loss 2.927667\n",
      "After iteration 414, Loss 2.927667\n",
      "After iteration 415, Loss 2.927666\n",
      "After iteration 416, Loss 2.927666\n",
      "After iteration 417, Loss 2.927664\n",
      "After iteration 418, Loss 2.927664\n",
      "After iteration 419, Loss 2.927663\n",
      "After iteration 420, Loss 2.927662\n",
      "After iteration 421, Loss 2.927663\n",
      "After iteration 422, Loss 2.927662\n",
      "After iteration 423, Loss 2.927661\n",
      "After iteration 424, Loss 2.927660\n",
      "After iteration 425, Loss 2.927660\n",
      "After iteration 426, Loss 2.927659\n",
      "After iteration 427, Loss 2.927659\n",
      "After iteration 428, Loss 2.927658\n",
      "After iteration 429, Loss 2.927657\n",
      "After iteration 430, Loss 2.927657\n",
      "After iteration 431, Loss 2.927657\n",
      "After iteration 432, Loss 2.927656\n",
      "After iteration 433, Loss 2.927656\n",
      "After iteration 434, Loss 2.927656\n",
      "After iteration 435, Loss 2.927655\n",
      "After iteration 436, Loss 2.927655\n",
      "After iteration 437, Loss 2.927655\n",
      "After iteration 438, Loss 2.927654\n",
      "After iteration 439, Loss 2.927654\n",
      "After iteration 440, Loss 2.927654\n",
      "After iteration 441, Loss 2.927653\n",
      "After iteration 442, Loss 2.927653\n",
      "After iteration 443, Loss 2.927653\n",
      "After iteration 444, Loss 2.927653\n",
      "After iteration 445, Loss 2.927653\n",
      "After iteration 446, Loss 2.927653\n",
      "After iteration 447, Loss 2.927651\n",
      "After iteration 448, Loss 2.927650\n",
      "After iteration 449, Loss 2.927651\n",
      "After iteration 450, Loss 2.927651\n",
      "After iteration 451, Loss 2.927651\n",
      "After iteration 452, Loss 2.927651\n",
      "After iteration 453, Loss 2.927650\n",
      "After iteration 454, Loss 2.927650\n",
      "After iteration 455, Loss 2.927650\n",
      "After iteration 456, Loss 2.927650\n",
      "After iteration 457, Loss 2.927650\n",
      "After iteration 458, Loss 2.927649\n",
      "After iteration 459, Loss 2.927649\n",
      "After iteration 460, Loss 2.927649\n",
      "After iteration 461, Loss 2.927650\n",
      "After iteration 462, Loss 2.927649\n",
      "After iteration 463, Loss 2.927649\n",
      "After iteration 464, Loss 2.927649\n",
      "After iteration 465, Loss 2.927649\n",
      "After iteration 466, Loss 2.927649\n",
      "After iteration 467, Loss 2.927648\n",
      "After iteration 468, Loss 2.927648\n",
      "After iteration 469, Loss 2.927648\n",
      "After iteration 470, Loss 2.927648\n",
      "After iteration 471, Loss 2.927648\n",
      "After iteration 472, Loss 2.927648\n",
      "After iteration 473, Loss 2.927648\n",
      "After iteration 474, Loss 2.927648\n",
      "After iteration 475, Loss 2.927648\n",
      "After iteration 476, Loss 2.927647\n",
      "After iteration 477, Loss 2.927648\n",
      "After iteration 478, Loss 2.927648\n",
      "After iteration 479, Loss 2.927647\n",
      "After iteration 480, Loss 2.927647\n",
      "After iteration 481, Loss 2.927647\n",
      "After iteration 482, Loss 2.927647\n",
      "After iteration 483, Loss 2.927648\n",
      "After iteration 484, Loss 2.927647\n",
      "After iteration 485, Loss 2.927647\n",
      "After iteration 486, Loss 2.927647\n",
      "After iteration 487, Loss 2.927648\n",
      "After iteration 488, Loss 2.927647\n",
      "After iteration 489, Loss 2.927647\n",
      "After iteration 490, Loss 2.927646\n",
      "After iteration 491, Loss 2.927647\n",
      "After iteration 492, Loss 2.927647\n",
      "After iteration 493, Loss 2.927646\n",
      "After iteration 494, Loss 2.927647\n",
      "After iteration 495, Loss 2.927646\n",
      "After iteration 496, Loss 2.927646\n",
      "After iteration 497, Loss 2.927647\n",
      "After iteration 498, Loss 2.927646\n",
      "After iteration 499, Loss 2.927647\n",
      "After iteration 500, Loss 2.927646\n",
      "After iteration 501, Loss 2.927647\n",
      "After iteration 502, Loss 2.927647\n",
      "After iteration 503, Loss 2.927647\n",
      "After iteration 504, Loss 2.927647\n",
      "After iteration 505, Loss 2.927646\n",
      "After iteration 506, Loss 2.927646\n",
      "After iteration 507, Loss 2.927646\n",
      "After iteration 508, Loss 2.927646\n",
      "After iteration 509, Loss 2.927646\n",
      "After iteration 510, Loss 2.927646\n",
      "After iteration 511, Loss 2.927646\n",
      "After iteration 512, Loss 2.927646\n",
      "After iteration 513, Loss 2.927645\n",
      "After iteration 514, Loss 2.927646\n",
      "After iteration 515, Loss 2.927646\n",
      "After iteration 516, Loss 2.927646\n",
      "After iteration 517, Loss 2.927647\n",
      "After iteration 518, Loss 2.927646\n",
      "After iteration 519, Loss 2.927647\n",
      "After iteration 520, Loss 2.927646\n",
      "After iteration 521, Loss 2.927646\n",
      "After iteration 522, Loss 2.927646\n",
      "After iteration 523, Loss 2.927647\n",
      "After iteration 524, Loss 2.927646\n",
      "After iteration 525, Loss 2.927646\n",
      "After iteration 526, Loss 2.927647\n",
      "After iteration 527, Loss 2.927646\n",
      "After iteration 528, Loss 2.927646\n",
      "After iteration 529, Loss 2.927647\n",
      "After iteration 530, Loss 2.927646\n",
      "After iteration 531, Loss 2.927646\n",
      "After iteration 532, Loss 2.927646\n",
      "After iteration 533, Loss 2.927646\n",
      "After iteration 534, Loss 2.927646\n",
      "After iteration 535, Loss 2.927647\n",
      "After iteration 536, Loss 2.927646\n",
      "After iteration 537, Loss 2.927646\n",
      "After iteration 538, Loss 2.927646\n",
      "After iteration 539, Loss 2.927645\n",
      "After iteration 540, Loss 2.927645\n",
      "After iteration 541, Loss 2.927646\n",
      "After iteration 542, Loss 2.927646\n",
      "After iteration 543, Loss 2.927646\n",
      "After iteration 544, Loss 2.927646\n",
      "After iteration 545, Loss 2.927646\n",
      "After iteration 546, Loss 2.927646\n",
      "After iteration 547, Loss 2.927645\n",
      "After iteration 548, Loss 2.927646\n",
      "After iteration 549, Loss 2.927645\n",
      "After iteration 550, Loss 2.927646\n",
      "After iteration 551, Loss 2.927646\n",
      "After iteration 552, Loss 2.927646\n",
      "After iteration 553, Loss 2.927645\n",
      "After iteration 554, Loss 2.927645\n",
      "After iteration 555, Loss 2.927645\n",
      "After iteration 556, Loss 2.927646\n",
      "After iteration 557, Loss 2.927645\n",
      "After iteration 558, Loss 2.927646\n",
      "After iteration 559, Loss 2.927646\n",
      "After iteration 560, Loss 2.927646\n",
      "After iteration 561, Loss 2.927646\n",
      "After iteration 562, Loss 2.927646\n",
      "After iteration 563, Loss 2.927646\n",
      "After iteration 564, Loss 2.927646\n",
      "After iteration 565, Loss 2.927645\n",
      "After iteration 566, Loss 2.927646\n",
      "After iteration 567, Loss 2.927645\n",
      "After iteration 568, Loss 2.927646\n",
      "After iteration 569, Loss 2.927646\n",
      "After iteration 570, Loss 2.927646\n",
      "After iteration 571, Loss 2.927645\n",
      "After iteration 572, Loss 2.927645\n",
      "After iteration 573, Loss 2.927646\n",
      "After iteration 574, Loss 2.927646\n",
      "After iteration 575, Loss 2.927646\n",
      "After iteration 576, Loss 2.927646\n",
      "After iteration 577, Loss 2.927646\n",
      "After iteration 578, Loss 2.927644\n",
      "After iteration 579, Loss 2.927646\n",
      "After iteration 580, Loss 2.927645\n",
      "After iteration 581, Loss 2.927646\n",
      "After iteration 582, Loss 2.927646\n",
      "After iteration 583, Loss 2.927646\n",
      "After iteration 584, Loss 2.927646\n",
      "After iteration 585, Loss 2.927645\n",
      "After iteration 586, Loss 2.927646\n",
      "After iteration 587, Loss 2.927646\n",
      "After iteration 588, Loss 2.927645\n",
      "After iteration 589, Loss 2.927646\n",
      "After iteration 590, Loss 2.927646\n",
      "After iteration 591, Loss 2.927644\n",
      "After iteration 592, Loss 2.927646\n",
      "After iteration 593, Loss 2.927646\n",
      "After iteration 594, Loss 2.927644\n",
      "After iteration 595, Loss 2.927644\n",
      "After iteration 596, Loss 2.927645\n",
      "After iteration 597, Loss 2.927645\n",
      "After iteration 598, Loss 2.927645\n",
      "After iteration 599, Loss 2.927645\n",
      "After iteration 600, Loss 2.927645\n",
      "After iteration 601, Loss 2.927644\n",
      "After iteration 602, Loss 2.927645\n",
      "After iteration 603, Loss 2.927646\n",
      "After iteration 604, Loss 2.927646\n",
      "After iteration 605, Loss 2.927645\n",
      "After iteration 606, Loss 2.927646\n",
      "After iteration 607, Loss 2.927646\n",
      "After iteration 608, Loss 2.927645\n",
      "After iteration 609, Loss 2.927645\n",
      "After iteration 610, Loss 2.927645\n",
      "After iteration 611, Loss 2.927647\n",
      "After iteration 612, Loss 2.927646\n",
      "After iteration 613, Loss 2.927644\n",
      "After iteration 614, Loss 2.927645\n",
      "After iteration 615, Loss 2.927646\n",
      "After iteration 616, Loss 2.927646\n",
      "After iteration 617, Loss 2.927646\n",
      "After iteration 618, Loss 2.927645\n",
      "After iteration 619, Loss 2.927645\n",
      "After iteration 620, Loss 2.927646\n",
      "After iteration 621, Loss 2.927644\n",
      "After iteration 622, Loss 2.927645\n",
      "After iteration 623, Loss 2.927647\n",
      "After iteration 624, Loss 2.927645\n",
      "After iteration 625, Loss 2.927646\n",
      "After iteration 626, Loss 2.927646\n",
      "After iteration 627, Loss 2.927646\n",
      "After iteration 628, Loss 2.927645\n",
      "After iteration 629, Loss 2.927645\n",
      "After iteration 630, Loss 2.927646\n",
      "After iteration 631, Loss 2.927646\n",
      "After iteration 632, Loss 2.927646\n",
      "After iteration 633, Loss 2.927645\n",
      "After iteration 634, Loss 2.927645\n",
      "After iteration 635, Loss 2.927645\n",
      "After iteration 636, Loss 2.927646\n",
      "After iteration 637, Loss 2.927645\n",
      "After iteration 638, Loss 2.927646\n",
      "After iteration 639, Loss 2.927645\n",
      "After iteration 640, Loss 2.927647\n",
      "After iteration 641, Loss 2.927646\n",
      "After iteration 642, Loss 2.927646\n",
      "After iteration 643, Loss 2.927645\n",
      "After iteration 644, Loss 2.927645\n",
      "After iteration 645, Loss 2.927645\n",
      "After iteration 646, Loss 2.927646\n",
      "After iteration 647, Loss 2.927644\n",
      "After iteration 648, Loss 2.927645\n",
      "After iteration 649, Loss 2.927646\n",
      "After iteration 650, Loss 2.927646\n",
      "After iteration 651, Loss 2.927644\n",
      "After iteration 652, Loss 2.927644\n",
      "After iteration 653, Loss 2.927646\n",
      "After iteration 654, Loss 2.927644\n",
      "After iteration 655, Loss 2.927645\n",
      "After iteration 656, Loss 2.927645\n",
      "After iteration 657, Loss 2.927646\n",
      "After iteration 658, Loss 2.927647\n",
      "After iteration 659, Loss 2.927646\n",
      "After iteration 660, Loss 2.927646\n",
      "After iteration 661, Loss 2.927645\n",
      "After iteration 662, Loss 2.927647\n",
      "After iteration 663, Loss 2.927645\n",
      "After iteration 664, Loss 2.927646\n",
      "After iteration 665, Loss 2.927645\n",
      "After iteration 666, Loss 2.927646\n",
      "After iteration 667, Loss 2.927646\n",
      "After iteration 668, Loss 2.927646\n",
      "After iteration 669, Loss 2.927647\n",
      "After iteration 670, Loss 2.927646\n",
      "After iteration 671, Loss 2.927645\n",
      "After iteration 672, Loss 2.927645\n",
      "After iteration 673, Loss 2.927646\n",
      "After iteration 674, Loss 2.927646\n",
      "After iteration 675, Loss 2.927646\n",
      "After iteration 676, Loss 2.927645\n",
      "After iteration 677, Loss 2.927646\n",
      "After iteration 678, Loss 2.927647\n",
      "After iteration 679, Loss 2.927645\n",
      "After iteration 680, Loss 2.927646\n",
      "After iteration 681, Loss 2.927645\n",
      "After iteration 682, Loss 2.927646\n",
      "After iteration 683, Loss 2.927645\n",
      "After iteration 684, Loss 2.927645\n",
      "After iteration 685, Loss 2.927645\n",
      "After iteration 686, Loss 2.927645\n",
      "After iteration 687, Loss 2.927645\n",
      "After iteration 688, Loss 2.927645\n",
      "After iteration 689, Loss 2.927645\n",
      "After iteration 690, Loss 2.927645\n",
      "After iteration 691, Loss 2.927645\n",
      "After iteration 692, Loss 2.927645\n",
      "After iteration 693, Loss 2.927645\n",
      "After iteration 694, Loss 2.927645\n",
      "After iteration 695, Loss 2.927645\n",
      "After iteration 696, Loss 2.927645\n",
      "After iteration 697, Loss 2.927645\n",
      "After iteration 698, Loss 2.927645\n",
      "After iteration 699, Loss 2.927645\n",
      "After iteration 700, Loss 2.927645\n",
      "After iteration 701, Loss 2.927645\n",
      "After iteration 702, Loss 2.927645\n",
      "After iteration 703, Loss 2.927645\n",
      "After iteration 704, Loss 2.927645\n",
      "After iteration 705, Loss 2.927645\n",
      "After iteration 706, Loss 2.927645\n",
      "After iteration 707, Loss 2.927645\n",
      "After iteration 708, Loss 2.927645\n",
      "After iteration 709, Loss 2.927645\n",
      "After iteration 710, Loss 2.927645\n",
      "After iteration 711, Loss 2.927645\n",
      "After iteration 712, Loss 2.927645\n",
      "After iteration 713, Loss 2.927645\n",
      "After iteration 714, Loss 2.927645\n",
      "After iteration 715, Loss 2.927645\n",
      "After iteration 716, Loss 2.927645\n",
      "After iteration 717, Loss 2.927645\n",
      "After iteration 718, Loss 2.927645\n",
      "After iteration 719, Loss 2.927645\n",
      "After iteration 720, Loss 2.927645\n",
      "After iteration 721, Loss 2.927645\n",
      "After iteration 722, Loss 2.927645\n",
      "After iteration 723, Loss 2.927645\n",
      "After iteration 724, Loss 2.927645\n",
      "After iteration 725, Loss 2.927645\n",
      "After iteration 726, Loss 2.927645\n",
      "After iteration 727, Loss 2.927645\n",
      "After iteration 728, Loss 2.927645\n",
      "After iteration 729, Loss 2.927645\n",
      "After iteration 730, Loss 2.927645\n",
      "After iteration 731, Loss 2.927645\n",
      "After iteration 732, Loss 2.927645\n",
      "After iteration 733, Loss 2.927645\n",
      "After iteration 734, Loss 2.927645\n",
      "After iteration 735, Loss 2.927645\n",
      "After iteration 736, Loss 2.927645\n",
      "After iteration 737, Loss 2.927645\n",
      "After iteration 738, Loss 2.927645\n",
      "After iteration 739, Loss 2.927645\n",
      "After iteration 740, Loss 2.927645\n",
      "After iteration 741, Loss 2.927645\n",
      "After iteration 742, Loss 2.927645\n",
      "After iteration 743, Loss 2.927645\n",
      "After iteration 744, Loss 2.927645\n",
      "After iteration 745, Loss 2.927645\n",
      "After iteration 746, Loss 2.927645\n",
      "After iteration 747, Loss 2.927645\n",
      "After iteration 748, Loss 2.927645\n",
      "After iteration 749, Loss 2.927645\n",
      "After iteration 750, Loss 2.927645\n",
      "After iteration 751, Loss 2.927645\n",
      "After iteration 752, Loss 2.927645\n",
      "After iteration 753, Loss 2.927645\n",
      "After iteration 754, Loss 2.927645\n",
      "After iteration 755, Loss 2.927645\n",
      "After iteration 756, Loss 2.927645\n",
      "After iteration 757, Loss 2.927645\n",
      "After iteration 758, Loss 2.927645\n",
      "After iteration 759, Loss 2.927645\n",
      "After iteration 760, Loss 2.927645\n",
      "After iteration 761, Loss 2.927645\n",
      "After iteration 762, Loss 2.927645\n",
      "After iteration 763, Loss 2.927645\n",
      "After iteration 764, Loss 2.927645\n",
      "After iteration 765, Loss 2.927645\n",
      "After iteration 766, Loss 2.927645\n",
      "After iteration 767, Loss 2.927645\n",
      "After iteration 768, Loss 2.927645\n",
      "After iteration 769, Loss 2.927645\n",
      "After iteration 770, Loss 2.927645\n",
      "After iteration 771, Loss 2.927645\n",
      "After iteration 772, Loss 2.927645\n",
      "After iteration 773, Loss 2.927645\n",
      "After iteration 774, Loss 2.927645\n",
      "After iteration 775, Loss 2.927645\n",
      "After iteration 776, Loss 2.927645\n",
      "After iteration 777, Loss 2.927645\n",
      "After iteration 778, Loss 2.927645\n",
      "After iteration 779, Loss 2.927645\n",
      "After iteration 780, Loss 2.927645\n",
      "After iteration 781, Loss 2.927645\n",
      "After iteration 782, Loss 2.927645\n",
      "After iteration 783, Loss 2.927645\n",
      "After iteration 784, Loss 2.927645\n",
      "After iteration 785, Loss 2.927645\n",
      "After iteration 786, Loss 2.927645\n",
      "After iteration 787, Loss 2.927645\n",
      "After iteration 788, Loss 2.927645\n",
      "After iteration 789, Loss 2.927645\n",
      "After iteration 790, Loss 2.927645\n",
      "After iteration 791, Loss 2.927645\n",
      "After iteration 792, Loss 2.927645\n",
      "After iteration 793, Loss 2.927645\n",
      "After iteration 794, Loss 2.927645\n",
      "After iteration 795, Loss 2.927645\n",
      "After iteration 796, Loss 2.927645\n",
      "After iteration 797, Loss 2.927645\n",
      "After iteration 798, Loss 2.927645\n",
      "After iteration 799, Loss 2.927645\n",
      "After iteration 800, Loss 2.927645\n",
      "After iteration 801, Loss 2.927645\n",
      "After iteration 802, Loss 2.927645\n",
      "After iteration 803, Loss 2.927645\n",
      "After iteration 804, Loss 2.927645\n",
      "After iteration 805, Loss 2.927645\n",
      "After iteration 806, Loss 2.927645\n",
      "After iteration 807, Loss 2.927645\n",
      "After iteration 808, Loss 2.927645\n",
      "After iteration 809, Loss 2.927645\n",
      "After iteration 810, Loss 2.927645\n",
      "After iteration 811, Loss 2.927645\n",
      "After iteration 812, Loss 2.927645\n",
      "After iteration 813, Loss 2.927645\n",
      "After iteration 814, Loss 2.927645\n",
      "After iteration 815, Loss 2.927645\n",
      "After iteration 816, Loss 2.927645\n",
      "After iteration 817, Loss 2.927645\n",
      "After iteration 818, Loss 2.927645\n",
      "After iteration 819, Loss 2.927645\n",
      "After iteration 820, Loss 2.927645\n",
      "After iteration 821, Loss 2.927645\n",
      "After iteration 822, Loss 2.927645\n",
      "After iteration 823, Loss 2.927645\n",
      "After iteration 824, Loss 2.927645\n",
      "After iteration 825, Loss 2.927645\n",
      "After iteration 826, Loss 2.927645\n",
      "After iteration 827, Loss 2.927645\n",
      "After iteration 828, Loss 2.927645\n",
      "After iteration 829, Loss 2.927645\n",
      "After iteration 830, Loss 2.927645\n",
      "After iteration 831, Loss 2.927645\n",
      "After iteration 832, Loss 2.927645\n",
      "After iteration 833, Loss 2.927645\n",
      "After iteration 834, Loss 2.927645\n",
      "After iteration 835, Loss 2.927645\n",
      "After iteration 836, Loss 2.927645\n",
      "After iteration 837, Loss 2.927645\n",
      "After iteration 838, Loss 2.927645\n",
      "After iteration 839, Loss 2.927645\n",
      "After iteration 840, Loss 2.927645\n",
      "After iteration 841, Loss 2.927645\n",
      "After iteration 842, Loss 2.927645\n",
      "After iteration 843, Loss 2.927645\n",
      "After iteration 844, Loss 2.927645\n",
      "After iteration 845, Loss 2.927645\n",
      "After iteration 846, Loss 2.927645\n",
      "After iteration 847, Loss 2.927645\n",
      "After iteration 848, Loss 2.927645\n",
      "After iteration 849, Loss 2.927645\n",
      "After iteration 850, Loss 2.927645\n",
      "After iteration 851, Loss 2.927645\n",
      "After iteration 852, Loss 2.927645\n",
      "After iteration 853, Loss 2.927645\n",
      "After iteration 854, Loss 2.927645\n",
      "After iteration 855, Loss 2.927645\n",
      "After iteration 856, Loss 2.927645\n",
      "After iteration 857, Loss 2.927645\n",
      "After iteration 858, Loss 2.927645\n",
      "After iteration 859, Loss 2.927645\n",
      "After iteration 860, Loss 2.927645\n",
      "After iteration 861, Loss 2.927645\n",
      "After iteration 862, Loss 2.927645\n",
      "After iteration 863, Loss 2.927645\n",
      "After iteration 864, Loss 2.927645\n",
      "After iteration 865, Loss 2.927645\n",
      "After iteration 866, Loss 2.927645\n",
      "After iteration 867, Loss 2.927645\n",
      "After iteration 868, Loss 2.927645\n",
      "After iteration 869, Loss 2.927645\n",
      "After iteration 870, Loss 2.927645\n",
      "After iteration 871, Loss 2.927645\n",
      "After iteration 872, Loss 2.927645\n",
      "After iteration 873, Loss 2.927645\n",
      "After iteration 874, Loss 2.927645\n",
      "After iteration 875, Loss 2.927645\n",
      "After iteration 876, Loss 2.927645\n",
      "After iteration 877, Loss 2.927645\n",
      "After iteration 878, Loss 2.927645\n",
      "After iteration 879, Loss 2.927645\n",
      "After iteration 880, Loss 2.927645\n",
      "After iteration 881, Loss 2.927645\n",
      "After iteration 882, Loss 2.927645\n",
      "After iteration 883, Loss 2.927645\n",
      "After iteration 884, Loss 2.927645\n",
      "After iteration 885, Loss 2.927645\n",
      "After iteration 886, Loss 2.927645\n",
      "After iteration 887, Loss 2.927645\n",
      "After iteration 888, Loss 2.927645\n",
      "After iteration 889, Loss 2.927645\n",
      "After iteration 890, Loss 2.927645\n",
      "After iteration 891, Loss 2.927645\n",
      "After iteration 892, Loss 2.927645\n",
      "After iteration 893, Loss 2.927645\n",
      "After iteration 894, Loss 2.927645\n",
      "After iteration 895, Loss 2.927645\n",
      "After iteration 896, Loss 2.927645\n",
      "After iteration 897, Loss 2.927645\n",
      "After iteration 898, Loss 2.927645\n",
      "After iteration 899, Loss 2.927645\n",
      "After iteration 900, Loss 2.927645\n",
      "After iteration 901, Loss 2.927645\n",
      "After iteration 902, Loss 2.927645\n",
      "After iteration 903, Loss 2.927645\n",
      "After iteration 904, Loss 2.927645\n",
      "After iteration 905, Loss 2.927645\n",
      "After iteration 906, Loss 2.927645\n",
      "After iteration 907, Loss 2.927645\n",
      "After iteration 908, Loss 2.927645\n",
      "After iteration 909, Loss 2.927645\n",
      "After iteration 910, Loss 2.927645\n",
      "After iteration 911, Loss 2.927645\n",
      "After iteration 912, Loss 2.927645\n",
      "After iteration 913, Loss 2.927645\n",
      "After iteration 914, Loss 2.927645\n",
      "After iteration 915, Loss 2.927645\n",
      "After iteration 916, Loss 2.927645\n",
      "After iteration 917, Loss 2.927645\n",
      "After iteration 918, Loss 2.927645\n",
      "After iteration 919, Loss 2.927645\n",
      "After iteration 920, Loss 2.927645\n",
      "After iteration 921, Loss 2.927645\n",
      "After iteration 922, Loss 2.927645\n",
      "After iteration 923, Loss 2.927645\n",
      "After iteration 924, Loss 2.927645\n",
      "After iteration 925, Loss 2.927645\n",
      "After iteration 926, Loss 2.927645\n",
      "After iteration 927, Loss 2.927645\n",
      "After iteration 928, Loss 2.927645\n",
      "After iteration 929, Loss 2.927645\n",
      "After iteration 930, Loss 2.927645\n",
      "After iteration 931, Loss 2.927645\n",
      "After iteration 932, Loss 2.927645\n",
      "After iteration 933, Loss 2.927645\n",
      "After iteration 934, Loss 2.927645\n",
      "After iteration 935, Loss 2.927645\n",
      "After iteration 936, Loss 2.927645\n",
      "After iteration 937, Loss 2.927645\n",
      "After iteration 938, Loss 2.927645\n",
      "After iteration 939, Loss 2.927645\n",
      "After iteration 940, Loss 2.927645\n",
      "After iteration 941, Loss 2.927645\n",
      "After iteration 942, Loss 2.927645\n",
      "After iteration 943, Loss 2.927645\n",
      "After iteration 944, Loss 2.927645\n",
      "After iteration 945, Loss 2.927645\n",
      "After iteration 946, Loss 2.927645\n",
      "After iteration 947, Loss 2.927645\n",
      "After iteration 948, Loss 2.927645\n",
      "After iteration 949, Loss 2.927645\n",
      "After iteration 950, Loss 2.927645\n",
      "After iteration 951, Loss 2.927645\n",
      "After iteration 952, Loss 2.927645\n",
      "After iteration 953, Loss 2.927645\n",
      "After iteration 954, Loss 2.927645\n",
      "After iteration 955, Loss 2.927645\n",
      "After iteration 956, Loss 2.927645\n",
      "After iteration 957, Loss 2.927645\n",
      "After iteration 958, Loss 2.927645\n",
      "After iteration 959, Loss 2.927645\n",
      "After iteration 960, Loss 2.927645\n",
      "After iteration 961, Loss 2.927645\n",
      "After iteration 962, Loss 2.927645\n",
      "After iteration 963, Loss 2.927645\n",
      "After iteration 964, Loss 2.927645\n",
      "After iteration 965, Loss 2.927645\n",
      "After iteration 966, Loss 2.927645\n",
      "After iteration 967, Loss 2.927645\n",
      "After iteration 968, Loss 2.927645\n",
      "After iteration 969, Loss 2.927645\n",
      "After iteration 970, Loss 2.927645\n",
      "After iteration 971, Loss 2.927645\n",
      "After iteration 972, Loss 2.927645\n",
      "After iteration 973, Loss 2.927645\n",
      "After iteration 974, Loss 2.927645\n",
      "After iteration 975, Loss 2.927645\n",
      "After iteration 976, Loss 2.927645\n",
      "After iteration 977, Loss 2.927645\n",
      "After iteration 978, Loss 2.927645\n",
      "After iteration 979, Loss 2.927645\n",
      "After iteration 980, Loss 2.927645\n",
      "After iteration 981, Loss 2.927645\n",
      "After iteration 982, Loss 2.927645\n",
      "After iteration 983, Loss 2.927645\n",
      "After iteration 984, Loss 2.927645\n",
      "After iteration 985, Loss 2.927645\n",
      "After iteration 986, Loss 2.927645\n",
      "After iteration 987, Loss 2.927645\n",
      "After iteration 988, Loss 2.927645\n",
      "After iteration 989, Loss 2.927645\n",
      "After iteration 990, Loss 2.927645\n",
      "After iteration 991, Loss 2.927645\n",
      "After iteration 992, Loss 2.927645\n",
      "After iteration 993, Loss 2.927645\n",
      "After iteration 994, Loss 2.927645\n",
      "After iteration 995, Loss 2.927645\n",
      "After iteration 996, Loss 2.927645\n",
      "After iteration 997, Loss 2.927645\n",
      "After iteration 998, Loss 2.927645\n",
      "After iteration 999, Loss 2.927645\n",
      "After iteration 1000, Loss 2.927645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 9.0349, 10.5000], requires_grad=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Sprawdź, jak w porównaniu do `SGD` poradzi sobie optymalizator `Adam` dla `learning_rate = 0.01` dla nieznormalizowanego (oryginalnego) tensora danych wejściowych `t_u`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:38:36.796531Z",
     "start_time": "2025-03-20T16:38:36.395827Z"
    }
   },
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "optim_Adam = optim.Adam([params], lr=0.01)\n",
    "training_loop_optim(1000, optim_Adam, params, t_u, t_c)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After iteration 1, Loss 1763.884766\n",
      "After iteration 2, Loss 1718.190308\n",
      "After iteration 3, Loss 1673.121216\n",
      "After iteration 4, Loss 1628.688721\n",
      "After iteration 5, Loss 1584.901978\n",
      "After iteration 6, Loss 1541.771606\n",
      "After iteration 7, Loss 1499.305176\n",
      "After iteration 8, Loss 1457.512085\n",
      "After iteration 9, Loss 1416.400391\n",
      "After iteration 10, Loss 1375.976685\n",
      "After iteration 11, Loss 1336.247925\n",
      "After iteration 12, Loss 1297.220581\n",
      "After iteration 13, Loss 1258.899414\n",
      "After iteration 14, Loss 1221.289429\n",
      "After iteration 15, Loss 1184.394653\n",
      "After iteration 16, Loss 1148.218384\n",
      "After iteration 17, Loss 1112.763184\n",
      "After iteration 18, Loss 1078.031250\n",
      "After iteration 19, Loss 1044.023438\n",
      "After iteration 20, Loss 1010.740662\n",
      "After iteration 21, Loss 978.182800\n",
      "After iteration 22, Loss 946.348877\n",
      "After iteration 23, Loss 915.237366\n",
      "After iteration 24, Loss 884.846436\n",
      "After iteration 25, Loss 855.173401\n",
      "After iteration 26, Loss 826.214478\n",
      "After iteration 27, Loss 797.966370\n",
      "After iteration 28, Loss 770.423889\n",
      "After iteration 29, Loss 743.582336\n",
      "After iteration 30, Loss 717.436035\n",
      "After iteration 31, Loss 691.978760\n",
      "After iteration 32, Loss 667.204041\n",
      "After iteration 33, Loss 643.104736\n",
      "After iteration 34, Loss 619.673523\n",
      "After iteration 35, Loss 596.902100\n",
      "After iteration 36, Loss 574.782410\n",
      "After iteration 37, Loss 553.305908\n",
      "After iteration 38, Loss 532.463379\n",
      "After iteration 39, Loss 512.245667\n",
      "After iteration 40, Loss 492.642944\n",
      "After iteration 41, Loss 473.645538\n",
      "After iteration 42, Loss 455.243042\n",
      "After iteration 43, Loss 437.425507\n",
      "After iteration 44, Loss 420.182129\n",
      "After iteration 45, Loss 403.502258\n",
      "After iteration 46, Loss 387.375092\n",
      "After iteration 47, Loss 371.789520\n",
      "After iteration 48, Loss 356.734680\n",
      "After iteration 49, Loss 342.199219\n",
      "After iteration 50, Loss 328.171692\n",
      "After iteration 51, Loss 314.640961\n",
      "After iteration 52, Loss 301.595520\n",
      "After iteration 53, Loss 289.024139\n",
      "After iteration 54, Loss 276.915222\n",
      "After iteration 55, Loss 265.257477\n",
      "After iteration 56, Loss 254.039459\n",
      "After iteration 57, Loss 243.249893\n",
      "After iteration 58, Loss 232.877441\n",
      "After iteration 59, Loss 222.910828\n",
      "After iteration 60, Loss 213.338913\n",
      "After iteration 61, Loss 204.150589\n",
      "After iteration 62, Loss 195.334961\n",
      "After iteration 63, Loss 186.880997\n",
      "After iteration 64, Loss 178.778000\n",
      "After iteration 65, Loss 171.015335\n",
      "After iteration 66, Loss 163.582504\n",
      "After iteration 67, Loss 156.469070\n",
      "After iteration 68, Loss 149.664825\n",
      "After iteration 69, Loss 143.159683\n",
      "After iteration 70, Loss 136.943756\n",
      "After iteration 71, Loss 131.007278\n",
      "After iteration 72, Loss 125.340645\n",
      "After iteration 73, Loss 119.934448\n",
      "After iteration 74, Loss 114.779472\n",
      "After iteration 75, Loss 109.866676\n",
      "After iteration 76, Loss 105.187225\n",
      "After iteration 77, Loss 100.732414\n",
      "After iteration 78, Loss 96.493782\n",
      "After iteration 79, Loss 92.463043\n",
      "After iteration 80, Loss 88.632133\n",
      "After iteration 81, Loss 84.993141\n",
      "After iteration 82, Loss 81.538414\n",
      "After iteration 83, Loss 78.260452\n",
      "After iteration 84, Loss 75.151955\n",
      "After iteration 85, Loss 72.205856\n",
      "After iteration 86, Loss 69.415237\n",
      "After iteration 87, Loss 66.773468\n",
      "After iteration 88, Loss 64.274040\n",
      "After iteration 89, Loss 61.910660\n",
      "After iteration 90, Loss 59.677231\n",
      "After iteration 91, Loss 57.567867\n",
      "After iteration 92, Loss 55.576855\n",
      "After iteration 93, Loss 53.698681\n",
      "After iteration 94, Loss 51.928028\n",
      "After iteration 95, Loss 50.259727\n",
      "After iteration 96, Loss 48.688847\n",
      "After iteration 97, Loss 47.210617\n",
      "After iteration 98, Loss 45.820408\n",
      "After iteration 99, Loss 44.513805\n",
      "After iteration 100, Loss 43.286556\n",
      "After iteration 101, Loss 42.134556\n",
      "After iteration 102, Loss 41.053883\n",
      "After iteration 103, Loss 40.040768\n",
      "After iteration 104, Loss 39.091610\n",
      "After iteration 105, Loss 38.202930\n",
      "After iteration 106, Loss 37.371433\n",
      "After iteration 107, Loss 36.593948\n",
      "After iteration 108, Loss 35.867447\n",
      "After iteration 109, Loss 35.189037\n",
      "After iteration 110, Loss 34.555965\n",
      "After iteration 111, Loss 33.965599\n",
      "After iteration 112, Loss 33.415424\n",
      "After iteration 113, Loss 32.903069\n",
      "After iteration 114, Loss 32.426243\n",
      "After iteration 115, Loss 31.982813\n",
      "After iteration 116, Loss 31.570707\n",
      "After iteration 117, Loss 31.187994\n",
      "After iteration 118, Loss 30.832819\n",
      "After iteration 119, Loss 30.503448\n",
      "After iteration 120, Loss 30.198212\n",
      "After iteration 121, Loss 29.915550\n",
      "After iteration 122, Loss 29.653986\n",
      "After iteration 123, Loss 29.412109\n",
      "After iteration 124, Loss 29.188616\n",
      "After iteration 125, Loss 28.982252\n",
      "After iteration 126, Loss 28.791838\n",
      "After iteration 127, Loss 28.616278\n",
      "After iteration 128, Loss 28.454531\n",
      "After iteration 129, Loss 28.305622\n",
      "After iteration 130, Loss 28.168629\n",
      "After iteration 131, Loss 28.042688\n",
      "After iteration 132, Loss 27.927004\n",
      "After iteration 133, Loss 27.820812\n",
      "After iteration 134, Loss 27.723394\n",
      "After iteration 135, Loss 27.634111\n",
      "After iteration 136, Loss 27.552320\n",
      "After iteration 137, Loss 27.477461\n",
      "After iteration 138, Loss 27.408985\n",
      "After iteration 139, Loss 27.346394\n",
      "After iteration 140, Loss 27.289209\n",
      "After iteration 141, Loss 27.237007\n",
      "After iteration 142, Loss 27.189381\n",
      "After iteration 143, Loss 27.145948\n",
      "After iteration 144, Loss 27.106371\n",
      "After iteration 145, Loss 27.070312\n",
      "After iteration 146, Loss 27.037481\n",
      "After iteration 147, Loss 27.007597\n",
      "After iteration 148, Loss 26.980406\n",
      "After iteration 149, Loss 26.955671\n",
      "After iteration 150, Loss 26.933174\n",
      "After iteration 151, Loss 26.912714\n",
      "After iteration 152, Loss 26.894102\n",
      "After iteration 153, Loss 26.877169\n",
      "After iteration 154, Loss 26.861767\n",
      "After iteration 155, Loss 26.847742\n",
      "After iteration 156, Loss 26.834959\n",
      "After iteration 157, Loss 26.823311\n",
      "After iteration 158, Loss 26.812677\n",
      "After iteration 159, Loss 26.802959\n",
      "After iteration 160, Loss 26.794069\n",
      "After iteration 161, Loss 26.785917\n",
      "After iteration 162, Loss 26.778429\n",
      "After iteration 163, Loss 26.771534\n",
      "After iteration 164, Loss 26.765173\n",
      "After iteration 165, Loss 26.759285\n",
      "After iteration 166, Loss 26.753828\n",
      "After iteration 167, Loss 26.748732\n",
      "After iteration 168, Loss 26.743975\n",
      "After iteration 169, Loss 26.739513\n",
      "After iteration 170, Loss 26.735313\n",
      "After iteration 171, Loss 26.731346\n",
      "After iteration 172, Loss 26.727577\n",
      "After iteration 173, Loss 26.723982\n",
      "After iteration 174, Loss 26.720539\n",
      "After iteration 175, Loss 26.717241\n",
      "After iteration 176, Loss 26.714050\n",
      "After iteration 177, Loss 26.710968\n",
      "After iteration 178, Loss 26.707966\n",
      "After iteration 179, Loss 26.705048\n",
      "After iteration 180, Loss 26.702181\n",
      "After iteration 181, Loss 26.699373\n",
      "After iteration 182, Loss 26.696611\n",
      "After iteration 183, Loss 26.693892\n",
      "After iteration 184, Loss 26.691195\n",
      "After iteration 185, Loss 26.688524\n",
      "After iteration 186, Loss 26.685877\n",
      "After iteration 187, Loss 26.683241\n",
      "After iteration 188, Loss 26.680616\n",
      "After iteration 189, Loss 26.678011\n",
      "After iteration 190, Loss 26.675404\n",
      "After iteration 191, Loss 26.672796\n",
      "After iteration 192, Loss 26.670195\n",
      "After iteration 193, Loss 26.667591\n",
      "After iteration 194, Loss 26.664989\n",
      "After iteration 195, Loss 26.662376\n",
      "After iteration 196, Loss 26.659765\n",
      "After iteration 197, Loss 26.657148\n",
      "After iteration 198, Loss 26.654522\n",
      "After iteration 199, Loss 26.651894\n",
      "After iteration 200, Loss 26.649263\n",
      "After iteration 201, Loss 26.646620\n",
      "After iteration 202, Loss 26.643970\n",
      "After iteration 203, Loss 26.641314\n",
      "After iteration 204, Loss 26.638649\n",
      "After iteration 205, Loss 26.635979\n",
      "After iteration 206, Loss 26.633295\n",
      "After iteration 207, Loss 26.630610\n",
      "After iteration 208, Loss 26.627913\n",
      "After iteration 209, Loss 26.625216\n",
      "After iteration 210, Loss 26.622509\n",
      "After iteration 211, Loss 26.619793\n",
      "After iteration 212, Loss 26.617065\n",
      "After iteration 213, Loss 26.614326\n",
      "After iteration 214, Loss 26.611595\n",
      "After iteration 215, Loss 26.608843\n",
      "After iteration 216, Loss 26.606092\n",
      "After iteration 217, Loss 26.603331\n",
      "After iteration 218, Loss 26.600563\n",
      "After iteration 219, Loss 26.597792\n",
      "After iteration 220, Loss 26.595007\n",
      "After iteration 221, Loss 26.592218\n",
      "After iteration 222, Loss 26.589424\n",
      "After iteration 223, Loss 26.586626\n",
      "After iteration 224, Loss 26.583820\n",
      "After iteration 225, Loss 26.580999\n",
      "After iteration 226, Loss 26.578180\n",
      "After iteration 227, Loss 26.575350\n",
      "After iteration 228, Loss 26.572521\n",
      "After iteration 229, Loss 26.569679\n",
      "After iteration 230, Loss 26.566828\n",
      "After iteration 231, Loss 26.563971\n",
      "After iteration 232, Loss 26.561115\n",
      "After iteration 233, Loss 26.558256\n",
      "After iteration 234, Loss 26.555378\n",
      "After iteration 235, Loss 26.552502\n",
      "After iteration 236, Loss 26.549614\n",
      "After iteration 237, Loss 26.546724\n",
      "After iteration 238, Loss 26.543823\n",
      "After iteration 239, Loss 26.540922\n",
      "After iteration 240, Loss 26.538008\n",
      "After iteration 241, Loss 26.535089\n",
      "After iteration 242, Loss 26.532164\n",
      "After iteration 243, Loss 26.529236\n",
      "After iteration 244, Loss 26.526295\n",
      "After iteration 245, Loss 26.523357\n",
      "After iteration 246, Loss 26.520414\n",
      "After iteration 247, Loss 26.517456\n",
      "After iteration 248, Loss 26.514494\n",
      "After iteration 249, Loss 26.511524\n",
      "After iteration 250, Loss 26.508551\n",
      "After iteration 251, Loss 26.505573\n",
      "After iteration 252, Loss 26.502586\n",
      "After iteration 253, Loss 26.499592\n",
      "After iteration 254, Loss 26.496599\n",
      "After iteration 255, Loss 26.493593\n",
      "After iteration 256, Loss 26.490582\n",
      "After iteration 257, Loss 26.487566\n",
      "After iteration 258, Loss 26.484541\n",
      "After iteration 259, Loss 26.481508\n",
      "After iteration 260, Loss 26.478477\n",
      "After iteration 261, Loss 26.475435\n",
      "After iteration 262, Loss 26.472389\n",
      "After iteration 263, Loss 26.469330\n",
      "After iteration 264, Loss 26.466272\n",
      "After iteration 265, Loss 26.463203\n",
      "After iteration 266, Loss 26.460133\n",
      "After iteration 267, Loss 26.457054\n",
      "After iteration 268, Loss 26.453972\n",
      "After iteration 269, Loss 26.450874\n",
      "After iteration 270, Loss 26.447783\n",
      "After iteration 271, Loss 26.444679\n",
      "After iteration 272, Loss 26.441570\n",
      "After iteration 273, Loss 26.438452\n",
      "After iteration 274, Loss 26.435331\n",
      "After iteration 275, Loss 26.432209\n",
      "After iteration 276, Loss 26.429071\n",
      "After iteration 277, Loss 26.425932\n",
      "After iteration 278, Loss 26.422789\n",
      "After iteration 279, Loss 26.419636\n",
      "After iteration 280, Loss 26.416481\n",
      "After iteration 281, Loss 26.413317\n",
      "After iteration 282, Loss 26.410149\n",
      "After iteration 283, Loss 26.406965\n",
      "After iteration 284, Loss 26.403791\n",
      "After iteration 285, Loss 26.400602\n",
      "After iteration 286, Loss 26.397411\n",
      "After iteration 287, Loss 26.394209\n",
      "After iteration 288, Loss 26.391008\n",
      "After iteration 289, Loss 26.387794\n",
      "After iteration 290, Loss 26.384575\n",
      "After iteration 291, Loss 26.381355\n",
      "After iteration 292, Loss 26.378124\n",
      "After iteration 293, Loss 26.374895\n",
      "After iteration 294, Loss 26.371651\n",
      "After iteration 295, Loss 26.368408\n",
      "After iteration 296, Loss 26.365156\n",
      "After iteration 297, Loss 26.361900\n",
      "After iteration 298, Loss 26.358635\n",
      "After iteration 299, Loss 26.355364\n",
      "After iteration 300, Loss 26.352089\n",
      "After iteration 301, Loss 26.348808\n",
      "After iteration 302, Loss 26.345522\n",
      "After iteration 303, Loss 26.342230\n",
      "After iteration 304, Loss 26.338928\n",
      "After iteration 305, Loss 26.335621\n",
      "After iteration 306, Loss 26.332314\n",
      "After iteration 307, Loss 26.329002\n",
      "After iteration 308, Loss 26.325682\n",
      "After iteration 309, Loss 26.322351\n",
      "After iteration 310, Loss 26.319014\n",
      "After iteration 311, Loss 26.315680\n",
      "After iteration 312, Loss 26.312340\n",
      "After iteration 313, Loss 26.308989\n",
      "After iteration 314, Loss 26.305632\n",
      "After iteration 315, Loss 26.302271\n",
      "After iteration 316, Loss 26.298906\n",
      "After iteration 317, Loss 26.295532\n",
      "After iteration 318, Loss 26.292152\n",
      "After iteration 319, Loss 26.288765\n",
      "After iteration 320, Loss 26.285378\n",
      "After iteration 321, Loss 26.281982\n",
      "After iteration 322, Loss 26.278584\n",
      "After iteration 323, Loss 26.275177\n",
      "After iteration 324, Loss 26.271761\n",
      "After iteration 325, Loss 26.268343\n",
      "After iteration 326, Loss 26.264925\n",
      "After iteration 327, Loss 26.261497\n",
      "After iteration 328, Loss 26.258062\n",
      "After iteration 329, Loss 26.254620\n",
      "After iteration 330, Loss 26.251173\n",
      "After iteration 331, Loss 26.247719\n",
      "After iteration 332, Loss 26.244263\n",
      "After iteration 333, Loss 26.240812\n",
      "After iteration 334, Loss 26.237337\n",
      "After iteration 335, Loss 26.233864\n",
      "After iteration 336, Loss 26.230385\n",
      "After iteration 337, Loss 26.226906\n",
      "After iteration 338, Loss 26.223412\n",
      "After iteration 339, Loss 26.219917\n",
      "After iteration 340, Loss 26.216417\n",
      "After iteration 341, Loss 26.212914\n",
      "After iteration 342, Loss 26.209400\n",
      "After iteration 343, Loss 26.205885\n",
      "After iteration 344, Loss 26.202360\n",
      "After iteration 345, Loss 26.198835\n",
      "After iteration 346, Loss 26.195301\n",
      "After iteration 347, Loss 26.191761\n",
      "After iteration 348, Loss 26.188219\n",
      "After iteration 349, Loss 26.184664\n",
      "After iteration 350, Loss 26.181110\n",
      "After iteration 351, Loss 26.177553\n",
      "After iteration 352, Loss 26.173986\n",
      "After iteration 353, Loss 26.170416\n",
      "After iteration 354, Loss 26.166838\n",
      "After iteration 355, Loss 26.163258\n",
      "After iteration 356, Loss 26.159668\n",
      "After iteration 357, Loss 26.156078\n",
      "After iteration 358, Loss 26.152477\n",
      "After iteration 359, Loss 26.148870\n",
      "After iteration 360, Loss 26.145266\n",
      "After iteration 361, Loss 26.141651\n",
      "After iteration 362, Loss 26.138037\n",
      "After iteration 363, Loss 26.134411\n",
      "After iteration 364, Loss 26.130775\n",
      "After iteration 365, Loss 26.127142\n",
      "After iteration 366, Loss 26.123503\n",
      "After iteration 367, Loss 26.119856\n",
      "After iteration 368, Loss 26.116209\n",
      "After iteration 369, Loss 26.112549\n",
      "After iteration 370, Loss 26.108887\n",
      "After iteration 371, Loss 26.105223\n",
      "After iteration 372, Loss 26.101549\n",
      "After iteration 373, Loss 26.097872\n",
      "After iteration 374, Loss 26.094189\n",
      "After iteration 375, Loss 26.090504\n",
      "After iteration 376, Loss 26.086809\n",
      "After iteration 377, Loss 26.083111\n",
      "After iteration 378, Loss 26.079405\n",
      "After iteration 379, Loss 26.075697\n",
      "After iteration 380, Loss 26.071983\n",
      "After iteration 381, Loss 26.068266\n",
      "After iteration 382, Loss 26.064539\n",
      "After iteration 383, Loss 26.060810\n",
      "After iteration 384, Loss 26.057079\n",
      "After iteration 385, Loss 26.053333\n",
      "After iteration 386, Loss 26.049593\n",
      "After iteration 387, Loss 26.045839\n",
      "After iteration 388, Loss 26.042086\n",
      "After iteration 389, Loss 26.038330\n",
      "After iteration 390, Loss 26.034563\n",
      "After iteration 391, Loss 26.030790\n",
      "After iteration 392, Loss 26.027016\n",
      "After iteration 393, Loss 26.023232\n",
      "After iteration 394, Loss 26.019447\n",
      "After iteration 395, Loss 26.015654\n",
      "After iteration 396, Loss 26.011858\n",
      "After iteration 397, Loss 26.008057\n",
      "After iteration 398, Loss 26.004253\n",
      "After iteration 399, Loss 26.000444\n",
      "After iteration 400, Loss 25.996624\n",
      "After iteration 401, Loss 25.992798\n",
      "After iteration 402, Loss 25.988976\n",
      "After iteration 403, Loss 25.985144\n",
      "After iteration 404, Loss 25.981304\n",
      "After iteration 405, Loss 25.977465\n",
      "After iteration 406, Loss 25.973616\n",
      "After iteration 407, Loss 25.969763\n",
      "After iteration 408, Loss 25.965910\n",
      "After iteration 409, Loss 25.962048\n",
      "After iteration 410, Loss 25.958178\n",
      "After iteration 411, Loss 25.954309\n",
      "After iteration 412, Loss 25.950434\n",
      "After iteration 413, Loss 25.946545\n",
      "After iteration 414, Loss 25.942663\n",
      "After iteration 415, Loss 25.938768\n",
      "After iteration 416, Loss 25.934870\n",
      "After iteration 417, Loss 25.930969\n",
      "After iteration 418, Loss 25.927063\n",
      "After iteration 419, Loss 25.923151\n",
      "After iteration 420, Loss 25.919239\n",
      "After iteration 421, Loss 25.915312\n",
      "After iteration 422, Loss 25.911390\n",
      "After iteration 423, Loss 25.907457\n",
      "After iteration 424, Loss 25.903515\n",
      "After iteration 425, Loss 25.899578\n",
      "After iteration 426, Loss 25.895630\n",
      "After iteration 427, Loss 25.891682\n",
      "After iteration 428, Loss 25.887726\n",
      "After iteration 429, Loss 25.883760\n",
      "After iteration 430, Loss 25.879791\n",
      "After iteration 431, Loss 25.875826\n",
      "After iteration 432, Loss 25.871849\n",
      "After iteration 433, Loss 25.867867\n",
      "After iteration 434, Loss 25.863880\n",
      "After iteration 435, Loss 25.859894\n",
      "After iteration 436, Loss 25.855890\n",
      "After iteration 437, Loss 25.851896\n",
      "After iteration 438, Loss 25.847887\n",
      "After iteration 439, Loss 25.843884\n",
      "After iteration 440, Loss 25.839869\n",
      "After iteration 441, Loss 25.835846\n",
      "After iteration 442, Loss 25.831823\n",
      "After iteration 443, Loss 25.827791\n",
      "After iteration 444, Loss 25.823761\n",
      "After iteration 445, Loss 25.819721\n",
      "After iteration 446, Loss 25.815680\n",
      "After iteration 447, Loss 25.811628\n",
      "After iteration 448, Loss 25.807575\n",
      "After iteration 449, Loss 25.803516\n",
      "After iteration 450, Loss 25.799456\n",
      "After iteration 451, Loss 25.795387\n",
      "After iteration 452, Loss 25.791315\n",
      "After iteration 453, Loss 25.787239\n",
      "After iteration 454, Loss 25.783155\n",
      "After iteration 455, Loss 25.779066\n",
      "After iteration 456, Loss 25.774981\n",
      "After iteration 457, Loss 25.770882\n",
      "After iteration 458, Loss 25.766783\n",
      "After iteration 459, Loss 25.762676\n",
      "After iteration 460, Loss 25.758568\n",
      "After iteration 461, Loss 25.754448\n",
      "After iteration 462, Loss 25.750328\n",
      "After iteration 463, Loss 25.746204\n",
      "After iteration 464, Loss 25.742073\n",
      "After iteration 465, Loss 25.737944\n",
      "After iteration 466, Loss 25.733801\n",
      "After iteration 467, Loss 25.729658\n",
      "After iteration 468, Loss 25.725508\n",
      "After iteration 469, Loss 25.721361\n",
      "After iteration 470, Loss 25.717201\n",
      "After iteration 471, Loss 25.713036\n",
      "After iteration 472, Loss 25.708874\n",
      "After iteration 473, Loss 25.704700\n",
      "After iteration 474, Loss 25.700523\n",
      "After iteration 475, Loss 25.696342\n",
      "After iteration 476, Loss 25.692156\n",
      "After iteration 477, Loss 25.687969\n",
      "After iteration 478, Loss 25.683771\n",
      "After iteration 479, Loss 25.679573\n",
      "After iteration 480, Loss 25.675367\n",
      "After iteration 481, Loss 25.671160\n",
      "After iteration 482, Loss 25.666948\n",
      "After iteration 483, Loss 25.662727\n",
      "After iteration 484, Loss 25.658508\n",
      "After iteration 485, Loss 25.654280\n",
      "After iteration 486, Loss 25.650042\n",
      "After iteration 487, Loss 25.645815\n",
      "After iteration 488, Loss 25.641571\n",
      "After iteration 489, Loss 25.637327\n",
      "After iteration 490, Loss 25.633074\n",
      "After iteration 491, Loss 25.628817\n",
      "After iteration 492, Loss 25.624561\n",
      "After iteration 493, Loss 25.620300\n",
      "After iteration 494, Loss 25.616028\n",
      "After iteration 495, Loss 25.611755\n",
      "After iteration 496, Loss 25.607481\n",
      "After iteration 497, Loss 25.603197\n",
      "After iteration 498, Loss 25.598913\n",
      "After iteration 499, Loss 25.594622\n",
      "After iteration 500, Loss 25.590321\n",
      "After iteration 501, Loss 25.586023\n",
      "After iteration 502, Loss 25.581715\n",
      "After iteration 503, Loss 25.577406\n",
      "After iteration 504, Loss 25.573090\n",
      "After iteration 505, Loss 25.568773\n",
      "After iteration 506, Loss 25.564453\n",
      "After iteration 507, Loss 25.560125\n",
      "After iteration 508, Loss 25.555794\n",
      "After iteration 509, Loss 25.551460\n",
      "After iteration 510, Loss 25.547117\n",
      "After iteration 511, Loss 25.542768\n",
      "After iteration 512, Loss 25.538424\n",
      "After iteration 513, Loss 25.534071\n",
      "After iteration 514, Loss 25.529707\n",
      "After iteration 515, Loss 25.525347\n",
      "After iteration 516, Loss 25.520979\n",
      "After iteration 517, Loss 25.516603\n",
      "After iteration 518, Loss 25.512232\n",
      "After iteration 519, Loss 25.507849\n",
      "After iteration 520, Loss 25.503462\n",
      "After iteration 521, Loss 25.499077\n",
      "After iteration 522, Loss 25.494678\n",
      "After iteration 523, Loss 25.490282\n",
      "After iteration 524, Loss 25.485872\n",
      "After iteration 525, Loss 25.481468\n",
      "After iteration 526, Loss 25.477057\n",
      "After iteration 527, Loss 25.472639\n",
      "After iteration 528, Loss 25.468220\n",
      "After iteration 529, Loss 25.463795\n",
      "After iteration 530, Loss 25.459364\n",
      "After iteration 531, Loss 25.454931\n",
      "After iteration 532, Loss 25.450495\n",
      "After iteration 533, Loss 25.446051\n",
      "After iteration 534, Loss 25.441601\n",
      "After iteration 535, Loss 25.437153\n",
      "After iteration 536, Loss 25.432695\n",
      "After iteration 537, Loss 25.428234\n",
      "After iteration 538, Loss 25.423769\n",
      "After iteration 539, Loss 25.419300\n",
      "After iteration 540, Loss 25.414827\n",
      "After iteration 541, Loss 25.410357\n",
      "After iteration 542, Loss 25.405867\n",
      "After iteration 543, Loss 25.401381\n",
      "After iteration 544, Loss 25.396887\n",
      "After iteration 545, Loss 25.392393\n",
      "After iteration 546, Loss 25.387896\n",
      "After iteration 547, Loss 25.383392\n",
      "After iteration 548, Loss 25.378883\n",
      "After iteration 549, Loss 25.374371\n",
      "After iteration 550, Loss 25.369856\n",
      "After iteration 551, Loss 25.365328\n",
      "After iteration 552, Loss 25.360811\n",
      "After iteration 553, Loss 25.356281\n",
      "After iteration 554, Loss 25.351746\n",
      "After iteration 555, Loss 25.347212\n",
      "After iteration 556, Loss 25.342665\n",
      "After iteration 557, Loss 25.338118\n",
      "After iteration 558, Loss 25.333574\n",
      "After iteration 559, Loss 25.329018\n",
      "After iteration 560, Loss 25.324450\n",
      "After iteration 561, Loss 25.319891\n",
      "After iteration 562, Loss 25.315325\n",
      "After iteration 563, Loss 25.310753\n",
      "After iteration 564, Loss 25.306175\n",
      "After iteration 565, Loss 25.301592\n",
      "After iteration 566, Loss 25.297005\n",
      "After iteration 567, Loss 25.292419\n",
      "After iteration 568, Loss 25.287825\n",
      "After iteration 569, Loss 25.283226\n",
      "After iteration 570, Loss 25.278625\n",
      "After iteration 571, Loss 25.274023\n",
      "After iteration 572, Loss 25.269409\n",
      "After iteration 573, Loss 25.264793\n",
      "After iteration 574, Loss 25.260176\n",
      "After iteration 575, Loss 25.255554\n",
      "After iteration 576, Loss 25.250923\n",
      "After iteration 577, Loss 25.246290\n",
      "After iteration 578, Loss 25.241663\n",
      "After iteration 579, Loss 25.237022\n",
      "After iteration 580, Loss 25.232372\n",
      "After iteration 581, Loss 25.227724\n",
      "After iteration 582, Loss 25.223072\n",
      "After iteration 583, Loss 25.218420\n",
      "After iteration 584, Loss 25.213758\n",
      "After iteration 585, Loss 25.209093\n",
      "After iteration 586, Loss 25.204424\n",
      "After iteration 587, Loss 25.199751\n",
      "After iteration 588, Loss 25.195068\n",
      "After iteration 589, Loss 25.190392\n",
      "After iteration 590, Loss 25.185707\n",
      "After iteration 591, Loss 25.181013\n",
      "After iteration 592, Loss 25.176325\n",
      "After iteration 593, Loss 25.171631\n",
      "After iteration 594, Loss 25.166924\n",
      "After iteration 595, Loss 25.162218\n",
      "After iteration 596, Loss 25.157507\n",
      "After iteration 597, Loss 25.152794\n",
      "After iteration 598, Loss 25.148071\n",
      "After iteration 599, Loss 25.143354\n",
      "After iteration 600, Loss 25.138628\n",
      "After iteration 601, Loss 25.133892\n",
      "After iteration 602, Loss 25.129158\n",
      "After iteration 603, Loss 25.124420\n",
      "After iteration 604, Loss 25.119678\n",
      "After iteration 605, Loss 25.114931\n",
      "After iteration 606, Loss 25.110180\n",
      "After iteration 607, Loss 25.105431\n",
      "After iteration 608, Loss 25.100670\n",
      "After iteration 609, Loss 25.095905\n",
      "After iteration 610, Loss 25.091137\n",
      "After iteration 611, Loss 25.086365\n",
      "After iteration 612, Loss 25.081587\n",
      "After iteration 613, Loss 25.076811\n",
      "After iteration 614, Loss 25.072023\n",
      "After iteration 615, Loss 25.067238\n",
      "After iteration 616, Loss 25.062447\n",
      "After iteration 617, Loss 25.057648\n",
      "After iteration 618, Loss 25.052855\n",
      "After iteration 619, Loss 25.048052\n",
      "After iteration 620, Loss 25.043242\n",
      "After iteration 621, Loss 25.038427\n",
      "After iteration 622, Loss 25.033611\n",
      "After iteration 623, Loss 25.028791\n",
      "After iteration 624, Loss 25.023973\n",
      "After iteration 625, Loss 25.019142\n",
      "After iteration 626, Loss 25.014305\n",
      "After iteration 627, Loss 25.009474\n",
      "After iteration 628, Loss 25.004639\n",
      "After iteration 629, Loss 24.999788\n",
      "After iteration 630, Loss 24.994940\n",
      "After iteration 631, Loss 24.990089\n",
      "After iteration 632, Loss 24.985235\n",
      "After iteration 633, Loss 24.980375\n",
      "After iteration 634, Loss 24.975513\n",
      "After iteration 635, Loss 24.970644\n",
      "After iteration 636, Loss 24.965771\n",
      "After iteration 637, Loss 24.960896\n",
      "After iteration 638, Loss 24.956017\n",
      "After iteration 639, Loss 24.951139\n",
      "After iteration 640, Loss 24.946245\n",
      "After iteration 641, Loss 24.941357\n",
      "After iteration 642, Loss 24.936464\n",
      "After iteration 643, Loss 24.931562\n",
      "After iteration 644, Loss 24.926664\n",
      "After iteration 645, Loss 24.921753\n",
      "After iteration 646, Loss 24.916840\n",
      "After iteration 647, Loss 24.911932\n",
      "After iteration 648, Loss 24.907011\n",
      "After iteration 649, Loss 24.902082\n",
      "After iteration 650, Loss 24.897161\n",
      "After iteration 651, Loss 24.892229\n",
      "After iteration 652, Loss 24.887293\n",
      "After iteration 653, Loss 24.882357\n",
      "After iteration 654, Loss 24.877417\n",
      "After iteration 655, Loss 24.872465\n",
      "After iteration 656, Loss 24.867517\n",
      "After iteration 657, Loss 24.862568\n",
      "After iteration 658, Loss 24.857611\n",
      "After iteration 659, Loss 24.852642\n",
      "After iteration 660, Loss 24.847685\n",
      "After iteration 661, Loss 24.842707\n",
      "After iteration 662, Loss 24.837736\n",
      "After iteration 663, Loss 24.832762\n",
      "After iteration 664, Loss 24.827778\n",
      "After iteration 665, Loss 24.822798\n",
      "After iteration 666, Loss 24.817804\n",
      "After iteration 667, Loss 24.812817\n",
      "After iteration 668, Loss 24.807817\n",
      "After iteration 669, Loss 24.802818\n",
      "After iteration 670, Loss 24.797813\n",
      "After iteration 671, Loss 24.792805\n",
      "After iteration 672, Loss 24.787798\n",
      "After iteration 673, Loss 24.782776\n",
      "After iteration 674, Loss 24.777758\n",
      "After iteration 675, Loss 24.772736\n",
      "After iteration 676, Loss 24.767708\n",
      "After iteration 677, Loss 24.762678\n",
      "After iteration 678, Loss 24.757647\n",
      "After iteration 679, Loss 24.752607\n",
      "After iteration 680, Loss 24.747570\n",
      "After iteration 681, Loss 24.742525\n",
      "After iteration 682, Loss 24.737471\n",
      "After iteration 683, Loss 24.732416\n",
      "After iteration 684, Loss 24.727362\n",
      "After iteration 685, Loss 24.722296\n",
      "After iteration 686, Loss 24.717237\n",
      "After iteration 687, Loss 24.712164\n",
      "After iteration 688, Loss 24.707098\n",
      "After iteration 689, Loss 24.702021\n",
      "After iteration 690, Loss 24.696943\n",
      "After iteration 691, Loss 24.691862\n",
      "After iteration 692, Loss 24.686773\n",
      "After iteration 693, Loss 24.681677\n",
      "After iteration 694, Loss 24.676586\n",
      "After iteration 695, Loss 24.671486\n",
      "After iteration 696, Loss 24.666384\n",
      "After iteration 697, Loss 24.661280\n",
      "After iteration 698, Loss 24.656178\n",
      "After iteration 699, Loss 24.651056\n",
      "After iteration 700, Loss 24.645941\n",
      "After iteration 701, Loss 24.640821\n",
      "After iteration 702, Loss 24.635693\n",
      "After iteration 703, Loss 24.630566\n",
      "After iteration 704, Loss 24.625435\n",
      "After iteration 705, Loss 24.620300\n",
      "After iteration 706, Loss 24.615162\n",
      "After iteration 707, Loss 24.610018\n",
      "After iteration 708, Loss 24.604870\n",
      "After iteration 709, Loss 24.599720\n",
      "After iteration 710, Loss 24.594566\n",
      "After iteration 711, Loss 24.589409\n",
      "After iteration 712, Loss 24.584248\n",
      "After iteration 713, Loss 24.579084\n",
      "After iteration 714, Loss 24.573915\n",
      "After iteration 715, Loss 24.568748\n",
      "After iteration 716, Loss 24.563568\n",
      "After iteration 717, Loss 24.558392\n",
      "After iteration 718, Loss 24.553204\n",
      "After iteration 719, Loss 24.548018\n",
      "After iteration 720, Loss 24.542828\n",
      "After iteration 721, Loss 24.537634\n",
      "After iteration 722, Loss 24.532434\n",
      "After iteration 723, Loss 24.527233\n",
      "After iteration 724, Loss 24.522022\n",
      "After iteration 725, Loss 24.516815\n",
      "After iteration 726, Loss 24.511608\n",
      "After iteration 727, Loss 24.506392\n",
      "After iteration 728, Loss 24.501171\n",
      "After iteration 729, Loss 24.495949\n",
      "After iteration 730, Loss 24.490723\n",
      "After iteration 731, Loss 24.485487\n",
      "After iteration 732, Loss 24.480255\n",
      "After iteration 733, Loss 24.475019\n",
      "After iteration 734, Loss 24.469780\n",
      "After iteration 735, Loss 24.464533\n",
      "After iteration 736, Loss 24.459288\n",
      "After iteration 737, Loss 24.454029\n",
      "After iteration 738, Loss 24.448780\n",
      "After iteration 739, Loss 24.443520\n",
      "After iteration 740, Loss 24.438261\n",
      "After iteration 741, Loss 24.432989\n",
      "After iteration 742, Loss 24.427721\n",
      "After iteration 743, Loss 24.422447\n",
      "After iteration 744, Loss 24.417170\n",
      "After iteration 745, Loss 24.411888\n",
      "After iteration 746, Loss 24.406605\n",
      "After iteration 747, Loss 24.401318\n",
      "After iteration 748, Loss 24.396025\n",
      "After iteration 749, Loss 24.390736\n",
      "After iteration 750, Loss 24.385435\n",
      "After iteration 751, Loss 24.380133\n",
      "After iteration 752, Loss 24.374825\n",
      "After iteration 753, Loss 24.369518\n",
      "After iteration 754, Loss 24.364204\n",
      "After iteration 755, Loss 24.358892\n",
      "After iteration 756, Loss 24.353569\n",
      "After iteration 757, Loss 24.348248\n",
      "After iteration 758, Loss 24.342920\n",
      "After iteration 759, Loss 24.337591\n",
      "After iteration 760, Loss 24.332256\n",
      "After iteration 761, Loss 24.326921\n",
      "After iteration 762, Loss 24.321581\n",
      "After iteration 763, Loss 24.316235\n",
      "After iteration 764, Loss 24.310890\n",
      "After iteration 765, Loss 24.305540\n",
      "After iteration 766, Loss 24.300188\n",
      "After iteration 767, Loss 24.294825\n",
      "After iteration 768, Loss 24.289461\n",
      "After iteration 769, Loss 24.284096\n",
      "After iteration 770, Loss 24.278728\n",
      "After iteration 771, Loss 24.273357\n",
      "After iteration 772, Loss 24.267981\n",
      "After iteration 773, Loss 24.262604\n",
      "After iteration 774, Loss 24.257221\n",
      "After iteration 775, Loss 24.251833\n",
      "After iteration 776, Loss 24.246449\n",
      "After iteration 777, Loss 24.241051\n",
      "After iteration 778, Loss 24.235662\n",
      "After iteration 779, Loss 24.230257\n",
      "After iteration 780, Loss 24.224855\n",
      "After iteration 781, Loss 24.219452\n",
      "After iteration 782, Loss 24.214039\n",
      "After iteration 783, Loss 24.208630\n",
      "After iteration 784, Loss 24.203215\n",
      "After iteration 785, Loss 24.197796\n",
      "After iteration 786, Loss 24.192366\n",
      "After iteration 787, Loss 24.186937\n",
      "After iteration 788, Loss 24.181511\n",
      "After iteration 789, Loss 24.176075\n",
      "After iteration 790, Loss 24.170635\n",
      "After iteration 791, Loss 24.165199\n",
      "After iteration 792, Loss 24.159754\n",
      "After iteration 793, Loss 24.154303\n",
      "After iteration 794, Loss 24.148853\n",
      "After iteration 795, Loss 24.143404\n",
      "After iteration 796, Loss 24.137945\n",
      "After iteration 797, Loss 24.132484\n",
      "After iteration 798, Loss 24.127022\n",
      "After iteration 799, Loss 24.121550\n",
      "After iteration 800, Loss 24.116083\n",
      "After iteration 801, Loss 24.110607\n",
      "After iteration 802, Loss 24.105133\n",
      "After iteration 803, Loss 24.099651\n",
      "After iteration 804, Loss 24.094170\n",
      "After iteration 805, Loss 24.088676\n",
      "After iteration 806, Loss 24.083189\n",
      "After iteration 807, Loss 24.077692\n",
      "After iteration 808, Loss 24.072197\n",
      "After iteration 809, Loss 24.066694\n",
      "After iteration 810, Loss 24.061188\n",
      "After iteration 811, Loss 24.055676\n",
      "After iteration 812, Loss 24.050171\n",
      "After iteration 813, Loss 24.044649\n",
      "After iteration 814, Loss 24.039137\n",
      "After iteration 815, Loss 24.033619\n",
      "After iteration 816, Loss 24.028090\n",
      "After iteration 817, Loss 24.022566\n",
      "After iteration 818, Loss 24.017035\n",
      "After iteration 819, Loss 24.011499\n",
      "After iteration 820, Loss 24.005959\n",
      "After iteration 821, Loss 24.000422\n",
      "After iteration 822, Loss 23.994875\n",
      "After iteration 823, Loss 23.989326\n",
      "After iteration 824, Loss 23.983778\n",
      "After iteration 825, Loss 23.978222\n",
      "After iteration 826, Loss 23.972664\n",
      "After iteration 827, Loss 23.967102\n",
      "After iteration 828, Loss 23.961542\n",
      "After iteration 829, Loss 23.955971\n",
      "After iteration 830, Loss 23.950401\n",
      "After iteration 831, Loss 23.944826\n",
      "After iteration 832, Loss 23.939251\n",
      "After iteration 833, Loss 23.933668\n",
      "After iteration 834, Loss 23.928089\n",
      "After iteration 835, Loss 23.922503\n",
      "After iteration 836, Loss 23.916908\n",
      "After iteration 837, Loss 23.911316\n",
      "After iteration 838, Loss 23.905720\n",
      "After iteration 839, Loss 23.900124\n",
      "After iteration 840, Loss 23.894514\n",
      "After iteration 841, Loss 23.888910\n",
      "After iteration 842, Loss 23.883301\n",
      "After iteration 843, Loss 23.877686\n",
      "After iteration 844, Loss 23.872076\n",
      "After iteration 845, Loss 23.866453\n",
      "After iteration 846, Loss 23.860832\n",
      "After iteration 847, Loss 23.855207\n",
      "After iteration 848, Loss 23.849577\n",
      "After iteration 849, Loss 23.843946\n",
      "After iteration 850, Loss 23.838306\n",
      "After iteration 851, Loss 23.832674\n",
      "After iteration 852, Loss 23.827032\n",
      "After iteration 853, Loss 23.821381\n",
      "After iteration 854, Loss 23.815735\n",
      "After iteration 855, Loss 23.810081\n",
      "After iteration 856, Loss 23.804430\n",
      "After iteration 857, Loss 23.798769\n",
      "After iteration 858, Loss 23.793114\n",
      "After iteration 859, Loss 23.787447\n",
      "After iteration 860, Loss 23.781782\n",
      "After iteration 861, Loss 23.776110\n",
      "After iteration 862, Loss 23.770435\n",
      "After iteration 863, Loss 23.764765\n",
      "After iteration 864, Loss 23.759083\n",
      "After iteration 865, Loss 23.753395\n",
      "After iteration 866, Loss 23.747717\n",
      "After iteration 867, Loss 23.742027\n",
      "After iteration 868, Loss 23.736328\n",
      "After iteration 869, Loss 23.730633\n",
      "After iteration 870, Loss 23.724937\n",
      "After iteration 871, Loss 23.719236\n",
      "After iteration 872, Loss 23.713531\n",
      "After iteration 873, Loss 23.707819\n",
      "After iteration 874, Loss 23.702110\n",
      "After iteration 875, Loss 23.696400\n",
      "After iteration 876, Loss 23.690680\n",
      "After iteration 877, Loss 23.684959\n",
      "After iteration 878, Loss 23.679235\n",
      "After iteration 879, Loss 23.673510\n",
      "After iteration 880, Loss 23.667780\n",
      "After iteration 881, Loss 23.662043\n",
      "After iteration 882, Loss 23.656309\n",
      "After iteration 883, Loss 23.650568\n",
      "After iteration 884, Loss 23.644825\n",
      "After iteration 885, Loss 23.639080\n",
      "After iteration 886, Loss 23.633331\n",
      "After iteration 887, Loss 23.627581\n",
      "After iteration 888, Loss 23.621824\n",
      "After iteration 889, Loss 23.616072\n",
      "After iteration 890, Loss 23.610308\n",
      "After iteration 891, Loss 23.604546\n",
      "After iteration 892, Loss 23.598783\n",
      "After iteration 893, Loss 23.593010\n",
      "After iteration 894, Loss 23.587236\n",
      "After iteration 895, Loss 23.581459\n",
      "After iteration 896, Loss 23.575678\n",
      "After iteration 897, Loss 23.569893\n",
      "After iteration 898, Loss 23.564112\n",
      "After iteration 899, Loss 23.558325\n",
      "After iteration 900, Loss 23.552538\n",
      "After iteration 901, Loss 23.546740\n",
      "After iteration 902, Loss 23.540937\n",
      "After iteration 903, Loss 23.535145\n",
      "After iteration 904, Loss 23.529335\n",
      "After iteration 905, Loss 23.523529\n",
      "After iteration 906, Loss 23.517719\n",
      "After iteration 907, Loss 23.511908\n",
      "After iteration 908, Loss 23.506096\n",
      "After iteration 909, Loss 23.500277\n",
      "After iteration 910, Loss 23.494453\n",
      "After iteration 911, Loss 23.488625\n",
      "After iteration 912, Loss 23.482796\n",
      "After iteration 913, Loss 23.476967\n",
      "After iteration 914, Loss 23.471132\n",
      "After iteration 915, Loss 23.465300\n",
      "After iteration 916, Loss 23.459461\n",
      "After iteration 917, Loss 23.453619\n",
      "After iteration 918, Loss 23.447771\n",
      "After iteration 919, Loss 23.441919\n",
      "After iteration 920, Loss 23.436068\n",
      "After iteration 921, Loss 23.430214\n",
      "After iteration 922, Loss 23.424355\n",
      "After iteration 923, Loss 23.418495\n",
      "After iteration 924, Loss 23.412628\n",
      "After iteration 925, Loss 23.406769\n",
      "After iteration 926, Loss 23.400892\n",
      "After iteration 927, Loss 23.395021\n",
      "After iteration 928, Loss 23.389153\n",
      "After iteration 929, Loss 23.383276\n",
      "After iteration 930, Loss 23.377386\n",
      "After iteration 931, Loss 23.371510\n",
      "After iteration 932, Loss 23.365623\n",
      "After iteration 933, Loss 23.359730\n",
      "After iteration 934, Loss 23.353832\n",
      "After iteration 935, Loss 23.347939\n",
      "After iteration 936, Loss 23.342043\n",
      "After iteration 937, Loss 23.336140\n",
      "After iteration 938, Loss 23.330233\n",
      "After iteration 939, Loss 23.324333\n",
      "After iteration 940, Loss 23.318415\n",
      "After iteration 941, Loss 23.312500\n",
      "After iteration 942, Loss 23.306585\n",
      "After iteration 943, Loss 23.300671\n",
      "After iteration 944, Loss 23.294744\n",
      "After iteration 945, Loss 23.288824\n",
      "After iteration 946, Loss 23.282896\n",
      "After iteration 947, Loss 23.276964\n",
      "After iteration 948, Loss 23.271027\n",
      "After iteration 949, Loss 23.265089\n",
      "After iteration 950, Loss 23.259151\n",
      "After iteration 951, Loss 23.253212\n",
      "After iteration 952, Loss 23.247267\n",
      "After iteration 953, Loss 23.241318\n",
      "After iteration 954, Loss 23.235365\n",
      "After iteration 955, Loss 23.229416\n",
      "After iteration 956, Loss 23.223457\n",
      "After iteration 957, Loss 23.217499\n",
      "After iteration 958, Loss 23.211535\n",
      "After iteration 959, Loss 23.205564\n",
      "After iteration 960, Loss 23.199598\n",
      "After iteration 961, Loss 23.193632\n",
      "After iteration 962, Loss 23.187660\n",
      "After iteration 963, Loss 23.181681\n",
      "After iteration 964, Loss 23.175699\n",
      "After iteration 965, Loss 23.169720\n",
      "After iteration 966, Loss 23.163734\n",
      "After iteration 967, Loss 23.157743\n",
      "After iteration 968, Loss 23.151754\n",
      "After iteration 969, Loss 23.145760\n",
      "After iteration 970, Loss 23.139763\n",
      "After iteration 971, Loss 23.133764\n",
      "After iteration 972, Loss 23.127762\n",
      "After iteration 973, Loss 23.121759\n",
      "After iteration 974, Loss 23.115749\n",
      "After iteration 975, Loss 23.109739\n",
      "After iteration 976, Loss 23.103727\n",
      "After iteration 977, Loss 23.097708\n",
      "After iteration 978, Loss 23.091686\n",
      "After iteration 979, Loss 23.085667\n",
      "After iteration 980, Loss 23.079643\n",
      "After iteration 981, Loss 23.073620\n",
      "After iteration 982, Loss 23.067589\n",
      "After iteration 983, Loss 23.061548\n",
      "After iteration 984, Loss 23.055517\n",
      "After iteration 985, Loss 23.049480\n",
      "After iteration 986, Loss 23.043442\n",
      "After iteration 987, Loss 23.037397\n",
      "After iteration 988, Loss 23.031343\n",
      "After iteration 989, Loss 23.025301\n",
      "After iteration 990, Loss 23.019247\n",
      "After iteration 991, Loss 23.013191\n",
      "After iteration 992, Loss 23.007133\n",
      "After iteration 993, Loss 23.001076\n",
      "After iteration 994, Loss 22.995008\n",
      "After iteration 995, Loss 22.988941\n",
      "After iteration 996, Loss 22.982872\n",
      "After iteration 997, Loss 22.976803\n",
      "After iteration 998, Loss 22.970730\n",
      "After iteration 999, Loss 22.964649\n",
      "After iteration 1000, Loss 22.958574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2700, -2.1840], requires_grad=True)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 124
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podsumowanie:\n",
    "- zamiast obliczać gradienty \"ręcznie\", możemy użyć silnika `autograd`\n",
    "- tensory korzystające z `autograd` (`requires_grad=True`) posiadają atrybut `grad`\n",
    "- wykonywanie operacji na tensorach powoduje tworzenie grafu obliczeń (computation graph)\n",
    "- wywołanie funkcji `loss.backward()` na tensorze reprezentującym funkcję straty powoduje aktualizację gradientów (zmienia się zawartość `.grad` dla tensorów w grafie)\n",
    "- ze względu na akumulację gradientów typowo należy je wyzerować np. na początku każdej iteracji treningu\n",
    "- moduł `torch.optim` zawiera implementacje algorytmów optymalizacji funkcji straty, m.in. SGD, RMSProp, Adam\n",
    "- przy inicjalizacji optymalizatora należy podać jako argument zestaw parametrów modelu do optymalizacji oraz wartość stałej uczącej\n",
    "- korzystając z `torch.optim` w pętli treningowej należy wywołać `optim.zero_grad()` (zerowanie gradientów) oraz `optim.step()` (krok aktualizacji parametrów)\n",
    "\n",
    "Podczas poprzednich i dzisiejszych ćwiczeń poznaliśmy tak naprawdę podstawowy wariant mechanizmu uczenia sieci neuronowych. Sieci neuronowe to po prostu bardziej złożone modele niż model liniowy, posiadają one dużo więcej parametrów i są modelami nieliniowymi. W sieciach neuronowych w PyTorch także tworzy się graf obliczeń na tensorach, który służy następnie do obliczania gradientów funkcji straty z użyciem `autograd`. Krok aktualizacji parametrów wykonuje się typowo z użyciem jednego z dostępnych w `torch.optim` algorytmów."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Wnioski\n",
    "Zajęcia laboratoryjne przygotowały mnie do korzystania autograd. Poznałem również podstawowe algorytmy optymalizacji funkcji straty, takie jak SGD czy Adam. Dzięki temu będę w stanie wydajniej trenować sieci neuronowe."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR4_GOV9dLWb"
   },
   "source": [
    "## Sztuczne sieci neuronowe - laboratorium 8\n",
    "\n",
    "### Splotowe sieci neuronowe - cz. 2\n",
    "\n",
    "Na poprzednich zajęciach poznaliśmy warstwy tworzące **splotową sieć neuronową** i nauczyliśmy się tworzyć modele jako klasy  dziedziczące po `nn.Module`.\n",
    "\n",
    "Dziś wytrenujemy splotową sieć neuronową do binarnej klasyfikacji obrazu i poznamy dodatkowe techniki stosowane w sieciach neuronowych, m.in. do regularyzacji modeli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKyJUiMNdLWd"
   },
   "source": [
    "#### Pytania kontrolne\n",
    "\n",
    "1. Opisz budowę splotowej sieci neuronowej. Wyjaśnij, do czego służą jej poszczególne warstwy.\n",
    "2. Na czym polega regularyzacja modeli?\n",
    "3. Jakie znasz metody regularyzacji stosowane w sieciach neuronowych?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUmkjY-tdLWe"
   },
   "source": [
    "### Z poprzednich ćwiczeń\n",
    "\n",
    "Uruchom kolejne komórki, wykorzystujące kod z poprzednich zajęć, aby przygotować zbiór danych - `cifar2` oraz klasę `Net` definiującą model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4k_ssug-dLWe",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:32:29.458794Z",
     "start_time": "2025-05-12T11:32:29.456151Z"
    }
   },
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3HBBJIWzdLWf",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:32:29.494778Z",
     "start_time": "2025-05-12T11:32:29.491865Z"
    }
   },
   "source": [
    "class_names = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CqAGzP03dLWf",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:32:30.245442Z",
     "start_time": "2025-05-12T11:32:29.555422Z"
    }
   },
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(\"data\", train=True, download=False, transform=transforms.ToTensor())\n",
    "tensor_cifar10_val = datasets.CIFAR10(\"data\", train=False, download=False, transform=transforms.ToTensor())"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aPncsLzMdLWf",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:32:37.387733Z",
     "start_time": "2025-05-12T11:32:30.281525Z"
    }
   },
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "per_channel_means = imgs.view(3, -1).mean(dim=1)\n",
    "per_channel_std = imgs.view(3, -1).std(dim=1)"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TWPMfbfydLWf",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:32:38.051576Z",
     "start_time": "2025-05-12T11:32:37.419577Z"
    }
   },
   "source": [
    "transforms_compose = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(per_channel_means, per_channel_std)\n",
    "])\n",
    "\n",
    "transformed_cifar10 = datasets.CIFAR10(\"data\", train=True, download=False, transform=transforms_compose)\n",
    "transformed_cifar10_val = datasets.CIFAR10(\"data\", train=False, download=False, transform=transforms_compose)"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OclezaUbdLWg",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:32:47.694525Z",
     "start_time": "2025-05-12T11:32:38.080202Z"
    }
   },
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "new_class_names  = [class_names[i] for i in label_map]\n",
    "\n",
    "cifar2 = [(img, label_map[label]) for img, label in tensor_cifar10 if label in label_map]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in tensor_cifar10 if label in label_map]"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qx9CCQ8GdLWg",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:32:47.722535Z",
     "start_time": "2025-05-12T11:32:47.719562Z"
    }
   },
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF3-tD-idLWg"
   },
   "source": [
    "### Trening modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znyt0tsAdLWg"
   },
   "source": [
    "#### Ćwiczenie\n",
    "Uzupełnij poniższe komórki, aby wytrenować splotową sieć neuronową do zadania klasyfikacji binarnej.\n",
    "\n",
    "Przyjmij learning rate o wartości 0.01 i batch size 64. Trenuj przez 100 epok. Użyj optymaliatora SGD."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ogQxwzQDdLWh",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:32:47.771638Z",
     "start_time": "2025-05-12T11:32:47.768705Z"
    }
   },
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to()\n",
    "            labels = labels.to()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        loss_train /= len(train_loader.dataset)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train}\")"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TtJd9RnJdLWh",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:02.092143Z",
     "start_time": "2025-05-12T11:32:47.837129Z"
    }
   },
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.010727701449394226\n",
      "Epoch 10, Training loss 0.007026612052321434\n",
      "Epoch 20, Training loss 0.0056229409098625185\n",
      "Epoch 30, Training loss 0.005117171007394791\n",
      "Epoch 40, Training loss 0.004846780079975724\n",
      "Epoch 50, Training loss 0.004626769314706325\n",
      "Epoch 60, Training loss 0.004410410498082638\n",
      "Epoch 70, Training loss 0.0042155707865953445\n",
      "Epoch 80, Training loss 0.0040079746186733245\n",
      "Epoch 90, Training loss 0.003815111853182316\n",
      "Epoch 100, Training loss 0.00363273723423481\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcpJM_L4dLWh"
   },
   "source": [
    "### Walidacja modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4h7WPLVvdLWh"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Zaimplementuj funkcję `validate`, która zmierzy dokładność wytrenowanego modelu na dwóch zbiorach - uczącym i walidacyjnym.\n",
    "\n",
    "Porównaj wyniki z wynikami z laboratorium nr 5 (sieć gęsta)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xrw_uwkMdLWh",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:02.135933Z",
     "start_time": "2025-05-12T11:35:02.126392Z"
    }
   },
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bmKh5zBudLWh",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:02.194983Z",
     "start_time": "2025-05-12T11:35:02.191864Z"
    }
   },
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    model.eval()\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to()\n",
    "                labels = labels.to()\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        print(f\"{name} accuracy: {accuracy:.2f}\")"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZbD8bgIFdLWh",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:03.853976Z",
     "start_time": "2025-05-12T11:35:02.257436Z"
    }
   },
   "source": [
    "validate(model, train_loader, val_loader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.90\n",
      "val accuracy: 0.90\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98LT5GNvdLWh"
   },
   "source": [
    "### Zapis i odczyt modelu\n",
    "\n",
    "Wytrenowany model (zwłaszcza tak, którego trening trwa długo) warto zapisać, aby móc go użyć później.\n",
    "\n",
    "Typowo po każdej epoce treningu sprawdza się działanie modelu na zbiorze walidacyjnym (walidacja).\n",
    "Zapisu modelu (tzw. \"checkpoint\") typowo dokonuje się, jeśli wartość danej metryki (np. dokładność lub F1 na zbiorze walidacyjnym) jest lepsza niż najlepsza uzyskana dotychczas.\n",
    "\n",
    "PyTorch pozwala zapisać wagi (parametry) modelu z użyciem `torch.save` oraz tzw. `state_dict` modelu (https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html). Innym sposobem zapisu jest zapis całego modelu (z użyciem `pickle` \"pod spodem\"):\n",
    "https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html\n",
    "\n",
    "Następnie, do wczytania zapisanego modelu można użyć metody `load_state_dict` (oraz `torch.load`), jeśli zapisywaliśmy tylko `state_dict` lub tylko `torch.load`, jeśli zapisywaliśmy cały model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "elrHl6UgdLWh",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:03.898850Z",
     "start_time": "2025-05-12T11:35:03.888345Z"
    }
   },
   "source": [
    "print(model.state_dict())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'conv1.weight': tensor([[[[ 2.6952e-02,  1.5811e-01,  2.5854e-01],\n",
      "          [ 1.7185e-01,  2.1432e-01,  1.4635e-01],\n",
      "          [ 9.8063e-02,  4.8224e-02,  3.1590e-01]],\n",
      "\n",
      "         [[ 5.5910e-03, -7.7587e-02, -1.5105e-01],\n",
      "          [-1.8848e-01,  9.7199e-03, -2.9531e-01],\n",
      "          [-4.5475e-02,  2.2443e-03, -1.0404e-01]],\n",
      "\n",
      "         [[-8.4342e-02, -1.9070e-01, -2.8194e-01],\n",
      "          [-1.8346e-01, -6.2627e-02, -3.1266e-01],\n",
      "          [-1.5285e-01, -3.1271e-01, -2.0209e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.8254e-02, -6.3556e-02,  2.5772e-01],\n",
      "          [ 2.9880e-01, -2.0441e-01,  1.2164e-01],\n",
      "          [-1.2513e-01,  5.1281e-02,  1.5160e-01]],\n",
      "\n",
      "         [[-2.2792e-01, -5.6854e-02,  9.1111e-02],\n",
      "          [ 1.6413e-01, -2.7808e-01,  6.1409e-03],\n",
      "          [-1.4743e-01,  5.3272e-02, -6.5304e-02]],\n",
      "\n",
      "         [[-1.2947e-01, -1.1153e-01,  2.0584e-01],\n",
      "          [-1.7568e-01, -3.8679e-01,  9.4861e-02],\n",
      "          [-1.0282e-02,  1.3125e-01, -5.7939e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.1533e-01,  1.7195e-01, -1.4774e-01],\n",
      "          [-2.1040e-01, -8.8695e-02,  9.3978e-02],\n",
      "          [-1.5525e-01, -5.3976e-03, -2.5627e-02]],\n",
      "\n",
      "         [[ 1.3748e-01,  2.3350e-01,  2.6362e-01],\n",
      "          [ 1.6948e-01,  2.9261e-01,  2.5794e-01],\n",
      "          [ 3.8620e-02,  6.1867e-02,  3.0791e-01]],\n",
      "\n",
      "         [[-3.9810e-01, -2.3767e-01, -1.0507e-01],\n",
      "          [-5.1666e-01, -2.7324e-01, -2.0392e-01],\n",
      "          [-3.9386e-01, -3.4001e-01, -1.1448e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.6710e-02, -1.6465e-01,  1.1693e-01],\n",
      "          [ 2.6156e-01, -2.2325e-02, -2.0070e-02],\n",
      "          [-2.1114e-01, -1.8345e-02,  1.0500e-01]],\n",
      "\n",
      "         [[-1.2170e-01, -2.0466e-01,  4.1057e-02],\n",
      "          [ 2.7194e-02, -3.4318e-03,  1.7041e-02],\n",
      "          [-1.9264e-01,  1.5274e-01,  1.2450e-01]],\n",
      "\n",
      "         [[-2.0268e-01,  6.1501e-02, -2.3523e-01],\n",
      "          [ 1.4292e-01,  2.0530e-01, -1.4246e-01],\n",
      "          [ 2.2546e-01, -8.0446e-02,  2.1001e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.9095e-01, -2.2320e-03, -3.0196e-01],\n",
      "          [-2.8039e-01, -1.8614e-01, -9.1811e-02],\n",
      "          [-3.1354e-01, -1.0807e-01, -2.3019e-02]],\n",
      "\n",
      "         [[-1.4527e-01,  6.6114e-02, -2.5593e-01],\n",
      "          [ 1.7813e-01,  1.9501e-01, -1.8448e-01],\n",
      "          [ 2.5095e-01,  6.1696e-02,  1.1301e-01]],\n",
      "\n",
      "         [[-1.2204e-01, -7.8752e-02,  8.9479e-02],\n",
      "          [ 2.2857e-01,  1.7096e-02,  2.1423e-01],\n",
      "          [ 3.0184e-01,  3.7801e-01,  3.4580e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.5548e-02,  8.5582e-02,  1.0344e-01],\n",
      "          [-4.1456e-02,  3.9880e-02,  2.4018e-01],\n",
      "          [ 1.2925e-01, -1.2119e-04,  1.3204e-01]],\n",
      "\n",
      "         [[-1.3046e-01,  1.5758e-01,  1.4133e-01],\n",
      "          [ 9.0535e-02, -3.0716e-03,  1.9444e-01],\n",
      "          [ 1.4736e-01, -2.2257e-02,  9.0634e-02]],\n",
      "\n",
      "         [[-1.8102e-01, -1.6541e-01, -4.7330e-02],\n",
      "          [-1.4636e-01, -1.6923e-01, -1.1152e-01],\n",
      "          [-2.5423e-01, -2.1640e-02, -2.8585e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4017e-01, -1.5999e-01, -1.5167e-01],\n",
      "          [-5.8316e-02,  1.7033e-01, -2.3837e-01],\n",
      "          [ 3.3024e-02,  5.9931e-02,  1.1850e-01]],\n",
      "\n",
      "         [[ 8.5248e-02, -2.2740e-01, -2.2609e-01],\n",
      "          [-1.6958e-01,  2.1348e-01, -2.8373e-01],\n",
      "          [ 2.9805e-02,  2.3797e-01, -6.2361e-02]],\n",
      "\n",
      "         [[-1.0234e-02, -2.2711e-01, -2.5043e-01],\n",
      "          [ 6.5098e-03,  3.5995e-02, -5.8797e-03],\n",
      "          [-1.4014e-01,  3.3417e-01,  2.5283e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.9307e-02, -1.9662e-01,  3.9401e-02],\n",
      "          [-8.5461e-02, -3.1278e-01, -5.9418e-02],\n",
      "          [ 5.9877e-02,  2.4149e-02, -3.3798e-02]],\n",
      "\n",
      "         [[ 7.9247e-02,  1.1598e-01,  1.4517e-01],\n",
      "          [-6.6402e-02, -2.8796e-01, -3.1480e-01],\n",
      "          [ 9.1873e-02, -2.1596e-02, -4.9193e-02]],\n",
      "\n",
      "         [[ 2.1794e-01,  2.3882e-01, -1.0818e-03],\n",
      "          [ 5.5751e-02, -4.5255e-03,  8.0391e-02],\n",
      "          [ 9.9324e-02, -1.6397e-01,  6.2548e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0416e-01, -2.0732e-01, -6.5472e-02],\n",
      "          [-2.1374e-01,  1.8649e-01,  2.4670e-01],\n",
      "          [-2.2390e-01,  1.3679e-01, -2.2549e-01]],\n",
      "\n",
      "         [[-1.0833e-01, -5.4197e-02, -3.4047e-02],\n",
      "          [ 1.5672e-01,  7.1288e-02,  3.1081e-01],\n",
      "          [ 1.1526e-01,  2.0899e-01,  5.4135e-02]],\n",
      "\n",
      "         [[-1.9290e-01, -1.6333e-01, -1.1987e-01],\n",
      "          [ 3.8771e-02,  3.0146e-01,  2.5696e-01],\n",
      "          [ 2.6769e-03,  2.9586e-02, -1.6335e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.1440e-01,  4.1059e-01,  4.7887e-01],\n",
      "          [-3.6091e-02,  1.5881e-01,  1.2849e-01],\n",
      "          [-2.8999e-01, -1.2955e-01, -3.1552e-01]],\n",
      "\n",
      "         [[ 2.4293e-01, -3.1338e-02,  4.3382e-02],\n",
      "          [-2.8026e-01, -1.5237e-01, -1.2358e-01],\n",
      "          [-3.4276e-01, -2.6239e-01, -1.8144e-01]],\n",
      "\n",
      "         [[ 6.8989e-02, -2.1958e-02,  1.7838e-01],\n",
      "          [ 2.3224e-01, -3.1494e-02, -4.5773e-02],\n",
      "          [-1.2898e-01, -1.5975e-01, -2.0495e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.0142e-02, -1.2350e-01, -3.7712e-02],\n",
      "          [ 6.4635e-02,  1.5023e-03, -1.1783e-01],\n",
      "          [ 1.8039e-01, -2.8563e-02, -1.8396e-01]],\n",
      "\n",
      "         [[-1.8145e-02,  1.5727e-01,  1.6399e-01],\n",
      "          [-3.9295e-02, -1.8137e-01, -8.0289e-02],\n",
      "          [ 1.4613e-01,  4.8315e-02, -1.8578e-02]],\n",
      "\n",
      "         [[ 2.5027e-02, -1.2896e-01, -1.4439e-01],\n",
      "          [ 1.3930e-01,  6.3737e-02, -2.7369e-01],\n",
      "          [ 5.4251e-02,  1.1388e-02, -2.2041e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6419e-02, -4.2335e-02,  1.6178e-01],\n",
      "          [ 2.0456e-01,  2.1029e-01,  2.2233e-01],\n",
      "          [ 1.3045e-01,  2.4113e-01,  1.2089e-01]],\n",
      "\n",
      "         [[-8.0143e-02,  5.2011e-02, -2.7120e-01],\n",
      "          [-6.3727e-02,  1.3660e-01,  4.8609e-02],\n",
      "          [-2.4766e-01,  1.2323e-01,  7.3404e-02]],\n",
      "\n",
      "         [[ 1.4606e-01, -2.3295e-01,  5.0620e-02],\n",
      "          [-1.0612e-01,  3.9064e-02, -2.1393e-01],\n",
      "          [-1.1164e-01,  6.0314e-02, -1.5375e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0096e-01,  1.3601e-02,  7.7952e-02],\n",
      "          [ 1.7724e-01, -2.9048e-02,  1.9467e-01],\n",
      "          [-1.2416e-01,  9.8535e-02, -9.7490e-02]],\n",
      "\n",
      "         [[-1.0184e-02,  1.5757e-01,  1.5526e-02],\n",
      "          [-1.4811e-01, -7.3281e-02,  1.0728e-01],\n",
      "          [ 1.4870e-01,  1.9544e-02,  1.2905e-01]],\n",
      "\n",
      "         [[ 1.5807e-01,  2.4964e-02,  7.4951e-02],\n",
      "          [-1.9655e-01, -1.8612e-01,  1.7647e-01],\n",
      "          [-1.1780e-02, -1.1170e-01,  3.5026e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.9067e-04,  1.3164e-01,  2.4454e-02],\n",
      "          [ 1.4447e-01,  1.1772e-01, -1.1253e-01],\n",
      "          [ 1.2056e-01,  1.6734e-01, -1.3769e-02]],\n",
      "\n",
      "         [[ 1.3327e-02,  1.2225e-01, -1.5966e-01],\n",
      "          [ 1.5434e-01, -1.1715e-01,  8.7126e-02],\n",
      "          [ 9.3836e-02, -1.4199e-01, -2.2164e-02]],\n",
      "\n",
      "         [[ 7.0271e-02, -7.5073e-02, -1.1799e-01],\n",
      "          [-1.1837e-01, -7.7264e-02, -6.8654e-02],\n",
      "          [-5.7018e-02,  5.1209e-02,  1.0057e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3702e-01, -1.1248e-02,  2.1866e-01],\n",
      "          [ 1.8011e-01,  1.5095e-02,  3.4825e-02],\n",
      "          [ 2.5307e-02,  2.5974e-01,  2.5380e-01]],\n",
      "\n",
      "         [[-2.5487e-01, -1.7202e-01,  2.6528e-02],\n",
      "          [-8.7012e-02, -1.9089e-01,  2.6789e-02],\n",
      "          [-2.5446e-01, -1.1332e-01,  3.1590e-02]],\n",
      "\n",
      "         [[-1.1351e-01,  4.8331e-02, -1.8661e-01],\n",
      "          [-1.7779e-01,  1.6645e-02, -1.3083e-01],\n",
      "          [-1.1651e-01, -1.2249e-01, -1.7212e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.0788e-01,  3.5054e-02, -8.1442e-02],\n",
      "          [-1.6203e-01,  2.3248e-01,  2.0711e-02],\n",
      "          [ 2.2438e-02,  1.9716e-02, -3.2727e-01]],\n",
      "\n",
      "         [[ 1.0039e-01, -8.9430e-02, -1.6036e-02],\n",
      "          [ 1.8200e-01,  1.7465e-01,  9.1427e-02],\n",
      "          [-2.6342e-01, -9.7699e-02, -3.3147e-01]],\n",
      "\n",
      "         [[-1.0298e-01, -1.4200e-01, -6.6210e-02],\n",
      "          [ 2.7752e-01,  2.9650e-01,  2.2190e-01],\n",
      "          [ 2.0176e-01, -1.6152e-01, -6.2449e-02]]]]), 'conv1.bias': tensor([ 0.3343,  0.0059,  0.2766, -0.1554, -0.1493, -0.0565,  0.1664,  0.0990,\n",
      "        -0.2948,  0.0049,  0.1598, -0.1547, -0.0531, -0.1410,  0.1716,  0.0896]), 'conv2.weight': tensor([[[[ 0.0579, -0.0519, -0.0549],\n",
      "          [ 0.0820, -0.0185,  0.0642],\n",
      "          [-0.1773, -0.1529,  0.0284]],\n",
      "\n",
      "         [[ 0.0721,  0.0742,  0.1403],\n",
      "          [ 0.2000,  0.0879,  0.2362],\n",
      "          [-0.0006, -0.0115, -0.0215]],\n",
      "\n",
      "         [[ 0.2045,  0.1847,  0.1129],\n",
      "          [ 0.1385,  0.2188,  0.0428],\n",
      "          [ 0.0450,  0.0604, -0.0037]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0155,  0.0488, -0.0600],\n",
      "          [ 0.0796, -0.0619, -0.1183],\n",
      "          [ 0.0033,  0.0043,  0.0682]],\n",
      "\n",
      "         [[ 0.0604, -0.0647, -0.0478],\n",
      "          [-0.0793, -0.0784,  0.1014],\n",
      "          [-0.1583, -0.0517,  0.0038]],\n",
      "\n",
      "         [[ 0.1522,  0.1331,  0.1212],\n",
      "          [-0.0590,  0.1152,  0.0116],\n",
      "          [-0.0074,  0.0229, -0.0209]]],\n",
      "\n",
      "\n",
      "        [[[-0.0413, -0.0655, -0.0620],\n",
      "          [-0.0609, -0.0325, -0.1926],\n",
      "          [ 0.0174, -0.0166,  0.0503]],\n",
      "\n",
      "         [[ 0.0917, -0.0243, -0.0246],\n",
      "          [ 0.0212, -0.0277, -0.0546],\n",
      "          [-0.0076,  0.1526, -0.0078]],\n",
      "\n",
      "         [[-0.0175, -0.1646, -0.0789],\n",
      "          [-0.2313, -0.1798, -0.2376],\n",
      "          [-0.1504, -0.1803, -0.0371]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0473, -0.0766,  0.0115],\n",
      "          [ 0.0452, -0.0842, -0.0723],\n",
      "          [-0.0656, -0.1026,  0.0294]],\n",
      "\n",
      "         [[-0.0040, -0.0207,  0.0243],\n",
      "          [-0.0564, -0.0739, -0.0326],\n",
      "          [-0.0123,  0.0620,  0.0166]],\n",
      "\n",
      "         [[ 0.0783, -0.0116,  0.0517],\n",
      "          [ 0.0834,  0.0477,  0.0186],\n",
      "          [ 0.1506,  0.0627,  0.0313]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0228, -0.0274, -0.2050],\n",
      "          [ 0.0146, -0.0988, -0.2112],\n",
      "          [ 0.1152,  0.1149, -0.0584]],\n",
      "\n",
      "         [[ 0.0742, -0.0046,  0.0458],\n",
      "          [ 0.0588,  0.0859,  0.0555],\n",
      "          [ 0.1105,  0.0885, -0.0142]],\n",
      "\n",
      "         [[ 0.0799,  0.0651, -0.1194],\n",
      "          [ 0.1332,  0.1320,  0.0079],\n",
      "          [ 0.1117,  0.0291,  0.0817]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0223,  0.0296, -0.0331],\n",
      "          [-0.1127, -0.0789,  0.0381],\n",
      "          [ 0.0367, -0.0715, -0.0905]],\n",
      "\n",
      "         [[ 0.1068,  0.0222, -0.0573],\n",
      "          [ 0.0991,  0.0526, -0.0615],\n",
      "          [ 0.0438, -0.0249, -0.0570]],\n",
      "\n",
      "         [[ 0.0195, -0.0403, -0.0901],\n",
      "          [ 0.0421, -0.1003, -0.0473],\n",
      "          [ 0.0284,  0.0571, -0.0186]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.2350,  0.1850,  0.1477],\n",
      "          [ 0.1628,  0.2488,  0.0829],\n",
      "          [ 0.1133,  0.2848,  0.2493]],\n",
      "\n",
      "         [[ 0.0702,  0.1328,  0.1092],\n",
      "          [ 0.1125,  0.2159,  0.0905],\n",
      "          [ 0.0474,  0.1133,  0.1558]],\n",
      "\n",
      "         [[ 0.0358,  0.1388, -0.0183],\n",
      "          [ 0.0537,  0.0917,  0.0475],\n",
      "          [-0.0600, -0.0748,  0.1142]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0728,  0.0502,  0.0281],\n",
      "          [ 0.0597,  0.1109, -0.0169],\n",
      "          [-0.0153,  0.0884, -0.0915]],\n",
      "\n",
      "         [[ 0.0618,  0.1871,  0.1766],\n",
      "          [ 0.1556,  0.1960,  0.0436],\n",
      "          [ 0.1056,  0.0764,  0.1933]],\n",
      "\n",
      "         [[ 0.1612, -0.0048, -0.0389],\n",
      "          [ 0.0097, -0.0383, -0.1097],\n",
      "          [ 0.1271,  0.0075, -0.0212]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0627, -0.0059, -0.0143],\n",
      "          [-0.0421,  0.0162, -0.0831],\n",
      "          [ 0.1451,  0.1249,  0.1010]],\n",
      "\n",
      "         [[ 0.0749,  0.0526,  0.0895],\n",
      "          [ 0.0549,  0.0739,  0.1109],\n",
      "          [ 0.0411,  0.0992,  0.1142]],\n",
      "\n",
      "         [[ 0.1864,  0.0202, -0.0654],\n",
      "          [-0.0409,  0.0251,  0.0398],\n",
      "          [ 0.0682, -0.0143,  0.0208]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0578,  0.1363, -0.0351],\n",
      "          [ 0.0285,  0.0679, -0.0486],\n",
      "          [ 0.0161,  0.0501, -0.0032]],\n",
      "\n",
      "         [[-0.0600,  0.0261, -0.0047],\n",
      "          [-0.0075,  0.1133,  0.0544],\n",
      "          [ 0.0332, -0.0099, -0.0213]],\n",
      "\n",
      "         [[ 0.0210,  0.1195,  0.1051],\n",
      "          [ 0.1918,  0.0898,  0.0470],\n",
      "          [ 0.1513,  0.1454,  0.1692]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0259,  0.0752,  0.0321],\n",
      "          [ 0.1265,  0.1138, -0.0433],\n",
      "          [ 0.1243, -0.0370, -0.0714]],\n",
      "\n",
      "         [[-0.0384, -0.0629,  0.0039],\n",
      "          [ 0.0120,  0.0301,  0.0349],\n",
      "          [ 0.0094, -0.0326,  0.0164]],\n",
      "\n",
      "         [[ 0.0229,  0.1286, -0.0113],\n",
      "          [-0.0091,  0.0052, -0.0626],\n",
      "          [-0.0134, -0.0868, -0.0104]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0308, -0.0862, -0.0789],\n",
      "          [ 0.0561,  0.0328,  0.0462],\n",
      "          [ 0.0056,  0.0245, -0.0297]],\n",
      "\n",
      "         [[ 0.0591,  0.0741, -0.0574],\n",
      "          [ 0.0247,  0.1422, -0.0419],\n",
      "          [-0.0046,  0.0399,  0.0622]],\n",
      "\n",
      "         [[-0.0831, -0.1061, -0.0760],\n",
      "          [-0.0736, -0.0337,  0.0378],\n",
      "          [-0.0122, -0.0190,  0.0106]]]]), 'conv2.bias': tensor([-0.0603, -0.1673, -0.1035, -0.1053,  0.0291, -0.0223, -0.1350, -0.0091]), 'fc1.weight': tensor([[-0.0421, -0.0213, -0.0007,  ...,  0.0302, -0.0351,  0.0285],\n",
      "        [-0.0786,  0.0131, -0.0301,  ...,  0.0333,  0.0453,  0.0284],\n",
      "        [ 0.0276,  0.0309,  0.0085,  ..., -0.0101, -0.0362,  0.0250],\n",
      "        ...,\n",
      "        [-0.0419, -0.0260, -0.0091,  ..., -0.0236,  0.0057,  0.0027],\n",
      "        [-0.0267, -0.0348,  0.0282,  ...,  0.0009, -0.0080, -0.0029],\n",
      "        [ 0.0702,  0.0102, -0.0136,  ..., -0.0477, -0.0006,  0.0397]]), 'fc1.bias': tensor([ 0.0245, -0.0379,  0.0771,  0.0031,  0.0507,  0.0406,  0.0034, -0.0124,\n",
      "        -0.0004,  0.0199, -0.0312,  0.0755,  0.0717, -0.0002,  0.0332, -0.0644,\n",
      "        -0.0173,  0.0268, -0.0063,  0.0405,  0.0494,  0.0014, -0.0016,  0.0223,\n",
      "        -0.1365,  0.0005,  0.0759,  0.0259, -0.0285, -0.0127, -0.0137,  0.0378]), 'fc2.weight': tensor([[-2.6488e-02, -4.0495e-01,  4.2388e-01,  7.1581e-01, -1.3939e-02,\n",
      "         -5.5097e-01, -3.0616e-01, -2.2126e-01, -1.2017e-01,  4.5442e-01,\n",
      "         -6.5321e-02,  4.4935e-01,  1.2051e-01,  8.2842e-02,  1.4152e-01,\n",
      "         -2.7644e-01, -2.4096e-01,  3.2148e-01,  4.5698e-05,  1.7264e-01,\n",
      "          2.0367e-01, -3.6154e-01, -7.3952e-03, -7.3855e-02, -6.1583e-01,\n",
      "         -1.7463e-01, -5.4513e-01,  1.1635e-01,  2.1833e-01, -2.3234e-01,\n",
      "          1.0380e-01,  2.3793e-01],\n",
      "        [ 1.0362e-01,  1.9712e-01, -2.5530e-01, -6.3692e-01, -1.4885e-01,\n",
      "          4.8327e-01,  1.4922e-01,  5.9690e-02,  1.2058e-02, -5.3060e-01,\n",
      "          7.2479e-02, -4.7138e-01, -1.6829e-01,  7.6107e-02,  1.1687e-01,\n",
      "          4.7541e-02,  1.0678e-02, -3.4909e-01,  2.8851e-01, -2.2104e-01,\n",
      "         -4.0975e-01,  4.1166e-01, -3.5645e-02, -1.9668e-01,  5.6206e-01,\n",
      "          3.4395e-01,  4.0181e-01, -3.1919e-01, -4.1025e-01,  2.6872e-01,\n",
      "         -8.3688e-02, -2.1768e-01]]), 'fc2.bias': tensor([ 0.0262, -0.2003])})\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I3clLDKbdLWi",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:03.962976Z",
     "start_time": "2025-05-12T11:35:03.959407Z"
    }
   },
   "source": [
    "torch.save(model.state_dict(), \"data/birds_vs_airplanes.pt\")"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kSxntVY4dLWi",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:04.030080Z",
     "start_time": "2025-05-12T11:35:04.025338Z"
    }
   },
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(\"data/birds_vs_airplanes.pt\"))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Whsms-F4dLWi"
   },
   "source": [
    "### Trening na GPU (opcjonalnie)\n",
    "\n",
    "Aby przyspieszyć trening (zwłaszcza w przypadku głębokich modeli i dużych zbiorów danych), powszechnie stosuje się karty graficzne (GPU). Jeśli mamy dostęp do maszyny z kartą graficzną (najlepiej od NVIDIA, obsługującą CUDA), możemy łatwo \"przenieść\" trening na GPU.\n",
    "\n",
    "W tym celu należy przenieść zarówno dane, jak i model, na kartę graficzną, używając metody `.to` (tensora i `nn.Module`) na zdefiniowane urządzenie (patrz poniżej)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nf7j1G36dLWi",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:04.097128Z",
     "start_time": "2025-05-12T11:35:04.094701Z"
    }
   },
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1Vh9302dLWi"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Zmodyfikuj napisaną wyżej pętlę treningową oraz inicjalizację modelu, przenosząc odpowiednio dane (obrazki i etykiety) oraz model na `device`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wL54wJLbdLWi",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:04.165826Z",
     "start_time": "2025-05-12T11:35:04.162993Z"
    }
   },
   "source": [
    "def training_loop_gpu(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train}\")"
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:33.879680Z",
     "start_time": "2025-05-12T11:35:04.234500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "training_loop_gpu(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 107.0023803114891\n",
      "Epoch 10, Training loss 66.44348514080048\n",
      "Epoch 20, Training loss 54.032825261354446\n",
      "Epoch 30, Training loss 50.942595556378365\n",
      "Epoch 40, Training loss 48.321190878748894\n",
      "Epoch 50, Training loss 46.118479162454605\n",
      "Epoch 60, Training loss 43.678450144827366\n",
      "Epoch 70, Training loss 41.2571586817503\n",
      "Epoch 80, Training loss 39.37156615406275\n",
      "Epoch 90, Training loss 37.459661327302456\n",
      "Epoch 100, Training loss 35.56719648092985\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLcwspwEdLWi"
   },
   "source": [
    "### Rozbudowa modelu\n",
    "\n",
    "Możemy \"powiększyć\" model \"na szerokość\" (dodać więcej filtrów) lub \"na głębokość\" (dodać więcej warstw)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLuQoAWOdLWi"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Zmodyfikuj klasę `Net` i stwórz kolejno:\n",
    "- `NetWidth` - 2x więcej filtrów w warstwach splotowych (niech liczba filtrów będzie argumentem konstruktora)\n",
    "- `NetDepth` - dodatkowa warstwa splotowa `conv3`\n",
    "\n",
    "Pamiętaj o zmodyfikowaniu metody `forward`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rS0welLOdLWi",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:33.910977Z",
     "start_time": "2025-05-12T11:35:33.907856Z"
    }
   },
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 * 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(n_chans1 * 2 * 8 * 8, n_chans1 * 4)\n",
    "        self.fc2 = nn.Linear(n_chans1 * 4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, self.n_chans1 * 2 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "COo5D_UjdLWi",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:33.977650Z",
     "start_time": "2025-05-12T11:35:33.974351Z"
    }
   },
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(n_chans1 * 4 * 4, n_chans1 * 4)\n",
    "        self.fc2 = nn.Linear(n_chans1 * 4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = torch.tanh(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(self.conv3(out)), 2)\n",
    "        out = out.view(-1, self.n_chans1 * 4 * 4)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESSdeR9ldLWi"
   },
   "source": [
    "### Regularyzacja L2 / weight decay\n",
    "\n",
    "Regularyzację L2 można zaimplementować samemu (jak niżej).\n",
    "\n",
    "Jest ona jednak wbudowana w `torch.optim`, np. https://pytorch.org/docs/stable/optim.html#torch.optim.SGD, gdzie wystarczy podać wartość `weight_decay` tworząc optymalizator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng8WF-S8dLWi"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "W poniższej pętli treningowej dopisz fragment realizujący regularyzację L2 dla lambda = 0.001."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l57JRSapdLWj",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:35:34.050550Z",
     "start_time": "2025-05-12T11:35:34.047370Z"
    }
   },
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_reg = 0\n",
    "            for param in model.parameters():\n",
    "                l2_reg += torch.norm(param, 2)\n",
    "            loss += 0.001 * l2_reg\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train}\")"
   ],
   "outputs": [],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CoJOeZg3dLWj",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:36:15.610091Z",
     "start_time": "2025-05-12T11:35:34.113478Z"
    }
   },
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 108.10475647449493\n",
      "Epoch 10, Training loss 75.13450002670288\n",
      "Epoch 20, Training loss 59.52363866567612\n",
      "Epoch 30, Training loss 54.52742849290371\n",
      "Epoch 40, Training loss 51.23729193210602\n",
      "Epoch 50, Training loss 48.53098799288273\n",
      "Epoch 60, Training loss 46.67361645400524\n",
      "Epoch 70, Training loss 44.40358214080334\n",
      "Epoch 80, Training loss 42.65197682380676\n",
      "Epoch 90, Training loss 40.839143201708794\n",
      "Epoch 100, Training loss 39.36301167309284\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DQ6EN-xdLWj"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Wywołaj \"zwykłą\" pętlę treningową, tym razem podająć `weight_decay` optyamlizatora równe 0.001. Zaobserwuj wpływ na funkcję straty."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T11:36:44.269530Z",
     "start_time": "2025-05-12T11:36:15.637900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_loop_gpu(\n",
    "    n_epochs = 100,\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, weight_decay=0.001),\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 37.02860698103905\n",
      "Epoch 10, Training loss 35.84481417387724\n",
      "Epoch 20, Training loss 34.16949528455734\n",
      "Epoch 30, Training loss 32.87816595286131\n",
      "Epoch 40, Training loss 31.381581768393517\n",
      "Epoch 50, Training loss 29.919028103351593\n",
      "Epoch 60, Training loss 28.861051090061665\n",
      "Epoch 70, Training loss 27.01045712083578\n",
      "Epoch 80, Training loss 25.870101675391197\n",
      "Epoch 90, Training loss 24.33152087032795\n",
      "Epoch 100, Training loss 23.205924697220325\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Funkcja straty przyjmuje niższe wartości."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45GmfjvXdLWj"
   },
   "source": [
    "### Dropout\n",
    "\n",
    "Na poprzednich zajęciach poznaliśmy technikę regularyzacji modeli - tzw. \"regularyzację L2\" (a.k.a. \"weight decay\"). Polega ona na modyfikacji funkcji straty poprzez dodanie odpowiedniego cżłonu (lub modyfikacji kroku aktualizacji parametrów w algorytmie optymalizacyjnym), wpływając bezpośrednio na wartości wag (parametrów) modelu.\n",
    "\n",
    "Innym z mechanizmów regularyzacyjnych typowym dla (głębokich) sieci neuronowych jest **dropout**:\n",
    "- (Srivastava et al., \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", 2014) https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "\n",
    "Dropout polega na losowym wyłączaniu części neuronów w każdej iteracji treningu, co powoduje, że żadna porcja danych nie jest \"widziana\" przez całą sieć. Dla danej warstwy z dropoutem określa się prawdopodobieństwo wyłączenia (w PyTorch; lub pozostawienia - uwaga na różne konwencje!) danego neuronu.\n",
    "\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html\n",
    "\n",
    "**Uwaga**:  \n",
    "\n",
    "Dropout jest przykładem mechanizmu, który zachowuje się inaczej w trakcie treningu i walidacji. Tworząc modele w PyTorch należy pamiętać, aby przełączyć je w odpowiedni tryb: `model.train()` lub `model.eval()`. Dobrze jest wyrobić sobie nawyk używania tych trybów nawet, jeśli w modelu nie ma dropoutu (ani np. batch normalization - patrz poniżej)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7xIrxuhdLWj"
   },
   "source": [
    "#### Ćwiczenie\n",
    "Dołóż warstwy `nn.Dropout2d` po warstwach splotowych (po max poolingu) do sieci `Net` i stwórz w ten sposob `NetDropout`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NJd9P-HWdLWq",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:36:44.308884Z",
     "start_time": "2025-05-12T11:36:44.305647Z"
    }
   },
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(n_chans1 * 8 * 8, n_chans1 * 4)\n",
    "        self.fc2 = nn.Linear(n_chans1 * 4, 2)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, self.n_chans1 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZZouH-2dLWq"
   },
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Ważnym mechanizmem w kontekście (głębokich) sieci neuronowych jest tzw. **batch normalization**:\n",
    "\n",
    "- (Ioffe i Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", 2015) - https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "Polega on na normalizacji (standaryzacji) wejść do funkcji aktywacji (lub wyjść z nich - w praktyce nie powinno mieć to większego znaczenia):\n",
    "- w treningu: na podstawie średniej i wariancji pojedynczego wsadu danych (\"batch\")\n",
    "- podczas inferencji: na podstawie średniej i wariancji całego zbioru uczącego (estymowane w czasie treningu)\n",
    "\n",
    "Zastosowanie BN:\n",
    "- pozwala ustabilizować / przyspieszyć trening\n",
    "- zapobiega zanikaniu gradientów\n",
    "- wprowadza dodatkowy efekt regularyzacyjny\n",
    "\n",
    "W PyTorch dla sieci splotowych BN zrealizowana jest jako:\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "\n",
    "Podobnie jak w przypadku dropoutu, należy przełączać model między trybem treningu i ewaluacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1yjYxQydLWq"
   },
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Dodaj warstwy `BatchNorm2d` po warstwach splotowych `Net` tworząc w ten sposób `NetBatchNorm`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QBTzPoTidLWq",
    "ExecuteTime": {
     "end_time": "2025-05-12T11:36:44.371702Z",
     "start_time": "2025-05-12T11:36:44.367998Z"
    }
   },
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(n_chans1)\n",
    "        self.fc1 = nn.Linear(n_chans1 * 8 * 8, n_chans1 * 4)\n",
    "        self.fc2 = nn.Linear(n_chans1 * 4, 2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = torch.tanh(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = torch.tanh(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "\n",
    "        out = out.view(-1, self.n_chans1 * 8 * 8)\n",
    "        out = self.fc1(out)\n",
    "        out = torch.tanh(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wnioski"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "W trakcie zajęć poznaliśmy kilka technik regularyzacji modeli, które można stosować w sieciach neuronowych:\n",
    "- regularyzacja L2 (weight decay)\n",
    "- dropout\n",
    "- batch normalization.\n",
    "Dzięki tym technikom możemy trenować głębsze i bardziej złożone modele, które są mniej podatne na przeuczenie."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
